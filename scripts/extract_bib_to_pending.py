import os
import re
import json
import hashlib

# ================= æ ¸å¿ƒé…ç½®åŒº =================
script_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.dirname(script_dir)

SOURCE_BIB = os.path.join(project_root, "assets", "raw_sources", "surveys", "2411_17040_refs.bib")
OUTPUT_JSON = os.path.join(project_root, "assets", "data", "pending", "imported_from_survey.json")
HISTORY_FILE = os.path.join(project_root, "assets", "raw_sources", "surveys", "history_log.txt")

DEFAULT_VENUE = "arXiv"


# ================= å·¥å…·å‡½æ•°åŒº =================

def load_history():
    if not os.path.exists(HISTORY_FILE): return set()
    with open(HISTORY_FILE, 'r', encoding='utf-8') as f:
        return set(line.strip() for line in f)


def append_to_history(identifier):
    with open(HISTORY_FILE, 'a', encoding='utf-8') as f:
        f.write(f"{identifier}\n")


def get_string_hash(text):
    """ç”Ÿæˆæ ‡é¢˜çš„å”¯ä¸€å“ˆå¸ŒæŒ‡çº¹"""
    return hashlib.md5(text.strip().lower().encode('utf-8')).hexdigest()


def parse_bib_file(file_path):
    if not os.path.exists(file_path):
        print(f"âŒ æ‰¾ä¸åˆ°è¾“å…¥æ–‡ä»¶: {file_path}")
        return []

    with open(file_path, 'r', encoding='utf-8') as f:
        content = f.read()

    raw_entries = re.split(r'@\w+\s*\{', content)[1:]
    parsed_entries = []
    seen_titles = set()

    for raw in raw_entries:
        t_match = re.search(r'title\s*=\s*[\{"](.+?)[\}"]', raw, re.IGNORECASE | re.DOTALL)
        if not t_match: continue

        # æ¸…ç†æ ‡é¢˜
        clean_title = " ".join(t_match.group(1).split()).replace('{', '').replace('}', '')
        title_fingerprint = clean_title.lower()
        if title_fingerprint in seen_titles: continue
        seen_titles.add(title_fingerprint)

        entry = {'title': clean_title}

        # å°è¯•æ‹¼å‡‘é“¾æ¥
        link = ""
        ep_match = re.search(r'eprint\s*=\s*[\{"](.*?)[\}"]', raw, re.IGNORECASE)
        if ep_match:
            clean_id = re.search(r'(\d{4}\.\d{4,5})', ep_match.group(1))
            if clean_id: link = f"https://arxiv.org/abs/{clean_id.group(1)}"

        if not link:
            doi_match = re.search(r'doi\s*=\s*[\{"](.+?)[\}"]', raw, re.IGNORECASE)
            if doi_match: link = f"https://doi.org/{doi_match.group(1)}"

        entry['link'] = link
        y_match = re.search(r'year\s*=\s*[\{"](\d{4})[\}"]', raw, re.IGNORECASE)
        entry['year'] = y_match.group(1) if y_match else "Unknown"
        v_match = re.search(r'(journal|booktitle)\s*=\s*[\{"](.+?)[\}"]', raw, re.IGNORECASE)
        entry['venue'] = v_match.group(2) if v_match else ""

        parsed_entries.append(entry)

    return parsed_entries


def save_to_json(new_paper):
    data = []
    if os.path.exists(OUTPUT_JSON):
        try:
            with open(OUTPUT_JSON, 'r', encoding='utf-8') as f:
                data = json.load(f)
        except:
            data = []

    data.append(new_paper)
    with open(OUTPUT_JSON, 'w', encoding='utf-8') as f:
        json.dump(data, f, indent=2, ensure_ascii=False)


# ================= ä¸»é€»è¾‘æµç¨‹ =================

def main():
    entries = parse_bib_file(SOURCE_BIB)
    history = load_history()

    print(f"ğŸš€ æ–‡çŒ®æå–å™¨å·²å°±ç»ªã€‚å¾…å¤„ç†: {len(entries)} ç¯‡ï¼Œå·²è·³è¿‡: {len(history)} ç¯‡ã€‚")
    print("-" * 50)

    for i, item in enumerate(entries):
        title_hash = get_string_hash(item['title'])
        if title_hash in history: continue

        print(f"\n[{i + 1}/{len(entries)}] --------------------------------")
        print(f"TITLE: {item['title']}")
        print(f"YEAR : {item['year']} | VENUE: {item['venue'] if item['venue'] else 'N/A'}")

        current_link = item['link']
        if current_link:
            print(f"LINK : {current_link}")
        else:
            print(f"LINK : [!] è„šæœ¬æœªèƒ½åœ¨ BibTeX ä¸­è‡ªåŠ¨æ‰¾åˆ°é“¾æ¥")

        while True:
            choice = input("ğŸ‘‰ æ“ä½œ (y:æ”¶å½• / n:è·³è¿‡ / q:é€€å‡º): ").lower().strip()

            if choice == 'q':
                print("ğŸ‘‹ è¿›åº¦å·²ä¿å­˜ã€‚")
                return

            if choice == 'n':
                append_to_history(title_hash)
                print("ğŸ—‘ï¸  å·²æ ‡è®°ä¸ºè·³è¿‡ã€‚")
                break

            if choice == 'y':
                # 1. é“¾æ¥è´¨é‡é—¸é—¨
                if not current_link:
                    manual_input = input("ğŸ”— ç¼ºå°‘é“¾æ¥ï¼Œè¯·è¾“å…¥ URL (å›è½¦åˆ™æ”¾å¼ƒæ”¶å½•æ­¤ç¯‡): ").strip()
                    if not manual_input:
                        append_to_history(title_hash)
                        print("â­ï¸  å› æ— é“¾æ¥ï¼Œå·²è‡ªåŠ¨æ”¾å¼ƒæ”¶å½•ã€‚")
                        break
                    else:
                        current_link = manual_input

                # 2. æ ‡ç­¾å¤„ç†ï¼šé»˜è®¤ç½®ç©ºï¼Œç”±ä½ è¾“å…¥
                user_tags_input = input("ğŸ·ï¸  è¯·è¾“å…¥æ ‡ç­¾ (å¤šä¸ªç”¨é€—å·æˆ–ç©ºæ ¼åˆ†éš”ï¼Œç›´æ¥å›è½¦åˆ™ä¸ºç©º): ").strip()

                final_tags = []
                if user_tags_input:
                    # åˆ†å‰²å¹¶æ¸…ç†æ ‡ç­¾
                    final_tags = [t.strip() for t in re.split(r'[,ï¼Œ\s]+', user_tags_input) if t.strip()]

                # 3. æ„é€ æ­£å¼å¯¹è±¡å¹¶ä¿å­˜
                paper_obj = {
                    "title": item['title'],
                    "link": current_link,
                    "venue": item['venue'] if item['venue'] else DEFAULT_VENUE,
                    "date": item['year'],
                    "tags": final_tags,  # å¯èƒ½æ˜¯ []
                    "source": "survey:2411.17040",
                    "code": "",
                    "authors": "TBA",
                    "description": "",
                    "image": ""
                }
                save_to_json(paper_obj)
                append_to_history(title_hash)
                print(f"âœ… å·²æ”¶å½•ã€‚æ ‡ç­¾: {final_tags}")
                break


if __name__ == "__main__":
    main()