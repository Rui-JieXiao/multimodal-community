import re
import json
import os

# --- 1. è·¯å¾„é…ç½® (ä½¿ç”¨ç›¸å¯¹äºè„šæœ¬ä½ç½®çš„ç»å¯¹è·¯å¾„) ---
SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
PROJECT_ROOT = os.path.dirname(SCRIPT_DIR)

# è¾“å…¥ï¼šåŸææ–™ä»“åº“ä¸­çš„ MD æ–‡ä»¶
INPUT_FILE = os.path.join(PROJECT_ROOT, "assets", "raw_sources", "source_awesome_mllm.md")
# è¾“å‡ºï¼šä¸»æ•°æ®åº“
OUTPUT_FILE = os.path.join(PROJECT_ROOT, "assets", "data", "datasets_data.json")

# ğŸŸ¢ æ ¸å¿ƒé…ç½®ï¼šæ•°æ®é›†ç« èŠ‚ç™½åå•
TARGET_SECTIONS = [
    ("## Datasets of Pre-Training for Alignment", "Pre-Training"),
    ("## Datasets of Multimodal Instruction Tuning", "Instruction Tuning"),
    ("## Datasets of In-Context Learning", "In-Context Learning"),
    ("## Datasets of Multimodal Chain-of-Thought", "Chain-of-Thought"),
    ("## Datasets of Multimodal RLHF", "RLHF"),
    ("## Benchmarks for Evaluation", "Benchmark"),
    ("## Others", "Others"),
]

def get_dataset_id(title):
    """ç”Ÿæˆæ•°æ®é›†å”¯ä¸€ ID (å¿½ç•¥å¤§å°å†™å’Œç‰¹æ®Šç¬¦å·)"""
    if not title:
        return "unknown"
    return re.sub(r'[^a-z0-9]', '', title.lower())

def extract_first_link(text_cell):
    """ä»å•å…ƒæ ¼æå–ç¬¬ä¸€ä¸ªéè£…é¥°æ€§é“¾æ¥"""
    candidates = re.findall(r'\]\((http.*?)\)', text_cell)
    for url in candidates:
        url = url.strip()
        if any(bad in url for bad in ['.svg', 'shields.io', 'badge']):
            continue
        return url
    return ""

def clean_text(text):
    """ç§»é™¤ Markdown è¯­æ³•ï¼Œä¿ç•™çº¯æ–‡æœ¬"""
    if not text: return ""
    text = text.replace('**', '')
    text = re.sub(r'\[([^\]]+)\]\(http.*?\)', r'\1', text) # æå– [Text](Link) ä¸­çš„ Text
    text = text.replace('`', '')
    text = text.replace('\n', ' ').strip()
    return text

def parse_datasets():
    print(f"ğŸš€ å¯åŠ¨æ•°æ®é›†æå–å™¨...")

    # --- A. å‡†å¤‡å·¥ä½œï¼šè¯»å–ç°æœ‰æ•°æ®åº“ ---
    existing_data = []
    existing_map = {}

    if os.path.exists(OUTPUT_FILE):
        try:
            with open(OUTPUT_FILE, 'r', encoding='utf-8') as f:
                existing_data = json.load(f)
                for item in existing_data:
                    if 'title' in item:
                        pid = get_dataset_id(item['title'])
                        existing_map[pid] = item
            print(f"ğŸ“– è¯»å–åˆ°ä¸»åº“ç°æœ‰æ•°æ®é›†ï¼š{len(existing_data)} æ¡")
        except Exception as e:
            print(f"âš ï¸ è¯»å–ç°æœ‰ JSON å¤±è´¥: {e}")
    else:
        os.makedirs(os.path.dirname(OUTPUT_FILE), exist_ok=True)

    # --- B. è¯»å– MD æºæ–‡ä»¶ ---
    if not os.path.exists(INPUT_FILE):
        print(f"âŒ é”™è¯¯ï¼šæ‰¾ä¸åˆ°æºæ–‡ä»¶ {INPUT_FILE}")
        print("ğŸ’¡ è¯·ç¡®è®¤ README.md å·²é‡å‘½åå¹¶æ”¾å…¥ assets/raw_sources/ æ–‡ä»¶å¤¹")
        return

    with open(INPUT_FILE, 'r', encoding='utf-8') as f:
        full_content = f.read()

    # é”å®šæ•°æ®é›†ä¸“å±åŒºåŸŸï¼Œé˜²æ­¢è¯¯æŠ“è®ºæ–‡
    datasets_start_marker = "# Awesome Datasets"
    if datasets_start_marker in full_content:
        content_for_scan = full_content.split(datasets_start_marker)[1]
    else:
        print("âš ï¸ è­¦å‘Šï¼šæœªæ‰¾åˆ° '# Awesome Datasets' æ ‡è®°ï¼Œå°†æ‰«æå…¨æ–‡ä»¶")
        content_for_scan = full_content

    current_run_map = {}

    # --- C. å¾ªç¯è§£æç« èŠ‚ ---
    for section_header, tag_name in TARGET_SECTIONS:
        print(f"   ğŸ” è§£æç« èŠ‚: [{section_header}]")

        pattern = re.escape(section_header) + r"\n(.*?)(?=\n## |$)"
        match = re.search(pattern, content_for_scan, re.DOTALL)

        if not match: continue

        lines = match.group(1).split('\n')

        # æ™ºèƒ½è¯†åˆ«è¡¨æ ¼ç»“æ„
        table_type = "TYPE_B"
        for line in lines[:5]:
            if "| Modalities |" in line:
                table_type = "TYPE_A"
                break

        count = 0
        for line in lines:
            line = line.strip()
            # è¿‡æ»¤æ‰éæ•°æ®è¡Œï¼ˆè¡¨å¤´ã€åˆ†éš”çº¿ç­‰ï¼‰
            if not line.startswith('|') or ':---' in line or '| Name' in line:
                continue

            cols = [c.strip() for c in line.split('|')]
            if len(cols) < 5: continue

            name_cell = cols[1]
            paper_cell = cols[2]

            # æå–åç§°
            title_match = re.search(r"\*\*(?P<title>.*?)\*\*", name_cell)
            title = title_match.group("title").strip() if title_match else clean_text(name_cell)

            if not title: continue
            pid = get_dataset_id(title)

            # æå–é“¾æ¥å’Œæè¿°
            final_link = ""
            final_desc = ""

            if table_type == "TYPE_A":
                if len(cols) >= 6:
                    type_val = clean_text(cols[3])
                    mod_val = clean_text(cols[4])
                    final_desc = f"{type_val} | {mod_val}"
                final_link = extract_first_link(paper_cell)
            else:
                if len(cols) >= 6:
                    link_cell = cols[3]
                    notes_cell = cols[4]
                    link_from_col3 = extract_first_link(link_cell)
                    link_from_col2 = extract_first_link(paper_cell)
                    final_link = link_from_col3 if link_from_col3 else link_from_col2
                    final_desc = clean_text(notes_cell)

            # å­˜å…¥æœ¬æ¬¡æŠ“å–ç»“æœæ˜ å°„è¡¨ï¼ˆå¤„ç†åŒæ–‡ä»¶é‡å¤é¡¹ï¼‰
            if pid in current_run_map:
                if tag_name not in current_run_map[pid]['tags']:
                    current_run_map[pid]['tags'].append(tag_name)
            else:
                current_run_map[pid] = {
                    "title": title,
                    "link": final_link,
                    "venue": "Dataset",
                    "description": final_desc,
                    "tags": [tag_name],
                    "source": "auto-script"
                }
            count += 1
        print(f"      âœ… æ‰«æåˆ° {count} æ¡")

    # --- D. æœ€ç»ˆåˆå¹¶ä¸ä¿æŠ¤ ---
    final_data = []
    current_run_ids = set(current_run_map.keys())

    # 1. åˆå¹¶æœ¬æ¬¡æŠ“å–çš„æ•°æ®ä¸æ—§æ•°æ®
    for pid, new_item in current_run_map.items():
        if pid in existing_map:
            old_item = existing_map[pid]
            merged_tags = list(set(old_item.get('tags', []) + new_item['tags']))

            # ğŸ›¡ï¸ æè¿°ä¿æŠ¤é€»è¾‘ï¼šå¦‚æœæ—§åº“å·²æœ‰æè¿°ï¼Œåˆ™ä¸è¦†ç›–
            saved_desc = old_item.get("description")
            final_desc = saved_desc if saved_desc else new_item['description']

            final_item = {
                "title": new_item['title'],
                "link": new_item['link'] if new_item['link'] else old_item.get("link"),
                "venue": "Dataset",
                "tags": merged_tags,
                "source": old_item.get("source", "auto-script"),
                "description": final_desc,
                # ç»§æ‰¿ä¿æŠ¤å­—æ®µ
                "authors": old_item.get("authors", ""),
                "image": old_item.get("image", ""),
                "code": old_item.get("code", "")
            }
            final_data.append(final_item)
        else:
            # ğŸ”µ çº¯æ–°å¢é¡¹åˆå§‹åŒ–
            new_item["authors"] = ""
            new_item["image"] = ""
            new_item["code"] = ""
            final_data.append(new_item)

    # 2. ä¿ç•™åº“ä¸­åŸæœ¬å­˜åœ¨ä½†æœ¬æ¬¡ MD ä¸­æ²¡æŠ“åˆ°çš„æ‰‹åŠ¨å½•å…¥é¡¹
    for old_item in existing_data:
        old_pid = get_dataset_id(old_item.get('title', ''))
        if old_pid not in current_run_ids:
            if old_item.get('source') != 'auto-script':
                final_data.append(old_item)
                print(f"ğŸ›¡ï¸  ä¿ç•™æ‰‹åŠ¨å½•å…¥é¡¹: {old_item.get('title')}")

    # --- E. ä¿å­˜ç»“æœ ---
    with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:
        json.dump(final_data, f, ensure_ascii=False, indent=2)

    print(f"\nğŸ‰ æ•°æ®é›†åŒæ­¥å®Œæˆï¼")
    print(f"ğŸ“Š æ•°æ®åº“æ€»æ¡æ•°: {len(final_data)}")
    print(f"ğŸ“‚ æ›´æ–°è·¯å¾„: {os.path.abspath(OUTPUT_FILE)}")

if __name__ == "__main__":
    parse_datasets()