import re
import json
import os

# --- 1. è·¯å¾„ä¸å…¨å±€é…ç½® ---
SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
PROJECT_ROOT = os.path.dirname(SCRIPT_DIR)

# ç”Ÿäº§ç¯å¢ƒä¸»åº“ (ç”¨äºå»é‡å‚è€ƒ)
DB_FILE = os.path.join(PROJECT_ROOT, "assets", "data", "papers_data.json")

# åŠ å·¥åŒº (Staging Area)
STAGING_FILE = os.path.join(PROJECT_ROOT, "assets", "data", "staging", "incoming_papers.json")

# ğŸŸ¢ æ ¸å¿ƒé…ç½®ï¼šå¤šæºç®¡ç†å™¨
SOURCES_CONFIG = {
    # æº 1: åŸå§‹çš„ MLLM è¡¨æ ¼
    "awesome_mllm": {
        "file_path": os.path.join(PROJECT_ROOT, "assets", "raw_sources", "source_awesome_mllm.md"),
        "parser_type": "table",
        "sections": [
            ("## Multimodal Instruction Tuning", "Instruction Tuning"),
            ("## Multimodal Hallucination", "Hallucination"),
            ("## Multimodal In-Context Learning", "In-Context Learning"),
            ("## Multimodal Chain-of-Thought", "Chain-of-Thought"),
            ("## LLM-Aided Visual Reasoning", "Visual Reasoning"),
            ("## Foundation Models", "Foundation Models"),
            ("## Evaluation", "Evaluation"),
            ("## Multimodal RLHF", "RLHF"),
            ("## Others", "Others"),
        ]
    },
    # æº 2: æ¢è®©å¤§ä½¬çš„åˆ—è¡¨ (Paul Liang) - ä»…ä¸‰å¤§æ ¸å¿ƒé¢†åŸŸ
    "pliang_list": {
        "file_path": os.path.join(PROJECT_ROOT, "assets", "raw_sources", "source_pliang_list.md"),
        "parser_type": "list",
        "sections": [
            ("### Multimodal Representations", "Representations"),
            ("### Multimodal Fusion", "Fusion"),
            ("### Multimodal Alignment", "Alignment"),
        ]
    }
}


# --- 2. å·¥å…·å‡½æ•° ---

def get_paper_id(title):
    """ç”Ÿæˆæ ‡é¢˜å”¯ä¸€æŒ‡çº¹"""
    if not title: return "unknown"
    return re.sub(r'[^a-z0-9]', '', title.lower())


def extract_valid_link(text):
    """ä»æ–‡æœ¬æå–ç¬¬ä¸€ä¸ª HTTP é“¾æ¥ (æ’é™¤å¾½ç« å›¾ç‰‡)"""
    candidates = re.findall(r'\]\((http.*?)\)', text)
    for url in candidates:
        url = url.strip()
        if any(bad in url for bad in ['.svg', 'shields.io', 'badge']):
            continue
        return url
    return ""


def clean_md_syntax(text):
    """æ¸…é™¤ Markdown è¯­æ³• (**bold**, `code`)"""
    if not text: return ""
    return text.replace('**', '').replace('`', '').strip()


def load_json_safely(file_path):
    if os.path.exists(file_path):
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                return json.load(f)
        except:
            return []
    return []


def save_json_safely(data, file_path):
    os.makedirs(os.path.dirname(file_path), exist_ok=True)
    with open(file_path, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)


# --- 3. æ ¸å¿ƒè§£æç­–ç•¥ (ç­–ç•¥åˆ†ç¦») ---

def parse_table_row(line):
    """ç­–ç•¥ A: è§£æ Markdown è¡¨æ ¼è¡Œ"""
    if not line.startswith('|') or ':---' in line or '| Title' in line:
        return None

    cols = [c.strip() for c in line.split('|')]
    if len(cols) < 4: return None

    title_cell = cols[1]
    venue_cell = cols[2]
    date_cell = cols[3]

    title_match = re.search(r"\*\*(?P<title>.*?)\*\*", title_cell)
    valid_url = extract_valid_link(title_cell)

    if title_match and valid_url:
        return {
            "title": clean_md_syntax(title_match.group("title")),
            "link": valid_url,
            "venue": venue_cell if venue_cell != "-" else "Research Community",
            "date": date_cell[:4] if len(date_cell) >= 4 else "2024",
            "code": ""
        }
    return None


def parse_list_line(line):
    """ç­–ç•¥ B: è§£æåµŒå¥—åˆ—è¡¨è¡Œ (Liang's format)"""
    # æ ¼å¼: [Title](Link), Venue Year [[blog]](BlogLink) [[code]](CodeLink)
    if not line.strip().startswith('['): return None

    # 1. æå–æ ‡é¢˜å’Œä¸»é“¾æ¥
    main_match = re.search(r'^\[(?P<title>.*?)\]\((?P<link>http.*?)\)', line)
    if not main_match: return None

    title = clean_md_syntax(main_match.group("title"))
    link = main_match.group("link").strip()

    # 2. æå– Code é“¾æ¥ (å…ˆæå–ï¼Œæå–å®Œå°±å¯ä»¥æŠŠåŸæ¥çš„å­—ç¬¦ä¸²åˆ æ‰äº†)
    code_link = ""
    code_match = re.search(r'\[\[code\]\]\((?P<code>http.*?)\)', line)
    if code_match:
        code_link = code_match.group("code").strip()

    # 3. ğŸ§¹ å¼€å§‹å¤§æ¸…æ´— (Cleaning)
    rest = line[main_match.end():]  # æˆªå–æ ‡é¢˜ä¹‹åçš„æ‰€æœ‰å†…å®¹

    # 3.1 ç§»é™¤ code å— (æ— è®ºæ˜¯å¦æœ‰é“¾æ¥)
    rest = re.sub(r'\[\[code\]\].*?(?=\s|\[|$)', '', rest, flags=re.IGNORECASE)
    # 3.2 ç§»é™¤ markdown é“¾æ¥ç»“æ„ [text](url) -> æ¯”å¦‚ [[blog]](http...)
    rest = re.sub(r'\[.*?\]\(.*?\)', '', rest)
    # 3.3 ç§»é™¤æ®‹ç•™çš„æ–¹æ‹¬å·æ ‡ç­¾ -> æ¯”å¦‚ [PDF], [Slides]
    rest = re.sub(r'\[.*?\]', '', rest)

    # 4. æå–å¹´ä»½
    year_match = re.search(r'(20\d{2}|19\d{2})', rest)
    year = year_match.group(1) if year_match else "2024"

    # 5. æå– Venue
    # ç§»é™¤å¹´ä»½
    rest = rest.replace(year, '')
    # ç§»é™¤é¦–å°¾æ‚è´¨ (é€—å·, å¥å·, æ‹¬å·, ç«–çº¿, ç©ºæ ¼)
    venue = rest.strip(' ,.-|()')

    # ğŸ›¡ï¸ 6. å®‰å…¨æ°”å›Š (Safety Guard)
    # å¦‚æœæ¸…æ´—å Venue ä¾ç„¶åŒ…å« httpï¼Œè¯´æ˜æºæ–‡æœ¬æ ¼å¼æåº¦æ··ä¹±
    # æ­¤æ—¶å¼ºåˆ¶é‡ç½®ï¼Œé˜²æ­¢å‰ç«¯å¡ç‰‡è¢«é•¿é“¾æ¥æ’‘çˆ†
    if 'http' in venue:
        venue = "Research Community"

    # å¦‚æœæ¸…æ´—è¿‡å¤´å˜ç©ºäº†ï¼Œç»™ä¸ªé»˜è®¤å€¼
    if not venue:
        venue = "Research Community"

    return {
        "title": title,
        "link": link,
        "venue": venue,
        "date": year,
        "code": code_link
    }


# --- 4. ä¸»æµç¨‹æ§åˆ¶å™¨ ---

def run_extraction(source_key):
    config = SOURCES_CONFIG.get(source_key)
    if not config:
        print(f"âŒ æ‰¾ä¸åˆ°é…ç½®é¡¹: {source_key}")
        return

    # A. å‡†å¤‡æ•°æ®
    main_db = load_json_safely(DB_FILE)
    existing_ids = {get_paper_id(item.get('title')) for item in main_db}

    staging_data = load_json_safely(STAGING_FILE)
    staging_map = {get_paper_id(item['title']): item for item in staging_data}

    # B. è¯»å–æ–‡ä»¶
    if not os.path.exists(config['file_path']):
        print(f"âŒ æ‰¾ä¸åˆ°æºæ–‡ä»¶: {config['file_path']}")
        return

    with open(config['file_path'], 'r', encoding='utf-8') as f:
        content = f.read()

    new_count = 0
    print(f"ğŸš€ å¼€å§‹ä» [{source_key}] è¿›è´§ (æ¨¡å¼: {config['parser_type']})...")

    # C. éå†ç« èŠ‚
    for section_header, tag_name in config['sections']:
        pattern = re.escape(section_header) + r"\n(.*?)(?=\n#+ |$)"
        match = re.search(pattern, content, re.DOTALL)

        if not match: continue

        lines = match.group(1).split('\n')

        for line in lines:
            line = line.strip()
            if not line: continue

            parsed_item = None
            if config['parser_type'] == 'table':
                parsed_item = parse_table_row(line)
            elif config['parser_type'] == 'list':
                parsed_item = parse_list_line(line)

            if parsed_item:
                pid = get_paper_id(parsed_item['title'])

                if pid in existing_ids: continue

                if pid not in staging_map:
                    staging_map[pid] = {
                        "title": parsed_item['title'],
                        "link": parsed_item['link'],
                        "venue": parsed_item['venue'],
                        "date": parsed_item['date'],
                        "tags": [tag_name],
                        "source": source_key,
                        "code": parsed_item.get('code', ""),
                        "authors": "",
                        "description": "",
                        "image": ""
                    }
                    new_count += 1
                else:
                    if tag_name not in staging_map[pid]['tags']:
                        staging_map[pid]['tags'].append(tag_name)

    # D. ä¿å­˜
    final_staging = list(staging_map.values())
    save_json_safely(final_staging, STAGING_FILE)

    print(f"âœ… [{source_key}] æå–å®Œæˆï¼")
    print(f"ğŸ“Š æœ¬æ¬¡æ–°å¢åˆ°åŠ å·¥åŒº: {new_count} ç¯‡")
    print(f"ğŸ“ åŠ å·¥åŒºå½“å‰æ€»æ•°: {len(final_staging)} ç¯‡")


if __name__ == "__main__":
    # æ‰§è¡Œç²¾ç®€åçš„æ¢è®©åˆ—è¡¨æŠ“å–
    run_extraction("pliang_list")