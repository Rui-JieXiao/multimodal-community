[
  {
    "title": "Seed1.8 Model Card: Towards Generalized Real-World Agency",
    "venue": "Bytedance Seed",
    "date": "2025",
    "link": "https://lf3-static.bytednsdoc.com/obj/eden-cn/lapzild-tss/ljhwZthlaukjlkulzlp/research/Seed-1.8-Modelcard.pdf",
    "source": "auto-script",
    "tags": [
      "Instruction Tuning"
    ],
    "authors": "ByteDance Seed Team",
    "description": "Seed1.8 是字节跳动发布的面向通用现实世界代理(Generalized Real-World Agency)的多模态模型。该模型将感知、推理和行动集成在单一模型中，支持搜索、代码执行和 GUI 交互等复杂的多步任务。在视觉理解、长视频推理及 Agent 任务上，其性能显著优于 Seed1.5-VL，并能与 Gemini 3 Pro 等前沿模型媲美。",
    "image": "",
    "code": ""
  },
  {
    "title": "Introducing GPT-5.2",
    "venue": "OpenAI",
    "date": "2025",
    "link": "https://openai.com/index/introducing-gpt-5-2/",
    "source": "auto-script",
    "tags": [
      "Instruction Tuning"
    ],
    "authors": "OpenAI",
    "description": "GPT-5.2 是 OpenAI 迄今为止最强大的模型系列，专为专业知识型工作打造。它在电子表格制作、演示文稿设计、代码编写、视觉识别、长上下文理解及复杂多步代理任务上均有显著提升。该系列包含 Instant、Thinking 和 Pro 三个版本，其中 GPT-5.2 Thinking 是首个在 GDPval 评测中达到人类专家水平的模型，并在 SWE-Bench Pro（软件工程）等基准测试中树立了新标杆。",
    "image": "",
    "code": ""
  },
  {
    "title": "Introducing Mistral 3",
    "venue": "Blog",
    "date": "2025",
    "link": "https://mistral.ai/news/mistral-3",
    "source": "auto-script",
    "tags": [
      "Instruction Tuning"
    ],
    "authors": "Mistral AI",
    "description": "Mistral 3 是 Mistral AI 推出的下一代开放多模态和多语言模型系列。该系列包含三个最先进的小型密集模型（Ministral 3B、8B 和 14B）以及 Mistral Large 3（41B 激活参数/675B 总参数的稀疏 MoE 模型）。所有模型均采用 Apache 2.0 许可发布，支持图像理解，并在多语言对话、长上下文处理及边缘计算效率上表现优异，其中 Mistral Large 3 在非推理类开源模型中表现顶尖。",
    "image": "",
    "code": "https://github.com/mistralai/mistral-inference"
  },
  {
    "title": "Qwen3-VL Technical Report",
    "venue": "arXiv",
    "date": "2025",
    "link": "https://arxiv.org/pdf/2511.21631",
    "source": "auto-script",
    "tags": [
      "Instruction Tuning"
    ],
    "authors": "Qwen Team",
    "description": "Qwen3-VL 是 Qwen 系列中迄今为止最强大的视觉语言模型，在广泛的多模态基准测试中实现了卓越的性能。它原生支持高达 256K token 的交错上下文，能够无缝集成文本、图像和视频，并提供了密集型（Dense）和混合专家（MoE）等多种变体。",
    "image": "",
    "code": "https://github.com/QwenLM/Qwen3-VL"
  },
  {
    "title": "Emu3.5: Native Multimodal Models are World Learners",
    "venue": "arXiv",
    "date": "2025",
    "link": "https://arxiv.org/pdf/2510.26583",
    "source": "auto-script",
    "tags": [
      "Instruction Tuning"
    ],
    "authors": "BAAI Emu3.5 Team",
    "description": "Emu3.5 是由智源研究院（BAAI）发布的原生多模态世界模型，在超过 10 万亿个多模态 token 上进行了端到端预训练。作为 Emu3 的升级版，它统一了语言、图像和视频的理解与生成，仅通过预测下一个 token 即可完成多模态任务，无需依赖 CLIP 或扩散模型。Emu3.5 引入了离散扩散适应（DiDA）机制，在保持高性能的同时将图像生成推理速度提升了约 20 倍。",
    "image": "",
    "code": "https://github.com/baaivision/Emu3.5"
  },
  {
    "title": "VITA-E: Natural Embodied Interaction with Concurrent Seeing, Hearing, Speaking, and Acting",
    "venue": "arXiv",
    "date": "2025",
    "link": "https://arxiv.org/pdf/2510.21817.pdf",
    "source": "auto-script",
    "tags": [
      "Instruction Tuning"
    ],
    "authors": "Xiaoyu Liu, Chaoyou Fu, Chi Yan, Chu Wu, Haihan Gao, et al.",
    "description": "VITA-E 是由南京大学、腾讯优图实验室、中科院自动化所及傅利叶智能联合推出的具身交互框架，旨在解决传统 VLA 模型交互僵化的问题。它采用“双模型架构”（Dual-Model Architecture），包含并行的“主动模型”和“待机模型”，赋予了机器人同时进行“看、听、说、动”的能力。这种设计让机器人能够在执行物理动作时处理语音中断或插入新指令，实现了接近实时的自然人机交互体验。",
    "image": "",
    "code": "https://github.com/VITA-MLLM/VITA-E"
  },
  {
    "title": "DeepSeek-OCR: Contexts Optical Compression",
    "venue": "arXiv",
    "date": "2025",
    "link": "https://arxiv.org/pdf/2510.18234",
    "source": "auto-script",
    "tags": [
      "Instruction Tuning"
    ],
    "authors": "Haoran Wei, Yaofeng Sun, Yukun Li",
    "description": "DeepSeek-OCR 是由 DeepSeek 发布的创新性模型，旨在探索通过光学 2D 映射（Optical 2D Mapping）压缩长上下文的可行性。该模型由专门设计的视觉编码器 DeepEncoder 和 DeepSeek3B-MoE 解码器组成，能够在处理高分辨率文档输入时保持极低的激活度并实现高压缩比。实验显示，在压缩比小于 10 倍（文本 token 数/视觉 token 数 < 10）的情况下，其 OCR 解码精度高达 97%，为大模型的长上下文压缩和视觉文档理解提供了新的高效路径。",
    "image": "",
    "code": "https://github.com/deepseek-ai/DeepSeek-OCR"
  },
  {
    "title": "OmniVinci: Enhancing Architecture and Data for Omni-Modal Understanding LLM",
    "venue": "arXiv",
    "date": "2025",
    "link": "https://arxiv.org/pdf/2510.15870",
    "source": "auto-script",
    "tags": [
      "Instruction Tuning"
    ],
    "authors": "Hanrong Ye, Chao-Han Huck Yang, Arushi Goel, Wei Huang, Ligeng Zhu, et al.",
    "description": "OmniVinci 是由 NVIDIA 推出的开源全模态大语言模型，旨在像人类一样通过多种感官感知世界。该模型在架构上引入了 OmniAlignNet、时间嵌入分组 (Temporal Embedding Grouping) 等创新机制，强化了视觉、音频与语言在统一潜在空间中的对齐与交互。此外，团队还构建了包含 2400 万条数据的高质量全模态对话数据集。OmniVinci 在多项基准测试中展现了优异的性能，特别是在需要视听协同理解的复杂场景中表现突出。",
    "image": "",
    "code": "https://github.com/NVlabs/OmniVinci"
  },
  {
    "title": "NExT-OMNI: Towards Any-to-Any Omnimodal Foundation Models with Discrete Flow Matching",
    "venue": "arXiv",
    "date": "2025",
    "link": "https://arxiv.org/pdf/2510.13721",
    "source": "auto-script",
    "tags": [
      "Instruction Tuning"
    ],
    "authors": "Run Luo, Xiaobo Xia, Lu Wang, Longze Chen, Renke Shan, et al.",
    "description": "NExT-OMNI 是首个完全基于离散流匹配 (Discrete Flow Matching) 的开源全模态基础模型。该模型在统一架构下实现了文本、图像、视频和音频的“任意对任意”(Any-to-Any) 理解与生成，无需依赖扩散解码器。通过双向特征融合和动态长度生成优化，NExT-OMNI 在多模态生成、多轮交互及跨模态检索任务中展现了卓越性能与高效推理能力。",
    "image": "",
    "code": ""
  },
  {
    "title": "InteractiveOmni: A Unified Omni-modal Model for Audio-Visual Multi-turn Dialogue",
    "venue": "arXiv",
    "date": "2025",
    "link": "https://arxiv.org/pdf/2510.13747",
    "source": "auto-script",
    "tags": [
      "Instruction Tuning"
    ],
    "authors": "Wenwen Tong, Hewei Guo, Dongchuan Ran, Jiangnan Chen, Jiefan Lu, et al.",
    "description": "InteractiveOmni 是由商汤科技 (SenseTime Research) 推出的统一全模态大模型，专为视听多轮交互设计。该模型（4B-8B 参数）采用单一架构处理图像、视频、音频和文本，实现了端到端的全模态感知与语音生成。通过全模态预训练和后续的语音对话微调，InteractiveOmni 在多轮对话和长时记忆能力上表现出色，性能超越了同量级的开源模型，甚至媲美 Qwen2 等更大规模模型。",
    "image": "",
    "code": "https://github.com/SenseTime-FVG/InteractiveOmni"
  },
  {
    "title": "VITA-VLA: Efficiently Teaching Vision-Language Models to Act via Action Expert Distillation",
    "venue": "arXiv",
    "date": "2025",
    "link": "https://arxiv.org/pdf/2510.09607",
    "source": "auto-script",
    "tags": [
      "Instruction Tuning"
    ],
    "authors": "Shaoqi Dong, Chaoyou Fu, Haihan Guo, Yi-Fan Zhang, Chi Yan, et al.",
    "description": "VITA-VLA 是由南京大学、腾讯优图实验室及中科院自动化所联合提出的高效 VLA 训练框架，旨在解决传统视觉语言动作模型训练成本高昂的问题。该方法采用“动作专家蒸馏”策略，通过引入动作 Token 和状态编码器，将轻量级动作专家的知识迁移到大型 VLM 中。这种双阶段训练方式避免了昂贵的从头训练，在保留 VLM 强大感知能力的同时，使模型在 LIBERO 基准测试中达到了 97.3% 的平均成功率，在真实机器人操作任务中也表现优异。",
    "image": "",
    "code": "https://github.com/Tencent/VITA"
  },
  {
    "title": "LLaVA-OneVision-1.5: Fully Open Framework for Democratized Multimodal Training",
    "venue": "arXiv",
    "date": "2025",
    "link": "https://arxiv.org/pdf/2509.23661",
    "source": "auto-script",
    "tags": [
      "Instruction Tuning"
    ],
    "authors": "LLaVA-OneVision Community Contributors",
    "description": "LLaVA-OneVision-1.5 是一个完全开源的大型多模态模型 (LMM) 系列，旨在以极低的成本实现 SOTA 性能。该项目开源了包括 85M 预训练数据、26M 指令微调数据在内的所有资产，并提供了一套仅需 1.6 万美元即可复现的高效训练框架。在多个基准测试中，其 8B 和 4B 模型表现优异，超越了 Qwen2.5-VL 等同类模型。",
    "image": "",
    "code": "https://github.com/EvolvingLMMs-Lab/LLaVA-OneVision-1.5"
  },
  {
    "title": "Qwen3-Omni Technical Report",
    "venue": "arXiv",
    "date": "2025",
    "link": "https://arxiv.org/pdf/2509.17765",
    "source": "auto-script",
    "tags": [
      "Instruction Tuning"
    ],
    "authors": "Qwen Team",
    "description": "Qwen3-Omni 是由 Qwen 团队推出的原生端到端全模态基础模型，首次在文本、图像、音频和视频全模态上实现了 SOTA 性能，且未牺牲单模态能力。该模型采用 Thinker-Talker MoE 架构，支持实时语音对话和低延迟流式输出（首包延迟仅 234ms）。它支持 119 种语言的文本交互及 19 种语言的语音理解，并发布了专用的高精度音频描述模型 (Captioner)。",
    "image": "",
    "code": "https://github.com/QwenLM/Qwen3-Omni"
  },
  {
    "title": "InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency",
    "venue": "arXiv",
    "date": "2025",
    "link": "https://arxiv.org/pdf/2508.18265",
    "source": "auto-script",
    "tags": [
      "Instruction Tuning"
    ],
    "authors": "Weiyun Wang, Zhangwei Gao, Lixin Gu, Hengjun Pu, Long Cui, et al.",
    "description": "InternVL 3.5 是由 InternVL Team (上海人工智能实验室) 推出的新一代开源多模态模型系列。该系列引入了级联强化学习 (Cascade RL) 框架，通过“离线预热+在线精调”的策略显著增强了推理能力。同时，模型配备了视觉分辨率路由器 (ViR) 和解耦视觉-语言部署 (DvD) 策略，实现了推理效率的 4 倍提升。其中最大的 241B 模型在多项多模态及 Agent 基准测试中取得了开源 SOTA 成绩，逼近闭源模型 GPT-5。",
    "image": "",
    "code": "https://github.com/OpenGVLab/InternVL"
  },
  {
    "title": "Thyme: Think Beyond Images",
    "venue": "arXiv",
    "date": "2025",
    "link": "https://arxiv.org/pdf/2508.11630",
    "source": "auto-script",
    "tags": [
      "Instruction Tuning"
    ],
    "authors": "Yi-Fan Zhang, Xingyu Lu, Shukang Yin, Chaoyou Fu, Wei Chen, et al.",
    "description": "Thyme (Think Beyond Images) 是由快手 (Kwai-Keye) 联合中科院自动化所、南京大学、清华大学及中科大共同提出的多模态新范式。该模型旨在让模型通过自主生成和执行代码来完成图像处理和复杂数学计算。模型采用“SFT + RL”两阶段训练，引入 GRPO-ATS 算法平衡探索与执行精度，显著提升了高分辨率感知和复杂推理任务的性能。",
    "image": "",
    "code": "https://github.com/yfzhang114/Thyme"
  },
  {
    "title": "Introducing GPT-5",
    "venue": "OpenAI",
    "date": "2025",
    "link": "https://openai.com/index/introducing-gpt-5/",
    "source": "auto-script",
    "tags": [
      "Instruction Tuning",
      "Foundation Models"
    ],
    "authors": "OpenAI",
    "description": "GPT-5 是 OpenAI 于 2025 年 8 月推出的新一代旗舰模型系列。该模型首创了“多模型实时路由”架构 (Multi-model Real-time Router)，能根据用户指令的复杂度，在极速模型 (gpt-5-main) 和深度推理模型 (gpt-5-thinking) 之间毫秒级切换，从而平衡响应速度与逻辑深度。GPT-5 统一了 o 系列的强化推理能力与 GPT-4o 的全模态（文本、音频、视频）理解能力，并在代码生成、复杂创意写作及医疗健康咨询等任务上确立了新的 SOTA 标准。此外，它引入了“安全补全” (Safe Completions) 机制，在保证安全的前提下提供更具建设性的回答，而非生硬拒绝。",
    "image": "",
    "code": ""
  },
  {
    "title": "dots.vlm1",
    "venue": "rednote-hilab",
    "date": "2025",
    "link": "https://github.com/rednote-hilab/dots.vlm1",
    "source": "auto-script",
    "tags": [
      "Instruction Tuning"
    ],
    "authors": "Rednote HiLab",
    "description": "dots.vlm1 是 Rednote HiLab 推出的首个开源视觉语言模型，属于 dots 模型家族。该模型结合了完全从头训练的 1.2B NaViT 视觉编码器与 DeepSeek V3 语言模型，原生支持动态分辨率及纯视觉监督训练。通过引入大规模合成数据（如图表、文档）及图文交错数据重写技术，dots.vlm1 在保持纯文本能力的同时，在视觉感知与推理任务上刷新了开源模型的性能天花板，特别是在 OCR 和结构化图像理解方面表现优异。",
    "image": "",
    "code": "https://github.com/rednote-hilab/dots.vlm1"
  },
  {
    "title": "Step3: Cost-Effective Multimodal Intelligence",
    "venue": "StepFun",
    "date": "2025",
    "link": "https://stepfun.ai/research/step3",
    "source": "auto-script",
    "tags": [
      "Instruction Tuning"
    ],
    "authors": "StepFun",
    "description": "Step-3 是由阶跃星辰 (StepFun) 推出的新一代千亿参数级多模态 MoE 模型（321B 总参数 / 38B 激活）。该模型采用了创新的 Attention-FFN 分离 (AFD) 架构与多矩阵分解注意力 (MFA) 机制，在大幅降低显存占用与推理成本的同时，实现了极高的训练稳定性。Step-3 具备原生万亿 token 级别的长上下文理解能力，并深度集成了搜索引擎 (Search-augmented)，在复杂逻辑推理、代码生成及实时信息检索任务上表现优异，综合性能对标 GPT-4o。",
    "image": "",
    "code": "https://github.com/stepfun-ai/Step3"
  },
  {
    "title": "GLM-4.1V-Thinking: Towards Versatile Multimodal Reasoning with Scalable Reinforcement Learning",
    "venue": "arXiv",
    "date": "2025",
    "link": "https://arxiv.org/pdf/2507.01006",
    "source": "auto-script",
    "tags": [
      "Instruction Tuning"
    ],
    "authors": "Guo Wang, Guobing Gan, Haomiao Tang, Jiale Cheng, Ji Qi, et al.",
    "description": "GLM-4.5V 和 GLM-4.1V-Thinking 是由 GLM-V Team (智谱 AI & 清华大学) 推出的通用多模态推理模型系列。该系列包含 106B 参数的 GLM-4.5V（支持思考与非思考双模式）和 9B 参数的 GLM-4.1V-Thinking。模型采用了以推理为中心的训练框架，引入了课程采样强化学习 (RLCS) 策略，显著提升了在 STEM、GUI Agent 及长文档理解等任务上的性能。实验表明，GLM-4.5V 在多项基准测试中超越了 Step-3 和 Qwen2.5-VL-72B，逼近 Gemini-2.5-Flash。",
    "image": "",
    "code": "https://github.com/zai-org/GLM-V"
  },
  {
    "title": "DenseWorld-1M: Towards Detailed Dense Grounded Caption in the Real World",
    "venue": "arXiv",
    "date": "2025",
    "link": "https://arxiv.org/pdf/2506.24102",
    "source": "auto-script",
    "tags": [
      "Instruction Tuning"
    ],
    "authors": "Xiangtai Li, Tao Zhang, Yanwei Li, Haobo Yuan, Shihao Chen, et al.",
    "description": "DenseWorld-1M 是由 ByteDance Seed 联合武汉大学和北京大学推出的首个大规模真实世界密集定位描述 (Dense Grounded Caption) 数据集。该项目设计了包含开放世界感知、详细对象描述生成和密集描述合并的三阶段自动化标注流水线，构建了包含 100 万张图像、2310 万条对象描述和 2360 万个对象掩码的宏大资源。基于此数据集训练的模型在 RefCOCO 等指代分割任务及 MMBench 等多模态理解基准上均取得了显著的性能提升。",
    "image": "",
    "code": "https://github.com/lxtGH/DenseWorld-1M"
  },
  {
    "title": "Qwen VLo: From \"Understanding\" the World to \"Depicting\" It",
    "venue": "Qwen",
    "date": "2025",
    "link": "https://qwenlm.github.io/blog/qwen-vlo/",
    "source": "auto-script",
    "tags": [
      "Instruction Tuning"
    ],
    "authors": "Qwen Team",
    "description": "Qwen VLo 是由 Qwen Team 推出的统一多模态理解与生成模型，旨在弥合感知与创造之间的鸿沟。该模型不仅继承了 Qwen 系列强大的图像理解能力，还引入了渐进式生成机制 (Progressive Generation)，能够像人类画家一样从左到右、从上到下逐步构建图像，从而实现更精准的语义一致性和细节控制。Qwen VLo 支持开放式自然语言指令编辑（如风格迁移、对象增删、背景替换）及多语言交互，并能通过简单的编辑指令完成深度图预测、分割等传统视觉感知任务。目前该模型为预览版本，可通过 Qwen Chat 体验。",
    "image": "",
    "code": "https://github.com/QwenLM/Qwen-Image"
  },
  {
    "title": "MMSearch-R1: Incentivizing LMMs to Search",
    "venue": "arXiv",
    "date": "2025",
    "link": "https://arxiv.org/pdf/2506.20670",
    "source": "auto-script",
    "tags": [
      "Instruction Tuning"
    ],
    "authors": "Jinming Wu, Zihao Deng, Wei Li, Yiding Liu, Bo You, et al.",
    "description": "MMSearch-R1 是由字节跳动 (ByteDance) 和南洋理工大学 (NTU) 联合提出的多模态搜索模型，旨在通过 R1 范式（Reinforcement Learning with Verifiable Rewards）激励多模态大模型主动进行搜索，从而解决知识密集型任务中的幻觉问题。该研究构建了大规模的多模态搜索指令数据集 LMM-Search（包含 113k 样本），并训练了专门的奖励模型来优化搜索过程。MMSearch-R1 在需要外部知识的视觉问答任务上显著超越了传统的 RAG 方法和 GPT-4o 等闭源模型。",
    "image": "",
    "code": "https://github.com/EvolvingLMMs-Lab/multimodal-search-r1"
  },
  {
    "title": "Show-o2: Improved Native Unified Multimodal Models",
    "venue": "arXiv",
    "date": "2025",
    "link": "https://arxiv.org/pdf/2506.15564",
    "source": "auto-script",
    "tags": [
      "Instruction Tuning"
    ],
    "authors": "Jinheng Xie, Zhenheng Yang, Mike Zheng Shou",
    "description": "Show-o2 是由新加坡国立大学 (NUS Show Lab) 和字节跳动 (ByteDance) 联合提出的改进版原生统一多模态模型。该模型基于 3D 因果变分自编码器 (Causal VAE) 空间，通过“空间-时间融合”双路径构建统一的视觉表示，有效解决了图像与视频模态的扩展性问题。Show-o2 在单一 Transformer 架构中原生结合了自回归建模 (针对文本) 和流匹配 (Flow Matching, 针对视觉生成)，实现了文本、图像和视频的“任意对任意”理解与生成。相比前代，Show-o2 采用了两阶段训练策略，在多模态理解和生成任务上均表现出更强的通用性和性能。",
    "image": "",
    "code": "https://github.com/showlab/Show-o"
  },
  {
    "title": "Gemini 2.5: Pushing the Frontier with Advanced Reasoning, Multimodality, Long Context, and Next Generation Agentic Capabilities",
    "venue": "Google",
    "date": "2025",
    "link": "https://storage.googleapis.com/deepmind-media/gemini/gemini_v2_5_report.pdf",
    "source": "auto-script",
    "tags": [
      "Instruction Tuning"
    ],
    "authors": "Google Gemini Team",
    "description": "Gemini 2.5 是 Google DeepMind 推出的新一代原生多模态 MoE 模型系列，包含 Gemini 2.5 Pro 和 Gemini 2.5 Flash。该系列引入了“思维链” (Thinking) 能力，使其在处理复杂推理和代码任务时表现卓越。Gemini 2.5 Pro 支持长达 3 小时的视频输入及百万级 Token 上下文，并在编码、多模态理解和 Agent 任务上刷新了 SOTA 标准。此外，模型采用了稀疏混合专家 (MoE) 架构，在保持高性能的同时显著提升了训练稳定性和推理效率。",
    "image": "",
    "code": "https://ai.google.dev/gemini-api/docs/models/gemini-v2-5"
  },
  {
    "title": "Ego-R1: Chain-of-Tool-Thought for Ultra-Long Egocentric Video Reasoning",
    "venue": "arXiv",
    "date": "2025",
    "link": "https://arxiv.org/pdf/2506.13654",
    "source": "auto-script",
    "tags": [
      "Instruction Tuning"
    ],
    "authors": "Shulin Tian, Ruiqi Wang, Hongming Guo, Penghao Wu, Yuhao Dong, et al.",
    "description": "Ego-R1 是由南洋理工大学 (S-Lab) 联合新加坡 A*STAR、西蒙弗雷泽大学 (SFU) 及上海人工智能实验室共同提出的针对超长自我中心视频 (Egocentric Video) 的推理框架。针对第一视角视频时长极长（可达数天）且包含大量冗余信息的挑战，该论文提出了“工具思维链” (Chain-of-Tool-Thought, CoTT) 范式。该方法无需训练，而是利用大语言模型作为中枢，通过多步推理动态调用外部工具（如分层 RAG 检索、VLM 视觉问答），从海量视频数据中精准定位关键片段并回答复杂问题。Ego-R1 在 EgoSchema 等长视频理解基准测试中展现了优越的性能。",
    "image": "",
    "code": "https://github.com/egolife-ai/Ego-R1"
  },
  {
    "title": "MiMo-VL Technical Report",
    "venue": "arXiv",
    "date": "2025",
    "link": "https://arxiv.org/pdf/2506.03569",
    "source": "auto-script",
    "tags": [
      "Instruction Tuning"
    ],
    "authors": "Zihao Yue, Zhenru Lin, Yifan Song, Weikun Wang, Shuhuai Ren, et al.",
    "description": "MiMo-VL 是由小米 (Xiaomi LLM-Core) 推出的高性能视觉语言模型系列，包含 MiMo-VL-7B-SFT 和 MiMo-VL-7B-RL 两个版本。该模型采用了四阶段预训练策略（消耗 2.4T Token）及创新的混合在线强化学习 (Mixed On-policy RL, MORL) 框架，有效整合了不同模态的奖励信号。MiMo-VL-7B-RL 在 40 项基准测试中有 35 项超越了 Qwen2.5-VL-7B，特别是在 OlympiadBench (59.4分) 及 OSWorld-G (56.1分) 等高难度推理和 GUI Agent 任务上表现卓越，甚至优于部分 70B+ 参数的模型。",
    "image": "",
    "code": "https://github.com/XiaomiMiMo/MiMo-VL"
  },
  {
    "title": "OpenUni: A Simple Baseline for Unified Multimodal Understanding and Generation",
    "venue": "arXiv",
    "date": "2025",
    "link": "https://arxiv.org/pdf/2505.23661",
    "source": "auto-script",
    "tags": [
      "Instruction Tuning"
    ],
    "authors": "Size Wu, Zhonghua Wu, Zerui Gong, Qingyi Tao, Sheng Jin, et al.",
    "description": "OpenUni 是由南洋理工大学 (S-Lab) 和商汤科技 (SenseTime) 联合提出的轻量级、全开源的统一多模态理解与生成基准模型。该模型采用极简架构，通过一组可学习查询和轻量级 Transformer 连接器，将现成的多模态大模型与扩散模型高效连接，从而最小化训练开销。尽管仅有 1.1B 和 3.1B 激活参数，OpenUni 能够生成高质量且符合指令的图像，并在 GenEval、DPG-Bench 和 WISE 等基准测试中表现出色。该项目完全开源，提供了模型权重、代码及包含 2300 万图文对的高质量训练数据集。",
    "image": "",
    "code": "https://github.com/wusize/OpenUni"
  },
  {
    "title": "Emerging Properties in Unified Multimodal Pretraining",
    "venue": "arXiv",
    "date": "2025",
    "link": "https://arxiv.org/pdf/2505.14683",
    "source": "auto-script",
    "tags": [
      "Instruction Tuning"
    ],
    "authors": "Chaorui Deng, Deyao Zhu, Kunchang Li, Chenhui Gou, Feng Li, et al.",
    "description": "BAGEL 是由字节跳动 (ByteDance Seed) 联合深圳先进院 (SIAT)、蒙纳士大学、香港科技大学及加州大学圣克鲁兹分校共同推出的开源统一多模态基础模型（14B 总参数/7B 激活参数）。该模型采用混合专家 (MoE) 架构，在海量交错图文视频数据上进行预训练，原生支持多模态理解与生成。BAGEL 展现了复杂的“涌现”推理能力，包括自由形式的图像编辑、未来帧预测、3D 操控及世界导航，其性能在理解与生成任务上均显著优于同类开源模型。",
    "image": "",
    "code": "https://github.com/ByteDance-Seed/Bagel"
  },
  {
    "title": "MMaDA: Multimodal Large Diffusion Language Models",
    "venue": "arXiv",
    "date": "2025",
    "link": "https://arxiv.org/pdf/2505.15809",
    "source": "auto-script",
    "tags": [
      "Instruction Tuning"
    ],
    "authors": "Ling Yang, Ye Tian, Bowen Li, Xinchen Zhang, Ke Shen, et al.",
    "description": "MMaDA 是由普林斯顿大学、北京大学、清华大学和字节跳动 (ByteDance Seed) 联合提出的一种新型多模态扩散基础模型。该模型采用统一的扩散架构 (Unified Diffusion Architecture)，通过共享的概率公式和模态无关设计，消除了特定模态组件的限制。MMaDA 引入了混合长思维链 (Mixed Long-CoT) 微调策略来对齐文本与视觉推理，并提出了 UniGRPO（统一策略梯度强化学习算法）以优化后续训练。该模型在文本推理、多模态理解及文生图任务上均表现出色，超越了 Show-o 和 SEED-X 等同类模型。",
    "image": "",
    "code": "https://github.com/Gen-Verse/MMaDA"
  },
  {
    "title": "UniGen: Enhanced Training & Test-Time Strategies for Unified Multimodal Understanding and Generation",
    "venue": "arXiv",
    "date": "2025",
    "link": "https://arxiv.org/pdf/2505.14682",
    "source": "auto-script",
    "tags": [
      "Instruction Tuning"
    ],
    "authors": "Rui Tian, Mingfei Gao, Mingze Xu, Jiaming Hu, Jiasen Lu, et al.",
    "description": "UniGen 是由 Apple 和复旦大学联合提出的统一多模态大模型。该模型从数据为中心的角度出发，优化了多阶段预训练、监督微调 (SFT) 和直接偏好优化 (DPO) 的全流程。关键创新在于提出了“思维链验证” (Chain-of-Thought Verification, CoT-V) 策略，使模型在测试时既作为生成器又作为验证器，通过 CoT 方式评估生成的图文一致性，从而利用 Best-of-N 策略显著提升图像生成质量。UniGen 仅使用开源数据训练，在 GenEval (0.78) 和 DPG-Bench (85.19) 等基准上达到了 SOTA 性能。",
    "image": "",
    "code": "https://github.com/apple/ml-unigen"
  },
  {
    "title": "BLIP3-o: A Family of Fully Open Unified Multimodal Models-Architecture, Training and Dataset",
    "venue": "arXiv",
    "date": "2025",
    "link": "https://arxiv.org/pdf/2505.09568",
    "source": "auto-script",
    "tags": [
      "Instruction Tuning"
    ],
    "authors": "Jiuhai Chen, Zhiyang Xu, Xichen Pan, Yushi Hu, Can Qin, et al.",
    "description": "BLIP3-0 (亦称 BLIP3-o) 是由 Salesforce Research 联合多所高校推出的完全开源统一多模态模型系列。该研究深入探索了自回归与扩散模型在统一架构中的应用，提出了一种利用 Diffusion Transformer 生成语义丰富的 CLIP 图像特征的新方法（替代传统 VAE），显著提升了生成质量和训练效率。BLIP3-0 验证了“顺序预训练”策略（先理解后生成）的有效性，并开源了全套模型权重、代码及包含 60k 高质量指令数据的 BLIP3o-60k 数据集。",
    "image": "",
    "code": "https://github.com/JiuhaiChen/BLIP3o"
  },
  {
    "title": "Seed1.5-VL Technical Report",
    "venue": "arXiv",
    "date": "2025",
    "link": "https://arxiv.org/pdf/2505.07062",
    "source": "auto-script",
    "tags": [
      "Instruction Tuning"
    ],
    "authors": "Dong Guo, Faming Wu, Feida Zhu, Fuxing Leng, Guang Shi, et al.",
    "description": "Seed1.5-VL 是由字节跳动 (ByteDance Seed) 推出的通用视觉语言基础模型。该模型由 532M 参数的视觉编码器和 20B 激活参数的混合专家 (MoE) 大语言模型组成。尽管架构相对紧凑，Seed1.5-VL 在 60 个公共基准测试中的 38 个上取得了 SOTA 性能。特别是在 GUI 控制和游戏玩法等 Agent 中心任务上，它超越了 OpenAI CUA 和 Claude 3.7 等顶级系统。此外，该模型在多模态推理（如视觉拼图）方面表现出强大的能力，能够处理动态分辨率图像和视频输入。",
    "image": "",
    "code": "https://github.com/ByteDance-Seed/Seed1.5-VL"
  },
  {
    "title": "Perception, Reason, Think, and Plan: A Survey on Large Multimodal Reasoning Models",
    "venue": "arXiv",
    "date": "2025",
    "link": "https://arxiv.org/pdf/2505.04921",
    "source": "auto-script",
    "tags": [
      "Instruction Tuning"
    ],
    "authors": "Yunxin Li, Zhenyu Liu, Zitao Li, Xuanyu Zhang, Zhenran Xu, et al.",
    "description": "这是一篇由哈尔滨工业大学（深圳）联合华为诺亚方舟实验室、腾讯等机构发表的关于大模型多模态推理 (LMRMs) 的权威综述。论文系统地回顾了多模态推理从传统的感知驱动管道向统一的语言中心框架的演进，深入探讨了包括指令微调、强化学习 (RL)、思维链 (CoT) 及搜索增强在内的核心技术。该综述不仅涵盖了文本、图像、音频、视频等全模态的集成方案，还详细分析了模型在感知、理解、推理及规划四个维度的能力评估，并指出了全模态理解、长程推理及实时交互等未来研究方向。",
    "image": "",
    "code": "https://github.com/HITsz-TMG/Awesome-Large-Multimodal-Reasoning-Models"
  },
  {
    "title": "VITA-Audio: Fast Interleaved Cross-Modal Token Generation for Efficient Large Speech-Language Model",
    "venue": "arXiv",
    "date": "2025",
    "link": "https://arxiv.org/pdf/2505.03739",
    "source": "auto-script",
    "tags": [
      "Instruction Tuning"
    ],
    "authors": "Zuwei Long, Yunhang Shen, Chaoyou Fu, Heting Gao, Lijiang Li, et al.",
    "description": "VITA-Audio 是由腾讯优图实验室联合南京大学、厦门大学提出的高效端到端语音大语言模型。针对现有模型在流式生成中首个音频 Token 延迟较高的问题，该研究引入了轻量级的“多跨模态 Token 预测”(MCTP) 模块，允许模型在单次前向传播中生成多个音频 Token，大幅降低了首包延迟并加速了整体推理。此外，论文探索了四阶段渐进式训练策略，在几乎不损失语音质量的前提下实现了显著的性能提升。VITA-Audio 是首个支持快速交错图文音频生成的开源系统之一。",
    "image": "",
    "code": "https://github.com/VITA-MLLM/VITA-Audio"
  },
  {
    "title": "Skywork R1V2: Multimodal Hybrid Reinforcement Learning for Reasoning",
    "venue": "arXiv",
    "date": "2025",
    "link": "https://arxiv.org/pdf/2504.16656",
    "source": "auto-script",
    "tags": [
      "Instruction Tuning"
    ],
    "authors": "Chris, Yichen Wei, Yi Peng, Xiaokun Wang, Weijie Qiu, et al.",
    "description": "Skywork R1V2 是由昆仑万维 (Skywork AI) 提出的新一代多模态推理模型（Skywork-R1V 的升级版）。该模型引入了“混合强化学习” (Hybrid RL) 范式，联合使用混合偏好优化 (MPO) 和组相对策略优化 (GRPO)，在平衡复杂推理能力与广泛泛化性方面取得了突破。针对 GRPO 中的优势消失问题，R1V2 提出了选择性样本缓冲 (SSB) 机制。实验显示，R1V2 在 OlympiadBench (62.6分) 和 AIME2024 (78.9分) 等高难度数学与推理基准上表现优异，并有效缓解了强化学习可能引发的视觉幻觉问题。",
    "image": "",
    "code": "https://github.com/SkyworkAI/Skywork-R1V"
  },
  {
    "title": "Eagle 2.5: Boosting Long-Context Post-Training for Frontier Vision-Language Models",
    "venue": "arXiv",
    "date": "2025",
    "link": "https://arxiv.org/pdf/2504.15271",
    "source": "auto-script",
    "tags": [
      "Instruction Tuning"
    ],
    "authors": "Guo Chen, Zhiqi Li, Shihao Wang, Jindong Jiang, Yicheng Liu, et al.",
    "description": "Eagle 2.5 是由 NVIDIA 推出的前沿视觉语言模型系列，专注于提升长上下文多模态学习能力。该研究提出了一套通用的后训练框架，引入了自动降级采样 (Automatic Degrade Sampling) 和图像区域保留 (Image Area Preservation) 技术，有效解决了长视频理解和高分辨率图像处理中的细节丢失问题。此外，该工作还发布了 Eagle-Video-110K 数据集，包含故事级和片段级注释。Eagle 2.5-8B 在 Video-MME 基准上（512 帧输入）达到 72.4% 的准确率，性能对标 GPT-4o 及 Qwen2-VL-72B 等顶尖模型。",
    "image": "",
    "code": "https://github.com/NVlabs/Eagle"
  },
  {
    "title": "An LMM for Efficient Video Understanding via Reinforced Compression of Video Cubes",
    "venue": "arXiv",
    "date": "2025",
    "link": "https://arxiv.org/pdf/2504.15270",
    "source": "auto-script",
    "tags": [
      "Instruction Tuning"
    ],
    "authors": "Ji Qi, Yuan Yao, Yushi Bai, Bin Xu, Juanzi Li, et al.",
    "description": "Quicksviewer 是由清华大学和新加坡国立大学联合提出的高效视频理解多模态大模型。针对传统模型将视频视为静态帧序列导致的计算冗余问题，该模型提出了一种基于强化学习的“视频立方体压缩” (Reinforced Compression of Video Cubes) 机制。它利用“立方体网络” (Cubing Network) 将视频自适应地划分为非均匀的 3D 块，并通过“3D 重采样器”提取最具信息量的视觉 Token。这种设计使得 Quicksviewer 具备了测试时缩放 (Test-Time Scaling) 能力，能够根据计算预算动态调整处理帧率，在 Video-MME 等基准上实现了效率与性能的最佳平衡。",
    "image": "",
    "code": "https://github.com/Quicksviewer/Quicksviewer"
  },
  {
    "title": "InternVL3: Exploring Advanced Training and Test-Time Recipes for Open-Source Multimodal Models",
    "venue": "arXiv",
    "date": "2025",
    "link": "https://arxiv.org/abs/2504.10479",
    "source": "auto-script",
    "tags": [
      "Instruction Tuning"
    ],
    "authors": "Jinguo Zhu, Weiyun Wang, Zhe Chen, Zhaoyang Liu, Shenglong Ye, et al.",
    "description": "InternVL3 是由上海人工智能实验室 (Shanghai AI Laboratory) 联合商汤科技、清华大学等多家机构推出的新一代开源多模态大模型。该研究系统性地探索了“训练时”与“测试时”的高级策略，旨在弥合开源模型与顶尖闭源模型之间的差距。InternVL3 不仅优化了训练配方，还重点引入了测试时计算 (Test-Time Compute) 扩展技术，使其在处理复杂多模态推理任务时表现出卓越的性能。该系列模型及代码已完全开源，是目前最强的开源多模态基座之一。",
    "image": "",
    "code": "https://github.com/OpenGVLab/InternVL"
  },
  {
    "title": "Introducing GPT-4.1 in the API",
    "venue": "OpenAI",
    "date": "2025",
    "link": "https://openai.com/index/gpt-4-1/",
    "source": "auto-script",
    "tags": [
      "Instruction Tuning"
    ],
    "authors": "OpenAI",
    "description": "GPT-4.1 是 OpenAI 于 2025 年 4 月 14 日发布的更高效、更具成本效益的大语言模型。作为 GPT-4 系列的优化版本，它在 GPT-4.5 发布后不久推出，旨在平衡高性能与推理成本，并逐步接替了部分 GPT-4.5 的 API 服务。GPT-4.1 在保持强大多模态能力的同时，优化了响应速度和指令遵循能力，是 OpenAI 从 GPT-4 时代向 GPT-5（2025 年 8 月发布）过渡期间的关键模型之一。",
    "image": "",
    "code": ""
  },
  {
    "title": "Kimi-VL Technical Report",
    "venue": "arXiv",
    "date": "2025",
    "link": "https://arxiv.org/pdf/2504.07491",
    "source": "auto-script",
    "tags": [
      "Instruction Tuning"
    ],
    "authors": "Kimi Team",
    "description": "Kimi-VL 是由月之暗面 (Moonshot AI) 推出的高效开源混合专家 (MoE) 视觉语言模型。该模型虽然仅激活 2.8B 参数（总参数 16B），但在多模态推理、长上下文理解及 Agent 能力上表现卓越。Kimi-VL 配备了原生分辨率视觉编码器 MoonViT 和 128K 超长上下文窗口，在 LongVideoBench (64.5) 和 ScreenSpot-Pro (34.5) 等基准上取得了 SOTA 成绩。此外，该系列还包含 Kimi-VL-Thinking 版本，通过长思维链 (CoT) 微调进一步增强了数学和逻辑推理能力。模型权重及代码已完全开源。",
    "image": "",
    "code": "https://github.com/MoonshotAI/Kimi-VL"
  },
  {
    "title": "The Llama 4 herd: The beginning of a new era of natively multimodal AI innovation",
    "venue": "Meta",
    "date": "2025",
    "link": "https://ai.meta.com/blog/llama-4-multimodal-intelligence/",
    "source": "auto-script",
    "tags": [
      "Instruction Tuning"
    ],
    "authors": "Meta AI",
    "description": "Llama 4 是 Meta 于 2025 年 4 月发布的最新一代开源基础模型系列，首次采用了混合专家 (MoE) 架构并原生支持多模态（文本、图像、视频、音频）。该系列包含两个已发布版本：**Llama 4 Scout**（109B 总参数/17B 激活，支持 10M 上下文，专为长文档和代码优化）和 **Llama 4 Maverick**（400B 总参数/17B 激活，128 个专家，擅长多语言和通用助手任务）。此外，Meta 还预告了仍在训练中的旗舰模型 **Llama 4 Behemoth**（约 2T 参数）。Llama 4 引入了 iRoPE 等新技术以支持超长上下文，并保持了开放权重的传统。",
    "image": "",
    "code": "https://github.com/meta-llama/llama-models"
  },
  {
    "title": "Qwen2.5-Omni Technical Report",
    "venue": "Qwen",
    "date": "2025",
    "link": "https://github.com/QwenLM/Qwen2.5-Omni/blob/main/assets/Qwen2.5_Omni.pdf",
    "source": "auto-script",
    "tags": [
      "Instruction Tuning"
    ],
    "authors": "Qwen Team",
    "description": "Qwen2.5-Omni 是由阿里云 Qwen 团队推出的端到端全模态模型。该模型采用了创新的 **Thinker-Talker** 架构：Thinker（基于 Qwen2.5-7B）负责多模态理解与推理，而 Talker 专注于流式语音生成。通过引入 **TMRoPE** (Time-aligned Multimodal RoPE) 位置编码，模型实现了视频与音频输入的精确时间同步。Qwen2.5-Omni 支持文本、图像、视频和音频的任意组合输入，并能进行低延迟的实时语音对话（无需外部 TTS），在 Omni-Bench 及 MMLU (语音版) 等基准测试中表现优异，是开源界首个具备 GPT-4o 级实时语音交互能力的模型之一。",
    "image": "",
    "code": "https://github.com/QwenLM/Qwen2.5-Omni"
  },
  {
    "title": "Addendum to GPT-4o System Card: Native image generation",
    "venue": "OpenAI",
    "date": "2025",
    "link": "https://cdn.openai.com/11998be9-5319-4302-bfbf-1167e093f1fb/Native_Image_Generation_System_Card.pdf",
    "source": "auto-script",
    "tags": [
      "Instruction Tuning"
    ],
    "authors": "OpenAI",
    "description": "这份报告是 GPT-4o 系统卡片的增补版，详细介绍了 GPT-4o 的原生图像生成能力。与之前的 DALL-E 系列（基于扩散模型）不同，GPT-4o 采用自回归架构，原生集成在模型中。这种架构不仅支持高质量的文生图，还实现了强大的“图生图”变换能力和精准的文字渲染。报告重点讨论了该技术带来的新安全挑战（如非自愿色情内容、版权问题等）以及 OpenAI 采取的红队测试和缓解措施。",
    "image": "",
    "code": ""
  },
  {
    "title": "Sparrow: Data-Efficient Video-LLM with Text-to-Image Augmentation",
    "venue": "arXiv",
    "date": "2025",
    "link": "https://arxiv.org/pdf/2411.19951",
    "source": "auto-script",
    "tags": [
      "Instruction Tuning"
    ],
    "authors": "Shukang Yin, Chaoyou Fu, Sirui Zhao, Chunjiang Ge, Yan Yang, et al.",
    "description": "Sparrow 是由中国科学技术大学 (USTC) 联合南京大学、清华大学及厦门大学提出的高效视频大模型训练方法。针对现有视频多模态模型 (Video-LLMs) 严重依赖海量视频数据且训练效率低下的问题，该研究提出了一种反直觉的数据增强策略：利用文生图 (Text-to-Image) 模型将纯文本的指令-回复对（如长文档问答）转化为“伪视频”数据。这种方法极大地丰富了训练数据的指令多样性，使得模型在仅使用少量真实视频数据的情况下，即可在 Video-MME 等基准上达到甚至超越使用海量数据训练的基线模型，有效挑战了传统的“数据量即正义”的扩展定律。",
    "image": "",
    "code": "https://github.com/VITA-MLLM/Sparrow"
  },
  {
    "title": "Nexus-O: An Omni-Perceptive And -Interactive Model for Language, Audio, And Vision",
    "venue": "arXiv",
    "date": "2025",
    "link": "https://arxiv.org/pdf/2503.01879",
    "source": "auto-script",
    "tags": [
      "Instruction Tuning"
    ],
    "authors": "Che Liu, Ziao Wang, Yingji Zhang, Yu Lu, Shilin Zhou, et al.",
    "description": "NEXUS-O 是由帝国理工学院 (Imperial College London)、同花顺 (HiThink Research) 和复旦大学联合提出的行业级全模态（Omni-modal）大模型。该模型旨在解决三模态（语言、音频、视觉）数据稀缺和对齐困难的问题。NEXUS-O 采用模块化的端到端架构，基于 Qwen2.5-VL 进行轻量级音频-语言对齐，无需昂贵的视觉预训练即可实现强大的三模态理解与交互能力。此外，研究团队还构建了可扩展的合成音频数据管线，生成了大量高质量的音频-文本对，支持 ASR 和实时语音对话等任务。",
    "image": "",
    "code": "https://github.com/HiThink-Research/NEXUS-O"
  },
  {
    "title": "Phi-4-Mini Technical Report: Compact yet Powerful Multimodal Language Models via Mixture-of-LoRAs",
    "venue": "arXiv",
    "date": "2025",
    "link": "https://arxiv.org/pdf/2503.01743",
    "source": "auto-script",
    "tags": [
      "Instruction Tuning"
    ],
    "authors": "Microsoft",
    "description": "Phi-4-Mini 技术报告详细介绍了 Microsoft Phi 系列的两个新成员：Phi-4-Mini 和 Phi-4-Multimodal。Phi-4-Mini 是一个 3.8B 参数的高效语言模型，通过扩展词汇表（200K）和高质量合成数据训练，在数学和编码任务上匹敌大一倍的模型。Phi-4-Multimodal (5.6B) 则采用创新的“LoRA 混合专家” (Mixture-of-LoRAs) 架构，利用特定模态的路由和适配器，将文本、视觉和语音/音频模态集成到单一模型中，解决了多模态干扰问题。该模型支持文本、图像和音频的任意组合输入，并在 OpenASR 等语音基准上取得了 SOTA 性能。",
    "image": "",
    "code": "https://github.com/microsoft/PhiCookBook"
  },
  {
    "title": "Long-VITA: Scaling Large Multi-modal Models to 1 Million Tokens with Leading Short-Context Accuray",
    "venue": "arXiv",
    "date": "2025",
    "link": "https://arxiv.org/pdf/2502.05177",
    "source": "auto-script",
    "tags": [
      "Instruction Tuning"
    ],
    "authors": "Yunhang Shen, Chaoyou Fu, Shaoqi Dong, Xiong Wang, Yi-Fan Zhang, et al.",
    "description": "Long-VITA 是由腾讯优图实验室 (Tencent Youtu Lab) 联合南京大学、厦门大学等机构提出的长上下文多模态大模型。该模型旨在同时实现超长上下文理解与领先的短上下文性能。Long-VITA 支持高达 100 万 Token（或超过 4000 帧视频）的输入，通过特殊的“两阶段长序列微调”策略和推理时的“上下文并行”技术，有效解决了长文本/长视频处理中的显存和计算瓶颈。令人印象深刻的是，它在显著扩展上下文窗口的同时，未牺牲在常规短上下文任务上的精度，且仅使用了 1700 万条开源数据进行训练。",
    "image": "",
    "code": "https://github.com/VITA-MLLM/Long-VITA"
  },
  {
    "title": "Qwen2.5-VL Technical Report",
    "venue": "arXiv",
    "date": "2025",
    "link": "https://arxiv.org/pdf/2502.13923",
    "source": "auto-script",
    "tags": [
      "Instruction Tuning"
    ],
    "authors": "Qwen Team",
    "description": "Qwen2.5-VL 是阿里云 (Alibaba Cloud) Qwen 团队推出的旗舰级视觉语言模型。该模型通过引入“原生动态分辨率” (Native Dynamic Resolution) 机制，能够处理任意长宽比的图像和长达数小时的视频，且无需传统的填充操作。Qwen2.5-VL 升级了 M-RoPE (Multimodal Rotary Positional Embedding)，实现了视频与音频的绝对时间对齐，使其具备了强大的长视频事件定位能力。此外，该模型在视觉 Agent 任务（如操作手机和电脑）以及结构化文档提取（图表、发票）方面表现出 SOTA 性能，并支持通过边界框或点进行精确的对象定位。",
    "image": "",
    "code": "https://github.com/QwenLM/Qwen2.5-VL"
  },
  {
    "title": "Baichuan-Omni-1.5 Technical Report",
    "venue": "Tech Report",
    "date": "2025",
    "link": "https://github.com/baichuan-inc/Baichuan-Omni-1.5/blob/main/baichuan_omni_1_5.pdf",
    "source": "auto-script",
    "tags": [
      "Instruction Tuning"
    ],
    "authors": "Yadong Li, Jun Liu, Tao Zhang, Song Chen, Tianpeng Li, et al.",
    "description": "Baichuan-Omni-1.5 是由百川智能 (Baichuan Inc.) 推出的全模态 (Omni-modal) 基础模型。该模型支持文本、图像、视频和音频的任意组合输入，并具备端到端的音频生成能力。通过引入自研的 Baichuan-Audio-Tokenizer 和基于流匹配 (Flow Matching) 的解码器，模型实现了高质量的实时语音交互。Baichuan-Omni-1.5 在 OmniBench 上超越了 GPT-4o-mini，特别是在医疗多模态任务（如 OpenMM-Medical）上表现出色，准确率高达 83.8%，优于 Qwen2-VL-72B。",
    "image": "",
    "code": "https://github.com/baichuan-inc/Baichuan-Omni-1.5"
  },
  {
    "title": "LlamaV-o1: Rethinking Step-by-step Visual Reasoning in LLMs",
    "venue": "arXiv",
    "date": "2025",
    "link": "https://arxiv.org/pdf/2501.06186",
    "source": "auto-script",
    "tags": [
      "Instruction Tuning"
    ],
    "authors": "Omkar Thawakar, Dinura Dissanayake, Ketan More, Ritesh Thawkar, Ahmed Heakl, et al.",
    "description": "LlamaV-o1 是由 MBZUAI（穆罕默德·本·扎耶德人工智能大学）联合中佛罗里达大学推出的视觉推理模型。该研究旨在复刻 OpenAI o1 在视觉模态上的分步推理能力，提出了一种包含“从描述到推理 (Caption-to-Reasoning)”和“从推理到回答 (Reasoning-to-Answer)”的两阶段训练策略。配合该模型，团队构建了 VRC-Bench (Visual Reasoning Chain Benchmark) 以专门评估模型的视觉思维链质量。LlamaV-o1 通过显式的多步推理，在 MMMU-Pro 和 MathVista 等复杂任务上显著优于传统的直接回答模型。",
    "image": "",
    "code": "https://github.com/mbzuai-oryx/LlamaV-o1"
  },
  {
    "title": "VITA-1.5: Towards GPT-4o Level Real-Time Vision and Speech Interaction",
    "venue": "arXiv",
    "date": "2025",
    "link": "https://arxiv.org/pdf/2501.01957",
    "source": "auto-script",
    "tags": [
      "Instruction Tuning"
    ],
    "authors": "Chaoyou Fu, Haojia Lin, Xiong Wang, Yi-Fan Zhang, Yunhang Shen, et al.",
    "description": "VITA-1.5 是由南京大学联合腾讯优图实验室推出的全模态实时交互模型。该模型旨在复刻 GPT-4o 的即时视听互动体验，采用了端到端架构，无需外部 ASR（语音转文字）或 TTS（文字转语音）模块即可实现流畅的“语音-语音”对话。VITA-1.5 采用了精心设计的多阶段训练策略，渐进地融合视觉和语音模态，在保持强大图文理解能力的同时，显著提升了语音交互的实时性和自然度。代码仓库中还提供了详细的部署指南。",
    "image": "",
    "code": "https://github.com/VITA-MLLM/VITA"
  },
  {
    "title": "QVQ: To See the World with Wisdom",
    "venue": "Qwen",
    "date": "2024",
    "link": "https://qwenlm.github.io/blog/qvq-72b-preview/",
    "source": "auto-script",
    "tags": [
      "Instruction Tuning"
    ],
    "authors": "Qwen Team",
    "description": "QVQ-72B-Preview 是 Qwen 团队于 2024 年 12 月推出的实验性多模态推理模型。该模型基于 Qwen2-VL-72B 构建，旨在通过引入类似于思维链 (CoT) 的逐步推理机制，显著提升视觉理解与复杂问题解决能力。QVQ 在 MMMU 基准测试中取得了 70.3 的高分，在 MathVista 和 OlympiadBench 等数学与科学推理任务上也表现卓越，有效缩小了与 o1 等顶尖模型的差距。作为一个预览版研究模型，它展示了多模态模型进行深度视觉推理的巨大潜力。",
    "image": "",
    "code": "https://huggingface.co/Qwen/QVQ-72B-Preview"
  },
  {
    "title": "DeepSeek-VL2: Mixture-of-Experts Vision-Language Models for Advanced Multimodal Understanding",
    "venue": "arXiv",
    "date": "2024",
    "link": "https://arxiv.org/pdf/2412.10302",
    "source": "auto-script",
    "tags": [
      "Instruction Tuning"
    ],
    "authors": "Zhiyu Wu, Xiaokang Chen, Zizheng Pan, Xingchao Liu, Wen Liu, et al.",
    "description": "DeepSeek-VL2 是由 DeepSeek-AI 推出的先进混合专家 (MoE) 视觉语言模型系列。该模型相比前代有两大核心升级：在视觉端，引入了“动态分块视觉编码” (Dynamic Tiling Vision Encoding) 策略，能够高效处理不同长宽比的高分辨率图像；在语言端，采用了集成 MLA (Multi-head Latent Attention) 机制的 DeepSeekMoE 架构，显著压缩了 KV 缓存并提升了推理吞吐量。模型包含 Tiny、Small 和 Large 三个版本，在视觉问答、OCR、文档/图表理解及视觉定位 (Visual Grounding) 等任务上表现卓越。",
    "image": "",
    "code": "https://github.com/deepseek-ai/DeepSeek-VL2"
  },
  {
    "title": "Apollo: An Exploration of Video Understanding in Large Multimodal Models",
    "venue": "arXiv",
    "date": "2024",
    "link": "https://arxiv.org/pdf/2412.10360",
    "source": "auto-script",
    "tags": [
      "Instruction Tuning"
    ],
    "authors": "Orr Zohar, Xiaohan Wang, Yann Dubois, Nikhil Mehta, Tong Xiao, et al.",
    "description": "Apollo 是由 Meta GenAI 团队与斯坦福大学联合开展的一项关于多模态大模型 (LMMs) 视频理解能力的系统性研究。该工作旨在揭示驱动视频理解的核心机制，并解决视频模型训练和评估中计算成本过高的问题。研究提出了“缩放一致性” (Scaling Consistency) 发现，即在较小模型和数据集上做出的设计决策（如视频采样、架构选择、数据组合等）可以有效迁移至大模型。此外，论文证明了在训练期间采用按帧率 (fps) 采样而非均匀采样能显著提升性能。Apollo 系列模型和研究结论为构建更高效的视频 LMM 提供了重要的技术指引。",
    "image": "",
    "code": "https://github.com/facebookresearch/apollo"
  },
  {
    "title": "InternLM-XComposer2.5-OmniLive: A Comprehensive Multimodal System for Long-term Streaming Video and Audio Interactions",
    "venue": "arXiv",
    "date": "2024",
    "link": "https://arxiv.org/pdf/2412.09596",
    "source": "auto-script",
    "tags": [
      "Instruction Tuning"
    ],
    "authors": "Pan Zhang, Xiaoyi Dong, Yuhang Cao, Yuhang Zang, Rui Qian, et al.",
    "description": "InternLM-XComposer2.5-OmniLive 是由上海人工智能实验室联合香港中文大学、复旦大学、中国科学技术大学、清华大学等多所高校，以及商汤科技共同研发的多模态交互系统。该系统旨在实现类似人类的长期、连续、流式视听交互，突破了传统模型无法同时进行持续感知、记忆与推理的局限。通过引入“流式记忆”机制，系统能够实时处理长周期的视频与音频流，并提供低延迟的语音反馈。该成果在长程流式感知任务上表现卓越，为构建具备长期认知能力的 AI 助理提供了重要技术支撑。",
    "image": "",
    "code": "https://github.com/InternLM/InternLM-XComposer"
  },
  {
    "title": "StreamChat: Chatting with Streaming Video",
    "venue": "arXiv",
    "date": "2024",
    "link": "https://arxiv.org/pdf/2412.08646",
    "source": "auto-script",
    "tags": [
      "Instruction Tuning"
    ],
    "authors": "Jihao Liu, Zhiding Yu, Shiyi Lan, Shihao Wang, Rongyao Fang, et al.",
    "description": "StreamChat 是由香港中文大学 (CUHK MMLab)、NVIDIA、上海人工智能实验室以及香港理工大学联合研发的流式视频对话模型。该研究针对传统视频模型无法处理持续视频流且交互延迟高的问题，提出了一种全新的流式架构。StreamChat 能够像人类一样边看边说，在视频流输入的过程中同步生成文本回复，并根据最新的视频画面实时修正和补充内容。通过引入“动态查询平衡”等技术，该模型在保持高效计算的同时，显著提升了在长时间视频流中的感知精度与交互体验。",
    "image": "",
    "code": "https://github.com/NVlabs/StreamChat"
  },
  {
    "title": "CompCap: Improving Multimodal Large Language Models with Composite Captions",
    "venue": "arXiv",
    "date": "2024",
    "link": "https://arxiv.org/pdf/2412.05243",
    "source": "auto-script",
    "tags": [
      "Instruction Tuning"
    ],
    "authors": "Xiaohui Chen, Satya Narayan Shukla, Mahmoud Azab, Aashu Singh, Qifan Wang, et al.",
    "description": "CompCap 是由 Meta 联合塔夫茨大学 (Tufts University) 和佐治亚理工学院 (Georgia Tech) 等高校研发的视觉语言对齐框架。该研究聚焦于多模态大模型在“复合图像”（如包含图表、海报或截图的合成图像）理解上的短板。团队开发了 CompCap 自动化框架，利用大语言模型和工具合成带有精确、详细描述的复合图像数据集 CompCap-118K。实验表明，通过在这些高质量的复合图像描述数据上进行微调，模型在图表问答 (ChartQA) 和科学推理 (ScienceQA) 等复杂任务上的性能得到了显著提升。",
    "image": "",
    "code": ""
  },
  {
    "title": "LinVT: Empower Your Image-level Large Language Model to Understand Videos",
    "venue": "arXiv",
    "date": "2024",
    "link": "https://arxiv.org/pdf/2412.05185",
    "source": "auto-script",
    "tags": [
      "Instruction Tuning"
    ],
    "authors": "Lishuai Gao, Yujie Zhong, Yingsen Zeng, Haoxian Tan, Dengjie Li, Zheng Zhao.",
    "description": "LinVT 是由美团 (Meituan Inc.) 研发的一种即插即用的线性视频分词器 (Linear Video Tokenizer)。该研究旨在通过最小的改动，将任何预训练好的图像级大语言模型（如 InternVL2、Qwen2-VL 等）转换为强大的视频理解模型。LinVT 遵循两大设计原则：一是通过线性变换保留原始的图文对齐能力，二是从冗余的视频帧中提取最具代表性的信息进行压缩。实验表明，搭载 LinVT 的模型在多个视频理解基准测试中均达到了 SOTA 性能，证明了该方法在多模态适配方面的高效性与普适性。",
    "image": "",
    "code": "https://github.com/gls0425/LinVT"
  },
  {
    "title": "Expanding Performance Boundaries of Open-Source Multimodal Models with Model, Data, and Test-Time Scaling",
    "venue": "arXiv",
    "date": "2024",
    "link": "https://arxiv.org/pdf/2412.05271",
    "source": "auto-script",
    "tags": [
      "Instruction Tuning"
    ],
    "authors": "Zhe Chen, Weiyun Wang, Yue Cao, Yangzhou Liu, Zhangwei Gao, et al.",
    "description": "InternVL 2.5 是由上海人工智能实验室联合清华大学、南京大学、复旦大学、香港中文大学、上海交通大学等多所高校，以及商汤科技共同研发的先进开源多模态大模型系列。该版本在 InternVL 2.0 的基础上，通过模型规模缩放、高质量数据增强以及引入测试时计算（Test-Time Compute）扩展技术，显著提升了多模态理解与推理能力。InternVL 2.5 支持 1M 超长上下文，在文档解析、复杂图表理解及视频问答等多个基准测试中表现出色，是目前开源界性能最强的多模态模型之一。",
    "image": "",
    "code": "https://github.com/OpenGVLab/InternVL"
  },
  {
    "title": "NVILA: Efficient Frontier Visual Language Models",
    "venue": "arXiv",
    "date": "2024",
    "link": "https://arxiv.org/pdf/2412.04468",
    "source": "auto-script",
    "tags": [
      "Instruction Tuning"
    ],
    "authors": "Cheng-Yu Hsieh, Ligeng Zhu, Baifeng Shi, De-An Huang, Yuming Lou, et al.",
    "description": "NVILA 是由 NVIDIA 联合麻省理工学院 (MIT)、清华大学、加州大学伯克利分校、加州大学圣地亚哥分校、华盛顿大学研发的高效多模态模型。该研究在 VILA 的基础上提出了“先缩放后压缩”架构，通过提升空间与时间分辨率捕捉细节，再利用视觉 Token 压缩保持推理效率。NVILA 优化了从数据管线到部署的生命周期效率，在实现顶尖准确率的同时，显著提升了处理高分辨率图像和长视频的速度。",
    "image": "",
    "code": "https://github.com/NVlabs/VILA"
  },
  {
    "title": "Inst-IT: Boosting Multimodal Instance Understanding via Explicit Visual Prompt Instruction Tuning",
    "venue": "arXiv",
    "date": "2024",
    "link": "https://arxiv.org/pdf/2412.03565",
    "source": "auto-script",
    "tags": [
      "Instruction Tuning"
    ],
    "authors": "Wujian Peng, Lingchen Meng, Yitong Chen, Yiweng Xie, Yang Liu, et al.",
    "description": "INST-IT 是由复旦大学（可靠性嵌入式智能实验室）、上海人工智能实验室以及华为诺亚方舟实验室联合研发的一种旨在增强多模态模型“实例级理解”能力的方案。该研究发现，虽然现有大模型能理解图像整体，但在处理特定目标实例（Instance）时仍感吃力。为此，团队提出了 INST-IT 框架，包含一个诊断性基准测试、一个大规模指令微调数据集，以及一种通过显式视觉提示（如检测框、点等）引导模型关注特定目标的训练范式。实验证明，该方法显著提升了模型在时空维度上对具体实例的对齐与理解精度。",
    "image": "",
    "code": ""
  },
  {
    "title": "TimeMarker: A Versatile Video-LLM for Long and Short Video Understanding with Superior Temporal Localization Ability",
    "venue": "arXiv",
    "date": "2024",
    "link": "https://arxiv.org/pdf/2411.18211",
    "source": "auto-script",
    "tags": [
      "Instruction Tuning"
    ],
    "authors": "Shimin Chen, Xiaohan Lan, Yitian Yuan, Zequn Jie, Lin Ma",
    "description": "TimeMarker 是由美团 (Meituan Inc.) 研发的一款多功能视频大语言模型，特别强化了时间定位（Temporal Localization）能力。该研究针对现有模型在处理变长视频和精确时刻定位上的不足，引入了“时间分隔符 Token”以增强模型的时间意识，并开发了 AnyLength 机制，通过动态帧采样和自适应 Token 合并，使模型能够同时兼顾短视频的细节与长视频的全局理解。TimeMarker 在多个视频问答和时间点定位基准测试中均取得了领先性能。",
    "image": "",
    "code": ""
  },
  {
    "title": "ChatRex: Taming Multimodal LLM for Joint Perception and Understanding",
    "venue": "arXiv",
    "date": "2024",
    "link": "https://arxiv.org/pdf/2411.18363",
    "source": "auto-script",
    "tags": [
      "Instruction Tuning"
    ],
    "authors": "Qing Jiang, Gen Luo, Yuqin Yang, Yuda Xiong, Zhaoyang Zeng, et al.",
    "description": "ChatRex 是由粤港澳大湾区数字经济研究院 (IDEA) 联合华南理工大学研发的多模态大模型。该研究旨在解决现有模型（如 Qwen2-VL）在精确感知（如目标检测）方面的短板。ChatRex 采用了“解耦感知”设计，将复杂的坐标回归任务转化为检索任务：通过通用的提议网络（Proposal Network）生成候选框，再由 LLM 选取对应的框索引。配合该架构，团队构建了包含 200 万数据的 Rexverse-2M 自动标注数据集。实验表明，ChatRex 在目标检测、物体计数及视觉对齐理解任务上均展现出极高的精确度。",
    "image": "",
    "code": "https://github.com/IDEA-Research/ChatRex"
  },
  {
    "title": "LongVU: Spatiotemporal Adaptive Compression for Long Video-Language Understanding",
    "venue": "arXiv",
    "date": "2024",
    "link": "https://arxiv.org/pdf/2410.17434",
    "source": "auto-script",
    "tags": [
      "Instruction Tuning"
    ],
    "authors": "Xiaoqian Shen, Yunyang Xiong, Changsheng Zhao, Lemeng Wu, Jun Chen, et al.",
    "description": "LongVU 是由 Meta AI 联合阿卜杜拉国王科技大学 (KAUST) 和高丽大学研发的长视频理解模型。该研究针对大语言模型上下文窗口限制长视频处理的问题，提出了一种“时空自适应压缩”机制。LongVU 利用 DINOv2 特征去除相似度高的冗余帧，并通过文本引导的跨模态查询（Cross-modal Query）有选择地保留与指令相关的视觉细节。通过这种时空维度的双重压缩，LongVU 能够在有限的 Token 预算内处理包含上千帧的超长视频，在视频问答和长视频定位任务中表现优异。",
    "image": "",
    "code": "https://github.com/mshunshin/LongVU"
  },
  {
    "title": "Deciphering Cross-Modal Alignment in Large Vision-Language Models with Modality Integration Rate",
    "venue": "arXiv",
    "date": "2024",
    "link": "https://arxiv.org/pdf/2410.07167",
    "source": "auto-script",
    "tags": [
      "Instruction Tuning"
    ],
    "authors": "Qidong Huang, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Yuhang Cao, et al.",
    "description": "MIR 是由中国科学技术大学 (USTC) 联合上海人工智能实验室、香港中文大学共同研发的一种多模态评估指标。该研究针对大视觉语言模型 (LVLM) 在预训练阶段缺乏有效衡量对齐质量指标的问题，提出了“模态整合率” (Modality Integration Rate)。MIR 通过测量不同模态间分布距离的变化，无需经过昂贵的有监督微调 (SFT) 即可直接评估预训练的效果。实验证明，MIR 与模型微调后的最终 Benchmark 表现高度正相关，能够有效指导预训练数据的选择和模型架构的优化。",
    "image": "",
    "code": ""
  },
  {
    "title": "AuroraCap: Efficient, Performant Video Detailed Captioning and a New Benchmark",
    "venue": "arXiv",
    "date": "2024",
    "link": "https://arxiv.org/pdf/2410.03051",
    "source": "auto-script",
    "tags": [
      "Instruction Tuning"
    ],
    "authors": "Wenhao Chai, Enxin Song, Yilun Du, Chenlin Meng, Vashisht Madhavan, et al.",
    "description": "AuroraCap 是由 Pika Labs 联合华盛顿大学、斯坦福大学、哈佛大学以及纽约大学研发的高效视频详细描述模型。该研究提出了一种简洁的架构，通过 Token 合并（Token Merging）策略大幅减少视觉输入冗余，实现在极低计算开销下处理长视频序列。AuroraCap 在多项视频描述基准测试中超越了 GPT-4V 和 Gemini-1.5 Pro。此外，团队还推出了 VDC (Video Detailed Captioning) 基准测试，填补了现有测试集在视频长文本描述领域的空白。",
    "image": "",
    "code": "https://github.com/v-pika/AuroraCap"
  },
  {
    "title": "EMOVA: Empowering Language Models to See, Hear and Speak with Vivid Emotions",
    "venue": "CVPR",
    "date": "2024",
    "link": "https://arxiv.org/pdf/2409.18042",
    "source": "auto-script",
    "tags": [
      "Instruction Tuning"
    ],
    "authors": "Kai Chen, Yunhao Gou, Runhui Huang, Zhili Liu, Daxin Tan, et al.",
    "description": "EMOVA 是由华为诺亚方舟实验室联合香港科技大学、香港大学、香港中文大学等多所高校研发的全模态基础模型。该模型旨在复刻 GPT-4o 的端到端视听交互能力，能够同时感知并生成带有生动情感的图像、文本和语音。通过引入“语义-声学解耦”架构和情感增强训练，EMOVA 克服了现有开源模型在语音交互中情感匮乏的问题。实验表明，该模型在多模态理解基准测试（如 MMMU）和语音对话质量评价上均表现优异，尤其在实时情感表达和语调控制方面具有显著优势。",
    "image": "",
    "code": "https://github.com/PKU-YuanGroup/Emova"
  },
  {
    "title": "Molmo and PixMo: Open Weights and Open Data for State-of-the-Art Multimodal Models",
    "venue": "arXiv",
    "date": "2024",
    "link": "https://arxiv.org/pdf/2409.17146",
    "source": "auto-script",
    "tags": [
      "Instruction Tuning"
    ],
    "authors": "Matt Deitke, Christopher Clark, Sangho Lee, Rohun Tripathi, Yue Yang, et al.",
    "description": "Molmo 是由艾伦人工智能研究所 (Allen Institute for AI) 联合华盛顿大学研发的开源视觉语言模型。该项目旨在通过高质量人工标注数据而非合成数据来提升模型性能，不仅开源了模型权重，还开放了包含海量详细描述的 PixMo 数据集。Molmo 具备独特的“基于点的交互”能力，其旗舰版本 Molmo-72B 在多项基准测试中达到了与 GPT-4o 相当的水平。通过结合研究机构的工程实力与高校的学术探索，该成果为开源社区构建高性能多模态模型提供了重要参考。",
    "image": "",
    "code": "https://github.com/allenai/molmo"
  },
  {
    "title": "Qwen2-VL: Enhancing Vision-Language Model's Perception of the World at Any Resolution",
    "venue": "arXiv",
    "date": "2024",
    "link": "https://arxiv.org/pdf/2409.12191",
    "source": "auto-script",
    "tags": [
      "Instruction Tuning"
    ],
    "authors": "Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, et al.",
    "description": "Qwen2-VL 是由阿里巴巴 Qwen 团队研发的旗舰级视觉语言模型系列。该模型引入了“原生动态分辨率”（Naive Dynamic Resolution）机制，能够处理任意分辨率的图像并将其动态转化为不同数量的 Token，更符合人类视觉感知。此外，模型集成了多模态旋转位置嵌入（M-ROPE），实现了文本、图像和视频位置信息的深度融合。Qwen2-VL 在 2B、8B 和 72B 三个规模上均表现卓越，不仅在多模态理解基准测试中取得领先，还在视觉 Agent 任务（如手机/电脑操作）和长视频分析方面展现了强大的实用潜力。",
    "image": "",
    "code": "https://github.com/QwenLM/Qwen2-VL"
  },
  {
    "title": "ChartMoE: Mixture of Expert Connector for Advanced Chart Understanding",
    "venue": "ICLR",
    "date": "2024",
    "link": "https://arxiv.org/pdf/2409.03277",
    "source": "auto-script",
    "tags": [
      "Instruction Tuning"
    ],
    "authors": "Zhengzhuo Xu, Bowen Qu, Yiyan Qi, Sinan Du, Chengjin Xu, et al.",
    "description": "ChartMoE 是由粤港澳大湾区数字经济研究院 (IDEA) 联合清华大学、北京大学、香港科技大学（广州）研发的图表理解模型。该研究创新性地采用专家混合 (MoE) 架构取代传统的线性投影层，以连接视觉与语言模态。通过在包含 100 万条图表-数据对的 ChartMoE-Align 数据集上进行多样化任务预训练，不同专家被赋予了从数据提取到逻辑分析的不同专长。实验表明，ChartMoE 在多个图表问答 (ChartQA) 基准测试中表现优异，有效缓解了多模态模型在图表解析中的幻觉问题。",
    "image": "",
    "code": "https://github.com/IDEA-FinAI/ChartMoE"
  },
  {
    "title": "LongLLaVA: Scaling Multi-modal LLMs to 1000 Images Efficiently via Hybrid Architecture",
    "venue": "arXiv",
    "date": "2024",
    "link": "https://arxiv.org/pdf/2409.02889",
    "source": "auto-script",
    "tags": [
      "Instruction Tuning"
    ],
    "authors": "Xidong Wang, Dingjie Song, Shunian Chen, Junyin Chen, Zhenyang Cai, et al.",
    "description": "LongLLaVA 是由香港中文大学（深圳）联合理海大学 (Lehigh University) 以及美团研发的一种高效长文本多模态大模型。该研究旨在解决多模态模型在处理超长视频或大量图像时计算开销巨大的问题。LongLLaVA 采用了集成 Mamba 和 Transformer 块的混合架构，并通过渐进式训练策略，实现了在单张 A100 GPU 上高效处理近千张图像的能力。实验表明，LongLLaVA 在保持高吞吐量和低显存占用的同时，在长视频理解和多图分析基准测试中展现出极强的竞争优势。",
    "image": "",
    "code": "https://github.com/FreedomIntelligence/LongLLaVA"
  },
  {
    "title": "EAGLE: Exploring The Design Space for Multimodal LLMs with Mixture of Encoders",
    "venue": "arXiv",
    "date": "2024",
    "link": "https://arxiv.org/pdf/2408.15998",
    "source": "auto-script",
    "tags": [
      "Instruction Tuning"
    ],
    "authors": "Min Shi, Fuxiao Liu, Shihao Wang, Shijia Liao, Subhashree Radhakrishnan, et al.",
    "description": "Eagle 是由 NVIDIA 联合佐治亚理工学院 (Georgia Tech)、马里兰大学 (UMD)、香港理工大学以及纽约大学研发的多模态大模型。该研究深入探索了多模态模型的“设计空间”，提出了一种混合视觉编码器架构。Eagle 不再依赖单一的视觉提取器，而是通过简单且可扩展的融合方案，将多个不同专长的编码器集成在一起，以获取更高分辨率和更丰富的语义特征。实验表明，Eagle 在 OCR、图表理解及多种视觉问答基准测试中均展现出极其强悍的性能，且该项目已完全开源了其训练数据配方与模型权重。",
    "image": "",
    "code": "https://github.com/NVlabs/Eagle"
  },
  {
    "title": "LLaVA-MoD: Making LLaVA Tiny via MoE Knowledge Distillation",
    "venue": "arXiv",
    "date": "2024",
    "link": "https://arxiv.org/pdf/2408.15881",
    "source": "auto-script",
    "tags": [
      "Instruction Tuning"
    ],
    "authors": "Fangxun Shu, Yue Liao, Le Zhuo, Haonan Shi, Long Chen, et al.",
    "description": "LLaVA-MoD 是由阿里巴巴联合香港中文大学、浙江大学、北京航空航天大学等高校研发的一种轻量化多模态大模型训练框架。该研究旨在通过知识蒸馏技术，将大型多模态模型 (l-MLLM) 的能力迁移到小规模模型 (s-MLLM) 中。LLaVA-MoD 创新性地将稀疏专家混合 (MoE) 架构引入小模型，在保持计算高效的同时提升了模型表达力；并提出了一种从“模仿蒸馏”到“偏好蒸馏”的渐进式策略。实验表明，仅拥有 2B 左右激活参数的 LLaVA-MoD 在多个基准测试中达到了与更大规模模型相当的性能。",
    "image": "",
    "code": ""
  },
  {
    "title": "mPLUG-Owl3: Towards Long Image-Sequence Understanding in Multi-Modal Large Language Models",
    "venue": "arXiv",
    "date": "2024",
    "link": "https://www.arxiv.org/pdf/2408.04840",
    "source": "auto-script",
    "tags": [
      "Instruction Tuning"
    ],
    "authors": "Jiabo Ye, Haiyang Xu, Haowei Liu, Anwen Hu, Ming Yan, et al.",
    "description": "mPLUG-Owl3 是由阿里巴巴 Qwen 团队（原 mPLUG 团队）研发的多模态大模型，旨在解决长图像序列理解中的效率与性能平衡问题。该研究提出了一种全新的“超注意力机制”（Hyper-Attention），通过在视觉 Token 注入语言模型前进行有效的特征筛选与整合，显著降低了处理大量图像时的计算开销。实验表明，mPLUG-Owl3 在处理包含数百张图像的超长序列时，不仅保持了极低的首字延迟（First Token Latency），在多图对比、长视频理解以及交错图文对话等任务上也达到了 SOTA 水平。",
    "image": "",
    "code": "https://github.com/X-PLUG/mPLUG-Owl"
  },
  {
    "title": "VITA: Towards Open-Source Interactive Omni Multimodal LLM",
    "venue": "arXiv",
    "date": "2024",
    "link": "https://arxiv.org/pdf/2408.05211",
    "source": "auto-script",
    "tags": [
      "Instruction Tuning"
    ],
    "authors": "Chaoyou Fu, Haojia Lin, Zuwei Long, Yunhang Shen, Yuhang Dai, et al.",
    "description": "VITA 是由腾讯优图实验室 (Tencent Youtu Lab) 联合南京大学、厦门大学以及中国科学院自动化研究所 (CASIA) 研发的开源全模态大模型。该模型以 Mixtral 8x7B 为语言基座，首次在开源领域实现了视频、图像、文本和音频四种模态的同步处理与分析。VITA 的核心优势在于其先进的多模态交互体验，通过两阶段多任务学习实现了高效的模态对齐。实验结果显示，VITA 在多语言理解、视觉识别和语音处理等多个基准测试中均表现稳健，是构建实时交互式智能体的重要尝试。",
    "image": "",
    "code": "https://github.com/VITA-Home/VITA"
  },
  {
    "title": "LLaVA-OneVision: Easy Visual Task Transfer",
    "venue": "arXiv",
    "date": "2024",
    "link": "https://arxiv.org/pdf/2408.03326",
    "source": "auto-script",
    "tags": [
      "Instruction Tuning"
    ],
    "authors": "Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, et al.",
    "description": "LLaVA-OneVision 是由字节跳动 (ByteDance) 联合南洋理工大学 (S-Lab, NTU)、香港中文大学、香港科技大学研发的全能多模态大模型。该研究旨在通过整合 LLaVA-NeXT 系列在数据、模型和视觉表示上的见解，构建首个能同时在单图、多图和视频三大场景中突破开源模型性能边界的统一模型。其核心优势在于强大的任务迁移能力，能够将图像领域的知识有效迁移至视频理解，从而在多模态理解基准测试中展现出卓越的泛化性能。",
    "image": "",
    "code": "https://github.com/LLaVA-VL/LLaVA-NeXT"
  },
  {
    "title": "MiniCPM-V: A GPT-4V Level MLLM on Your Phone",
    "venue": "arXiv",
    "date": "2024",
    "link": "https://arxiv.org/pdf/2408.01800",
    "source": "auto-script",
    "tags": [
      "Instruction Tuning"
    ],
    "authors": "Yuan Yao, Tianyu Yu, Ao Zhang, Chongyi Wang, Junbo Cui, et al.",
    "description": "MiniCPM-V 是由面壁智能 (ModelBest) 联合 OpenBMB 开源社区研发的轻量化多模态大模型系列。该系列旨在将 GPT-4V 级别的多模态能力部署在端侧设备（如智能手机）上。其代表作 MiniCPM-Llama3-V 2.5 仅有 8B 参数，但在 OCR 识别、复杂图像理解以及多语言支持方面表现卓越。通过高效的视觉特征压缩，该模型显著提升了在移动端处理高分辨率图像的效率。",
    "image": "",
    "code": "https://github.com/OpenBMB/MiniCPM-V"
  },
  {
    "title": "VILA^2: VILA Augmented VILA",
    "venue": "arXiv",
    "date": "2024",
    "link": "https://arxiv.org/pdf/2407.17453",
    "source": "auto-script",
    "tags": [
      "Instruction Tuning"
    ],
    "authors": "Yunhao Fang, Ligeng Zhu, Yao Lu, Yan Wang, Pavlo Molchanov, et al.",
    "description": "VILA² 是由 NVIDIA 联合德克萨斯大学奥斯汀分校 (UT Austin) 以及麻省理工学院 (MIT) 研发的一种视觉语言模型自我提升框架。该研究旨在解决多模态数据质量与数量的瓶颈问题，提出了一种“VLM 增强 VLM”的方案。VILA² 通过自我增强（Self-augment）和专家增强（Specialist-augment）两个步骤，让模型在无需昂贵人工标注的情况下，通过迭代重写预训练数据描述并自我训练，实现性能的持续提升。实验证明，经过三轮自我增强迭代，模型在多个多模态基准测试中的准确率显著提高，为构建高性能 VLM 提供了一种高效的“免费午餐”式数据优化方案。",
    "image": "",
    "code": "https://github.com/NVlabs/VILA"
  },
  {
    "title": "SlowFast-LLaVA: A Strong Training-Free Baseline for Video Large Language Models",
    "venue": "arXiv",
    "date": "2024",
    "link": "https://arxiv.org/pdf/2407.15841",
    "source": "auto-script",
    "tags": [
      "Instruction Tuning"
    ],
    "authors": "Mingze Xu, Mingfei Gao, Zhe Gan, Hong-You Chen, Zhengfeng Lai, et al.",
    "description": "SlowFast-LLaVA (SF-LLaVA) 是由苹果公司 (Apple) 研发的一种无需训练（Training-free）的视频大语言模型框架。该研究借鉴了经典视频识别中的 SlowFast 思想，通过双路径设计来平衡空间细节与时序上下文：Slow 路径以低帧率捕捉高分辨率的空间细节，而 Fast 路径以高帧率配合大步幅空间池化来捕捉动作信息。这种设计使得模型能在不超出大语言模型 Token 预算的前提下，同时获得精细的语义理解和长程时序感知。实验结果显示，SF-LLaVA 在无需任何视频数据微调的情况下，性能不仅超越了现有的免训练方法，甚至在多个基准测试中达到了与经过专门微调的视频模型相当的水平。",
    "image": "",
    "code": "https://github.com/apple/ml-slowfast-llava"
  },
  {
    "title": "EVLM: An Efficient Vision-Language Model for Visual Understanding",
    "venue": "arXiv",
    "date": "2024",
    "link": "https://arxiv.org/pdf/2407.14177",
    "source": "auto-script",
    "tags": [
      "Instruction Tuning"
    ],
    "authors": "Kaibing Chen, Dong Shen, Hanwen Zhong, Huasong Zhong, Kui Xia, et al.",
    "description": "EVLM 是由快手科技 (Kuaishou Technology) 研发的一种高效视觉语言模型。该研究针对类 LLaVA 架构在处理长视频或多图时计算开销过大的问题，提出了一套优化方案：(1) 借鉴 Flamingo 架构，采用门控交叉注意力（Gated Cross-Attention）进行模态交互，降低自注意力机制的压力；(2) 利用分层 ViT 特征，使模型能更全面地感知视觉信号；(3) 引入专家混合 (MoE) 机制提升模型效能。实验表明，EVLM 在保持较低计算成本的同时，在图像描述、视频描述及多个多模态基准测试中均取得了极具竞争力的成绩。",
    "image": "",
    "code": ""
  },
  {
    "title": "IDA-VLM: Towards Movie Understanding via ID-Aware Large Vision-Language Model",
    "venue": "arXiv",
    "date": "2024",
    "link": "https://arxiv.org/pdf/2407.07577",
    "source": "auto-script",
    "tags": [
      "Instruction Tuning"
    ],
    "authors": "Yatai Ji, Shilong Zhang, Jie Wu, Peize Sun, Weifeng Chen, et al.",
    "description": "IDA-VLM 是由香港大学、清华大学与字节跳动 (ByteDance) 联合研发的一种身份感知（ID-Aware）多模态大模型。该研究针对现有模型在处理长篇电影等复杂场景时无法跨镜头识别同一角色的痛点，通过引入“ID 引用”进行视觉指令微调，使模型具备了记忆和关联不同场景中角色身份的能力。为了验证这一能力，团队还推出了 MM-ID 基准测试，从匹配、定位、问答和描述四个维度评估模型对实例 ID 的认知水平。IDA-VLM 的出现为多模态模型从单一场景理解跨向更高级的剧情逻辑分析奠定了基础。",
    "image": "",
    "code": "https://github.com/jiyt17/IDA-VLM"
  },
  {
    "title": "InternLM-XComposer-2.5: A Versatile Large Vision Language Model Supporting Long-Contextual Input and Output",
    "venue": "arXiv",
    "date": "2024",
    "link": "https://arxiv.org/pdf/2407.03320",
    "source": "auto-script",
    "tags": [
      "Instruction Tuning"
    ],
    "authors": "Pan Zhang, Xiaoyi Dong, Yuhang Zang, Yuhang Cao, Rui Qian, et al.",
    "description": "InternLM-XComposer-2.5 (IXC-2.5) 是由上海人工智能实验室联合香港中文大学、商汤科技以及清华大学研发的全能级多模态大模型。该模型虽然仅基于 7B 参数量的 InternLM2 语言后端，但通过 24K 长度的交错图文数据训练，并配合 RoPE 外推技术，成功实现了高达 96K 的超长上下文处理能力。IXC-2.5 的核心特色在于其“超全能性”：它不仅能以超高分辨率理解复杂图像，还能进行复杂的图文交错内容创作（如自动生成网页）。实验表明，它在多项多模态理解基准测试中达到了 GPT-4V 级别的性能。",
    "image": "",
    "code": "https://github.com/InternLM/InternLM-XComposer"
  },
  {
    "title": "OMG-LLaVA: Bridging Image-level, Object-level, Pixel-level Reasoning and Understanding",
    "venue": "arXiv",
    "date": "2024",
    "link": "https://arxiv.org/pdf/2406.19389",
    "source": "auto-script",
    "tags": [
      "Instruction Tuning"
    ],
    "authors": "Tao Zhang, Xiangtai Li, Hao Fei, Haobo Yuan, Shengqiong Wu, et al.",
    "description": "OMG-LLaVA 是由武汉大学、Skywork AI（昆仑万维）、南洋理工大学 (S-Lab, NTU) 以及字节跳动 (ByteDance) 联合研发的一种能够兼顾图像、物体和像素级理解的多模态大模型。该研究创新性地将通用分割方法作为视觉编码器，使模型不仅能进行复杂的对话推理，还能接受视觉提示（如点、框、掩码）并输出精确的像素级分割结果。OMG-LLaVA 实现了从宏观场景理解到微观像素感知的统一，在多个视觉推理与分割基准测试中展现了强大的交互与理解能力。",
    "image": "",
    "code": "https://github.com/lxtGH/OMG-LLaVA"
  },
  {
    "title": "DocKylin: A Large Multimodal Model for Visual Document Understanding with Efficient Visual Slimming",
    "venue": "AAAI",
    "date": "2024",
    "link": "https://arxiv.org/pdf/2406.19101",
    "source": "auto-script",
    "tags": [
      "Instruction Tuning"
    ],
    "authors": "Jiaxin Zhang, Wentao Yang, Songxuan Lai, Zecheng Xie, Lianwen Jin",
    "description": "DocKylin 是由华南理工大学 (SCUT) 与华为云 (Huawei Cloud) 联合研发的一款面向文档理解（VDU）的多模态大模型。该研究针对文档图像分辨率高、文本密集且布局复杂导致的计算负担问题，提出了一种“视觉瘦身”机制。通过自适应像素瘦身（APS）预处理模块和动态 Token 瘦身（DTS）模块，模型能够从像素和 Token 两个层面过滤掉冗余信息，仅保留关键视觉细节。实验证明，DocKylin 在大幅缩短视觉 Token 序列长度的同时，在多个文档理解基准测试中展现了极具竞争力的性能。",
    "image": "",
    "code": ""
  },
  {
    "title": "Cambrian-1: A Fully Open, Vision-Centric Exploration of Multimodal LLMs",
    "venue": "arXiv",
    "date": "2024",
    "link": "https://arxiv.org/pdf/2406.16860",
    "source": "auto-script",
    "tags": [
      "Instruction Tuning"
    ],
    "authors": "Shengbang Tong, Ellis Brown, Penghao Wu, Sanghyun Woo, Manoj Middepogu, et al.",
    "description": "Cambrian-1 是由纽约大学 (NYU) 团队研发的一系列以视觉为中心的多模态大模型。该研究深入探讨了不同视觉表征（包括自监督、强监督及其组合）对模型性能的影响，实验涵盖了超过 20 种视觉编码器。为了增强视觉对齐，团队提出了“空间视觉聚合器”（SVA），这是一种动态且具备空间感知能力的连接器，能在减少 Token 数量的同时保留高分辨率特征。此外，该项目不仅完全开源了模型权重和数据配方，还推出了全新的视觉中心基准测试 CV-Bench，旨在为开源社区提供一个全面且透明的多模态研究基准。",
    "image": "",
    "code": "https://github.com/cambrian-mllm/cambrian"
  },
  {
    "title": "Long Context Transfer from Language to Vision",
    "venue": "arXiv",
    "date": "2024",
    "link": "https://arxiv.org/pdf/2406.16852",
    "source": "auto-script",
    "tags": [
      "Instruction Tuning"
    ],
    "authors": "Peiyuan Zhang, Kaichen Zhang, Bo Li, Guangtao Zeng, Jingkang Yang, et al.",
    "description": "LongVA 是由南洋理工大学 (S-Lab, NTU) 联合新加坡科技设计大学 (SUTD) 研发的长文本视频大模型。该研究提出了“长上下文迁移”现象，即通过外推语言基座的上下文长度，使模型在无需任何视频数据训练的情况下，直接获得处理海量视觉 Token 的能力。LongVA 能够高效处理超过 2000 帧视频（约 20 万个视觉 Token），并在多项视频理解基准测试中达到 SOTA 水平。此外，团队还推出了 V-NIAH（视觉大海捞针）基准测试，用于专门评估多模态模型在超长视觉序列中的信息检索能力。",
    "image": "",
    "code": "https://github.com/PKU-YuanGroup/LongVA"
  },
  {
    "title": "video-SALMONN: Speech-Enhanced Audio-Visual Large Language Models",
    "venue": "ICML",
    "date": "2024",
    "link": "https://arxiv.org/pdf/2406.15704",
    "source": "auto-script",
    "tags": [
      "Instruction Tuning"
    ],
    "authors": "Guangzhi Sun, Wenyi Yu, Changli Tang, Xianzhao Chen, Tian Tan, et al.",
    "description": "video-SALMONN 是由字节跳动 (ByteDance) 联合清华大学研发的一种增强了语音理解能力的端到端音视频大语言模型。该研究针对现有模型在处理视频时往往忽略关键语音信息的痛点，提出了一种“多分辨率因果 Q-Former”（MRC Q-Former）结构，用于连接预训练的音视频编码器与大语言模型。这种结构能捕捉到语音理解所需的细粒度时间信息，同时保持对视觉帧、音频事件和音乐的高效处理。实验证明，video-SALMONN 在涉及语音的多模态问答（Video-QA）任务上比现有模型提升了超过 25% 的准确率，并在复杂的视频推理场景中展现了卓越的性能。",
    "image": "",
    "code": "https://github.com/bytedance/video-SALMONN"
  },
  {
    "title": "TroL: Traversal of Layers for Large Language and Vision Models",
    "venue": "EMNLP",
    "date": "2024",
    "link": "https://arxiv.org/pdf/2406.12246",
    "source": "auto-script",
    "tags": [
      "Instruction Tuning"
    ],
    "authors": "Byung-Kwan Lee, Beomchan Park, Sangyun Chung, Yong Man Ro",
    "description": "TroL 是由韩国科学技术院 (KAIST) 研发的一种高效多模态大模型架构。该研究针对目前高性能多模态模型往往体积过大（如 26B 以上）的问题，提出了一种名为“层遍历”（Traversal of Layers）的技术。这种技术通过在 Token 级别复用现有的网络层，模拟“回溯”和“推敲”的过程，在不增加物理参数量的情况下有效增加了前向传播的深度。TroL 系列涵盖了 1.8B、3.8B 和 7B 等多种尺寸，实验表明，其 7B 模型在处理高分辨率图像及多模态推理任务上，能够与规模大得多的开源或闭源模型（如 GPT-4V）相媲美。",
    "image": "",
    "code": "https://github.com/ByungKwanLee/TroL"
  },
  {
    "title": "Unveiling Encoder-Free Vision-Language Models",
    "venue": "arXiv",
    "date": "2024",
    "link": "https://arxiv.org/pdf/2406.11832",
    "source": "auto-script",
    "tags": [
      "Instruction Tuning"
    ],
    "authors": "Haiwen Diao, Yufeng Cui, Xiaotong Li, Yueze Wang, Huchuan Lu, Xinlong Wang",
    "description": "EVE 是由北京人工智能研究院 (BAAI) 联合大连理工大学、北京大学研发的一种去编码器（Encoder-Free）的多模态大模型。该研究旨在打破主流 MLLM 对预训练视觉编码器（如 CLIP）的依赖，通过将原始图像像素直接输入语言模型解码器，实现了一种更简洁、灵活且高效的统一架构。EVE 提出了一套高效的训练策略，包括从编码器模型进行知识对齐以及在大规模图文数据上进行预训练，成功弥合了无编码器模型与传统模型之间的性能差距。其优势在于能原生支持任意分辨率和长宽比，并显著提升了数据处理的推理效率。",
    "image": "",
    "code": "https://github.com/baaivision/EVE"
  },
  {
    "title": "VideoLLM-online: Online Video Large Language Model for Streaming Video",
    "venue": "CVPR",
    "date": "2024",
    "link": "https://arxiv.org/pdf/2406.11816",
    "source": "auto-script",
    "tags": [
      "Instruction Tuning"
    ],
    "authors": "Joya Chen, Zhaoyang Lv, Shiwei Wu, Kevin Qinghong Lin, Chenan Song, et al.",
    "description": "VideoLLM-online 是由新加坡国立大学 (NUS) Show Lab 联合 Meta (Reality Labs Research) 研发的一款针对流媒体视频的在线多模态大模型。该研究解决了传统视频模型无法实时处理持续视频流的问题，通过引入“流式对话生成”方法，使模型能够像人类一样边看边聊。其核心技术包括一种时间压缩机制，能够在保持低延迟（A100 上可达 10~15 FPS）的同时，有效积攒长期的视频上下文。VideoLLM-online 在处理实时人体动作识别、实时解说及流式问答任务中表现出色，为构建能够实时响应物理世界的 AI 助手提供了技术路径。",
    "image": "",
    "code": "https://github.com/showlab/VideoLLM-online"
  },
  {
    "title": "RoboPoint: A Vision-Language Model for Spatial Affordance Prediction for Robotics",
    "venue": "CoRL",
    "date": "2024",
    "link": "https://arxiv.org/pdf/2406.10721",
    "source": "auto-script",
    "tags": [
      "Instruction Tuning"
    ],
    "authors": "Wentao Yuan, Jiafei Duan, Valts Blukis, Wilbert Pumacay, Arsalan Mousavian, et al.",
    "description": "RoboPoint 是由华盛顿大学 (University of Washington) 联合 NVIDIA、艾伦人工智能研究所 (AI2) 以及圣巴勃罗天主教大学 (UCSP) 研发的一种专为机器人空间推理设计的视觉语言模型。该研究针对现有 VLM 难以精确描述机器人动作的问题，提出通过预测图像关键点的“示能性”（Affordance）来指导机器人作业。RoboPoint 创新性地采用全自动合成数据生成流水线进行指令微调，完全脱离了对昂贵的人工演示数据的依赖。实验结果显示，RoboPoint 在空间关系预测准确率上比 GPT-4o 高出 21.8%，并能显著提升机器人导航、操纵及增强现实辅助任务的成功率。",
    "image": "",
    "code": "https://robo-point.github.io"
  },
  {
    "title": "Comparison Visual Instruction Tuning",
    "venue": "arXiv",
    "date": "2024",
    "link": "https://arxiv.org/abs/2406.09240",
    "source": "auto-script",
    "tags": [
      "Instruction Tuning"
    ],
    "authors": "Wei Lin, Muhammad Jehanzeb Mirza, Sivan Doveh, Rogerio Feris, Raja Giryes, et al.",
    "description": "CaD-VI 是由奥地利林茨大学 (JKU Linz) 领衔，联合 IBM Research、特拉维夫大学、MIT-IBM Watson AI Lab 等机构研发的一种新型视觉指令微调方法。该研究聚焦于人类的核心视觉能力——识别图像对之间的“共性与差异”(CaD)。通过两阶段的数据生成流程，团队构建了包含 34.9 万个图像对的指令数据集 CaD-Inst，并基于此训练了 CaD-LLaVA 模型。实验表明，该方法显著增强了大语言模型对多图进行对比推理的能力，在多项视觉比较基准测试中将 SOTA 性能提升了 17.5%。",
    "image": "",
    "code": "https://wlin-at.github.io/cad_vi"
  },
  {
    "title": "Beyond LLaVA-HD: Diving into High-Resolution Large Multimodal Models",
    "venue": "arXiv",
    "date": "2024",
    "link": "https://arxiv.org/pdf/2406.08487",
    "source": "auto-script",
    "tags": [
      "Instruction Tuning"
    ],
    "authors": "Yi-Fan Zhang, Qingsong Wen, Chaoyou Fu, Xue Wang, Zhang Zhang, et al.",
    "description": "来自中国科学院自动化研究所、中国科学院大学、松鼠Ai、阿里巴巴以及 Meta AI 的研究团队共同推出了 SliME，这是一个专为高分辨率场景设计的多模态模型框架。为了解决 LLaVA-HD 等现有模型在处理高分辨率图像时面临的计算冗余和全局语义丢失问题，SliME 采用了一种“适配器混合器”架构。该架构巧妙地结合了负责宏观上下文的全局适配器和专注于细粒度特征的局部适配器，并配合可学习的查询向量将视觉 Token 高效压缩至 256 个。这种设计不仅大幅降低了显存占用和推理延迟，更在多项高分辨率基准测试中实现了性能的显著提升。",
    "image": "",
    "code": "https://github.com/yfzhang114/SliME"
  },
  {
    "title": "VideoLLaMA 2: Advancing Spatial-Temporal Modeling and Audio Understanding in Video-LLMs",
    "venue": "arXiv",
    "date": "2024",
    "link": "https://arxiv.org/pdf/2406.07476",
    "source": "auto-script",
    "tags": [
      "Instruction Tuning"
    ],
    "authors": "Zesen Cheng, Sicong Leng, Hang Zhang, Yifei Xin, Xin Li, et al.",
    "description": "VideoLLaMA 2 是一套旨在增强视频大语言模型（Video-LLMs）中时空建模和音频理解能力的模型系列。该研究引入了定制的时空卷积（STC）连接器，能有效捕捉视频数据的复杂时空动态。此外，通过联合训练集成了音频分支，无缝结合音频线索，显著提升了模型在视频和音频导向任务中的多模态理解能力。实验表明，VideoLLaMA 2 在多个视频理解基准测试中表现出色。",
    "image": "",
    "code": "https://github.com/DAMO-NLP-SG/VideoLLaMA2"
  },
  {
    "title": "Parrot: Multilingual Visual Instruction Tuning",
    "venue": "arXiv",
    "date": "2024",
    "link": "https://arxiv.org/pdf/2406.02539",
    "source": "auto-script",
    "tags": [
      "Instruction Tuning"
    ],
    "authors": "Hai-Long Sun, Da-Wei Zhou, Yang Li, shiyin Lu, Chao Yi, et al.",
    "description": "PARROT 是一种新型多模态大语言模型（MLLM）微调方法，旨在解决现有模型在多语言环境下的“语言侵蚀”问题（即倾向于用英语回答）。该方法利用文本引导（Textual Guidance）实现视觉特征的语言级对齐，并引入轻量化的混合专家模块（MoE）将英语偏置的视觉 Token 转换为特定语言的表示。此外，研究者还发布了包含 6 种语言、1.2 万个问题的 MMMB 基准测试。实验证明，PARROT 在极少量多语言数据（不到同类模型 1%）的情况下，在土耳其语、阿拉伯语等多个语种上达到了 SOTA 性能",
    "image": "",
    "code": "https://github.com/AIDC-AI/Parrot"
  },
  {
    "title": "Ovis: Structural Embedding Alignment for Multimodal Large Language Model",
    "venue": "arXiv",
    "date": "2024",
    "link": "https://arxiv.org/pdf/2405.20797",
    "source": "auto-script",
    "tags": [
      "Instruction Tuning"
    ],
    "authors": "Shiyin Lu, Yang Li, Qing-Guo Chen, Zhao Xu, Weihua Luo, et al.",
    "description": "Ovis 是由阿里巴巴与南京大学联合提出的一种创新的多模态模型架构。现有的 MLLM 通常将视觉嵌入简单连接到 LLM 的输入空间，这缺乏明确的结构对齐。Ovis 引入了一种结构化对齐机制，通过视觉词表（Visual Vocabulary）将连续的视觉特征转换为离散且具有语义结构的表示，从而让 LLM 能更有效地理解图像内容。在多项多模态基准测试（如 MME, MMBench, MMMU）中，Ovis 展现了优于 LLaVA 等主流模型的性能，特别是在空间关系理解和精细物体识别方面表现突出。",
    "image": "",
    "code": "https://github.com/AIDC-AI/Ovis"
  },
  {
    "title": "Matryoshka Query Transformer for Large Vision-Language Models",
    "venue": "arXiv",
    "date": "2024",
    "link": "https://arxiv.org/pdf/2405.19315",
    "source": "auto-script",
    "tags": [
      "Instruction Tuning"
    ],
    "authors": "Wenbo Hu, Zi-Yi Dou, Liunian Harold Li, Amita Kamath, Nanyun Peng, Kai-Wei Chang",
    "description": "本文提出了 Matryoshka Query Transformer (MQT)，这是一种为大型视觉语言模型（LVLM）设计的灵活视觉 Token 编码方案。受俄罗斯套娃表示学习（MRL）启发，MQT 允许在推理时根据计算资源动态选择视觉 Token 的数量（从 1 到 256 个不等），而无需重新训练模型。实验表明，其构建的模型 MQT-LLaVA 仅需 256 个 Token 即可达到 LLaVA-1.5 使用 576 个 Token 的性能，且在缩减至 16 个 Token 时能实现 8 倍的 TFLOPS 加速，而性能损失极小。",
    "image": "",
    "code": "https://github.com/gordonhu608/MQT-LLaVA"
  },
  {
    "title": "ConvLLaVA: Hierarchical Backbones as Visual Encoder for Large Multimodal Models",
    "venue": "arXiv",
    "date": "2024",
    "link": "https://arxiv.org/pdf/2405.15738",
    "source": "auto-script",
    "tags": [
      "Instruction Tuning"
    ],
    "authors": "Chunjiang Ge, Sijie Cheng, Ziming Wang, Jiale Yuan, Yuan Gao, et al.",
    "description": "ConvLLaVA 是由清华大学与阿里巴巴集团联合研发的多模态大模型框架。为了解决高分辨率图像处理中视觉 Token 过多导致计算复杂度呈二次方增长的瓶颈，该研究创新性地采用分层骨干网络 ConvNeXt 替代了传统的 Vision Transformer (ViT) 作为视觉编码器。通过针对高分辨率场景优化预训练模型并引入两阶段压缩策略，ConvLLaVA 成功将 1536x1536 的高分辨率图像高效压缩为仅 576 个视觉 Token。这种设计不仅在处理任意长宽比图像时更加灵活，而且在显著降低计算冗余的同时，依然保持了与 SOTA 模型相当的性能表现。",
    "image": "",
    "code": "https://github.com/alibaba/conv-llava"
  },
  {
    "title": "Meteor: Mamba-based Traversal of Rationale for Large Language and Vision Models",
    "venue": "arXiv",
    "date": "2024",
    "link": "https://arxiv.org/pdf/2405.15574",
    "source": "auto-script",
    "tags": [
      "Instruction Tuning"
    ],
    "authors": "Byung-Kwan Lee, Beomchan Park, Chae Won Kim, Yong Man Ro",
    "description": "来自韩国科学技术院（KAIST）的研究团队提出了 Meteor，这是一个基于 Mamba 架构的高效多模态大模型。为了提升模型对复杂问题的理解能力并减少幻觉，Meteor 引入了“理性遍历”（Traversal of Rationale）的新概念。该方法利用 Mamba 架构在处理长序列数据时的线性复杂度优势，将详尽的推理过程（Rationale）嵌入到模型中。通过这种设计，Meteor 能够在不增加模型参数量、不依赖额外视觉编码器的情况下，仅凭 7B 级别的参数规模，就在多项理解与推理基准测试中展现出超越大规模闭源模型的性能表现。",
    "image": "",
    "code": "https://github.com/ByungKwanLee/Meteor"
  },
  {
    "title": "Libra: Building Decoupled Vision System on Large Language Models",
    "venue": "ICML",
    "date": "2024",
    "link": "https://arxiv.org/pdf/2405.10140",
    "source": "auto-script",
    "tags": [
      "Instruction Tuning"
    ],
    "authors": "Yifan Xu, Xiaoshan Yang, Yaguang Song, Changsheng Xu",
    "description": "Libra 是由中国科学院自动化研究所等团队提出的一种多模态大语言模型（MLLM）架构，旨在构建更合理的视觉系统。该模型引入了“解耦视觉系统”的概念，通过将视觉内部建模与跨模态交互分离，利用路由视觉专家（Routed Visual Expert）和跨模态桥接模块在预训练 LLM 中实现不同的注意力模式。Libra 采用离散自回归建模进行训练，实验证明其仅需 5000 万规模的训练数据即可在多项图像转文本基准测试中达到与现有主流 MLLM 相媲美的性能，为多模态基座模型的高效训练提供了新方案。",
    "image": "",
    "code": "https://github.com/YifanXu74/Libra"
  },
  {
    "title": "CuMo: Scaling Multimodal LLM with Co-Upcycled Mixture-of-Experts",
    "venue": "arXiv",
    "date": "2024",
    "link": "https://arxiv.org/pdf/2405.05949",
    "source": "auto-script",
    "tags": [
      "Instruction Tuning"
    ],
    "authors": "Jiachen Li, Xinyao Wang, Sijie Zhu, Chia-Wen Kuo, Lu Xu, et al.",
    "description": "CuMo针对多模态大语言模型提出了一种高效的扩展方案，与以往主要依靠增加图文对数据或增强语言模型性能的计算密集型方法不同，它侧重于从视觉侧提升模型能力。该方法在视觉编码器和MLP连接器中均引入了协同上循环（Co-upcycled）的Top-K稀疏门控混合专家（MoE）模块，使得模型在显著增强性能的同时，推理时的激活参数量仅略微增加。在训练过程中，CuMo先对MLP块进行预训练，再以此初始化MoE专家，并利用辅助损失函数来维持专家的负载均衡。实验结果表明，在仅使用开源数据集的情况下，CuMo在多项视觉问答及指令遵循基准测试中的表现均超越了同参数规模的最先进模型。",
    "image": "",
    "code": "https://github.com/SHI-Labs/CuMo"
  },
  {
    "title": "How Far Are We to GPT-4V? Closing the Gap to Commercial Multimodal Models with Open-Source Suites",
    "venue": "arXiv",
    "date": "2024",
    "link": "https://arxiv.org/pdf/2404.16821",
    "source": "auto-script",
    "tags": [
      "Instruction Tuning"
    ],
    "authors": "Zhe Chen, Weiyun Wang, Hao Tian, Shenglong Ye, Zhangwei Gao, et al.",
    "description": "InternVL 1.5 是一款开源的多模态大语言模型（MLLM），旨在缩小开源模型与商用闭源模型在多模态理解能力上的差距。该模型引入了三项关键改进：首先，通过持续学习策略优化了强大的视觉编码器 InternViT-6B，显著提升了其视觉理解能力并增强了在不同大模型间的可迁移性；其次，采用动态高分辨率策略，支持将图像根据长宽比灵活切分为最多 40 个 448×448 像素的瓦片，从而实现最高 4K 分辨率的输入处理；最后，通过构建涵盖常见场景和文档图像的高质量中英双语数据集，大幅增强了模型在 OCR 及中文相关任务中的表现。实验结果显示，InternVL 1.5 在 18 项基准测试中有 8 项取得了领先（SOTA）成绩，展现出极强的核心竞争力。",
    "image": "",
    "code": "https://github.com/OpenGVLab/InternVL"
  },
  {
    "title": "Graphic Design with Large Multimodal Model",
    "venue": "arXiv",
    "date": "2024",
    "link": "https://arxiv.org/pdf/2404.14368",
    "source": "auto-script",
    "tags": [
      "Instruction Tuning"
    ],
    "authors": "Yutao Cheng, Zhao Zhang, Maoke Yang, Hui Nie, Chunyuan Li, et al.",
    "description": "在平面设计领域，将多种设计元素自动整合为连贯的分层艺术作品，不仅能提高生产力，也为平面设计的民主化铺平了道路。现有的图形布局生成（GLG）实践主要针对有序的设计元素序列进行布局，但这通常需要预定义图层的正确顺序，从而限制了创作潜力并增加了用户工作负担。本文提出了一种更具灵活性和实用性的设定——层级布局生成（HLG），旨在从无序的设计元素集合中创建图形构图。为了解决 HLG 任务，我们推出了首个基于大多模态模型的布局生成模型 Graphist。该模型将 HLG 任务有效地重新定义为序列生成问题，以 RGB-A 图像作为输入，输出包含各元素坐标、尺寸和顺序的 JSON 草案协议。此外，我们还为 HLG 开发了新的评估指标。实验表明，Graphist 的表现优于以往技术，为该领域确立了强大的基准。",
    "image": "",
    "code": "https://github.com/graphic-design-ai/graphist"
  },
  {
    "title": "BRAVE: Broadening the visual encoding of vision-language models",
    "venue": "ECCV",
    "date": "2024",
    "link": "https://arxiv.org/abs/2404.07204",
    "source": "auto-script",
    "tags": [
      "Instruction Tuning"
    ],
    "authors": "Oğuzhan Fatih Kar, Alessio Tonioni, Petra Poklukar, Achin Kulshrestha, Amir Zamir, Federico Tombari",
    "description": "视觉语言模型（VLM）通常由视觉编码器和语言模型组成，但受限于视觉编码器能力的不足，常面临视觉特征“盲区”及视觉幻觉等问题。针对这一挑战，BRAVE 研究了如何拓宽 VLM 的视觉编码能力，通过对多种具有不同归纳偏置的视觉编码器进行全面基准测试，发现没有单一编码配置能在所有任务中始终保持领先。基于此发现，BRAVE 提出了一种将多个冻结编码器的特征整合为更通用表示的方法，并将其直接输入冻结的语言模型。实验结果表明，BRAVE 在多项图像描述和视觉问答基准测试中达到了领先水平，不仅显著缓解了视觉幻觉等问题，且相比现有方法所需的训练参数更少，表示形式也更为精简，充分展示了融合不同视觉偏置在增强模型语境化视觉理解方面的巨大潜力。",
    "image": "",
    "code": ""
  },
  {
    "title": "InternLM-XComposer2-4KHD: A Pioneering Large Vision-Language Model Handling Resolutions from 336 Pixels to 4K HD",
    "venue": "arXiv",
    "date": "2024",
    "link": "https://arxiv.org/pdf/2404.06512.pdf",
    "source": "auto-script",
    "tags": [
      "Instruction Tuning"
    ],
    "authors": "Xiaoyi Dong, Pan Zhang, Yuhang Zang, Yuhang Cao, Bin Wang, et al.",
    "description": "InternLM-XComposer2-4KHD 是一项旨在提升大型视觉语言模型（LVLM）分辨率处理能力的突破性研究，将分辨率上限提升至 4K HD（3840×1600）甚至更高。该模型引入了带有自动切片配置的动态分辨率机制，能够根据输入图像的长宽比自动调整切片数量和布局，从而支持从 336 像素到 4K 标准的广泛分辨率范围。研究表明，将训练分辨率提升至 4K 级别能带来持续的性能增长且未见上限。实验结果显示，InternLM-XComposer2-4KHD 展现出卓越的理解能力，在 16 个基准测试中的 10 个项目上达到甚至超越了 GPT-4V 和 Gemini Pro 的水平。",
    "image": "",
    "code": "https://github.com/InternLM/InternLM-XComposer"
  },
  {
    "title": "Ferret-UI: Grounded Mobile UI Understanding with Multimodal LLMs",
    "venue": "arXiv",
    "date": "2024",
    "link": "https://arxiv.org/pdf/2404.05719.pdf",
    "source": "auto-script",
    "tags": [
      "Instruction Tuning"
    ],
    "authors": "Keen You, Haotian Zhang, Eldon Schoop, Floris Weers, Amanda Swearngin, et al.",
    "description": "虽然多模态大语言模型（MLLM）取得了显著进展，但在理解和处理移动端用户界面（UI）屏幕方面仍存在局限。为此，本文推出了 Ferret-UI，这是一款专门为增强移动 UI 屏幕理解而设计的全新多模态大语言模型，具备精准的引用（referring）、定位（grounding）和推理能力。针对 UI 屏幕宽高比大且包含图标、文本等细微目标的特点，Ferret-UI 在原有 Ferret 架构基础上引入了“任意分辨率”技术，通过将屏幕按原始比例切分为两个子图分别编码，从而有效放大细节并获取增强的视觉特征。研究团队精心收集了涵盖图标识别、文本查找、组件列表等基础 UI 任务的训练样本，并结合区域注释将其转化为指令遵循格式；此外，还编制了包含详细描述、交互对话及功能推断等高级任务的数据集，以提升模型的推理水平。实验表明，Ferret-UI 在 UI 屏幕理解及执行开放式指令方面表现出色，其不仅在综合基准测试中超越了大多数开源 UI 模型，且在所有基础 UI 任务上的表现均优于 GPT-4V。",
    "image": "",
    "code": ""
  },
  {
    "title": "MA-LMM: Memory-Augmented Large Multimodal Model for Long-Term Video Understanding",
    "venue": "CVPR",
    "date": "2024",
    "link": "https://arxiv.org/pdf/2404.05726.pdf",
    "source": "auto-script",
    "tags": [
      "Instruction Tuning"
    ],
    "authors": "Bo He, Hengduo Li, Young Kyun Jang, Menglin Jia, Xuefei Cao, et al.",
    "description": "随着大语言模型（LLM）的成功，将视觉模型与之集成以构建视觉语言基座模型已成为研究热点。然而，现有的基于 LLM 的大型多模态模型（如 Video-LLaMA、VideoChat）在处理视频时通常只能接收有限的帧数，仅适用于短视频理解。本研究旨在设计一种高效且实用的长视频理解模型。与大多数现有工作试图同时处理更多帧的方法不同，我们提出以在线方式处理视频，并将过去的视频信息存储在存储库（Memory Bank）中。这种设计允许模型在不超出 LLM 上下文长度限制或 GPU 显存限制的情况下，参考历史视频内容进行长期分析。该存储库可以以即插即用的方式无缝集成到现有的多模态 LLM 中。通过在长视频理解、视频问答和视频描述等多种任务上的广泛实验证明，该模型在多个数据集上均达到了领先（SOTA）性能。",
    "image": "",
    "code": "https://github.com/boheumd/MA-LMM"
  },
  {
    "title": "VITRON: A Unified Pixel-level Vision LLM for Understanding, Generating, Segmenting, Editing",
    "venue": "NeurIPS",
    "date": "2024",
    "link": "https://haofei.vip/downloads/papers/Skywork_Vitron_2024.pdf",
    "source": "auto-script",
    "tags": [
      "Instruction Tuning"
    ],
    "authors": "Hao Fei, Shengqiong Wu, Hanwang Zhang, Tat-Seng Chua2, Shuicheng Yan",
    "description": "VITRON 是一款通用的像素级视觉大语言模型，旨在实现对静态图像和动态视频的全面理解、生成、分割及编辑。该模型在语言模型（LLM）骨干网络的基础上，于前端集成了图像、视频及像素级区域视觉编码器，并以后端先进的视觉专家模型为支撑，从而涵盖了从低级到高级、从视觉理解到生成的全方位任务。为了确保从语言模型到后端模块的功能调用指令传递既有效又精确，VITRON 提出了一种结合离散文本指令与连续信号嵌入的新型混合方法。此外，研究团队还设计了多种像素级时空视听对齐学习方案，以赋予模型卓越的细粒度视觉处理能力，并引入了跨任务协同模块来最大化任务不变的细粒度特征。在涵盖 12 项视觉任务、22 个数据集的评估中，VITRON 展示了其在理解、生成、分割和编辑四大任务集群中的强大实力，为开发更统一的多模态通用模型提供了新的可能。",
    "image": "",
    "code": "https://vitron-llm.github.io/"
  },
  {
    "title": "TOMGPT: Reliable Text-Only Training Approach for Cost-Effective Multi-modal Large Language Model",
    "venue": "ACM TKDD",
    "date": "2024",
    "link": "https://dl.acm.org/doi/pdf/10.1145/3654674",
    "source": "auto-script",
    "tags": [
      "Instruction Tuning"
    ],
    "authors": "Yunkai Chen, Qimeng Wang, Shiwei Wu, Yan Gao, Tong Xu, Yao Hu",
    "description": "TOMGPT（仅文本训练的多模态 GPT）提出了一种极具成本效益的多模态大语言模型（MLLM）微调方案，旨在摆脱对大规模高质量图文对数据的依赖。该方法巧妙利用了 CLIP 或 ALIGN 等预训练好的视觉-语言耦合空间，通过一种“仅文本训练”的策略，将已对齐的多模态潜空间进一步投影到大语言模型（LLM）的空间中，从而高效地赋予 LLM 视觉理解能力。研究发现，TOMGPT 无需海量图文数据，仅需少量且多样化的 GPT 生成的自由文本数据，即可建立起 LLM 与预训练视觉模型之间的语义连接。在 MME 和 LVLM 等主流基准测试中的定量评估表明，TOMGPT 的性能表现足以比肩许多使用大量图文对训练的模型。此外，案例研究也证明了该模型在不同图像类别下具备广泛的理解与对话能力。",
    "image": "",
    "code": ""
  },
  {
    "title": "LITA: Language Instructed Temporal-Localization Assistant",
    "venue": "arXiv",
    "date": "2024",
    "link": "https://arxiv.org/pdf/2403.19046",
    "source": "auto-script",
    "tags": [
      "Instruction Tuning"
    ],
    "authors": "De-An Huang, Shijia Liao, Subhashree Radhakrishnan, Hongxu Yin, Pavlo Molchanov, et al.",
    "description": "LITA（语言指示时序定位助手）旨在解决多模态大语言模型在视频理解中面临的核心挑战——准确的时序定位（即回答“何时发生”的问题）。该研究指出，现有模型在时间表示、架构设计及数据支撑三个方面存在局限。为此，LITA 引入了相对于视频长度进行编码的“时间标记（Time Tokens）”，并采用“SlowFast 标记”架构以捕获高分辨率的时序信息。此外，研究团队还提出了一项名为推理时序定位（RTL）的新任务及其对应的 ActivityNet-RTL 数据集，要求模型同时具备推理与定位能力。实验结果表明，LITA 在 RTL 任务上的表现显著优于基准模型，时序平均交并比（mIoU）近乎翻倍，同时在视频文本生成及时间理解能力上较现有视频大语言模型实现了大幅提升。",
    "image": "",
    "code": "https://github.com/NVlabs/LITA"
  },
  {
    "title": "Mini-Gemini: Mining the Potential of Multi-modality Vision Language Models",
    "venue": "arXiv",
    "date": "2024",
    "link": "https://arxiv.org/pdf/2403.18814.pdf",
    "source": "auto-script",
    "tags": [
      "Instruction Tuning"
    ],
    "authors": "Yanwei Li, Yuechen Zhang, Chengyao Wang, Zhisheng Zhong, Yixin Chen, et al.",
    "description": "Mini-Gemini 是一个简单且高效的多模态视觉语言模型（VLM）框架，旨在通过挖掘模型潜力来缩小与 GPT-4 和 Gemini 等先进模型之间的差距。该研究从三个维度提升了 VLM 的性能：首先，通过引入额外的视觉编码器进行高分辨率细化，在不增加视觉标记（Token）数量的前提下增强了视觉感知能力；其次，构建了一个高质量数据集，以促进精确的图像理解和基于推理的生成任务；最后，实现了由 VLM 引导的生成流程，使框架能够同时具备图像理解、推理和生成能力。Mini-Gemini 支持从 2B 到 34B 参数规模的多种稠密及混合专家（MoE）语言模型，实验证明其在多个零秒（Zero-shot）基准测试中达到了领先水平，甚至超越了部分现有的闭源商业模型。",
    "image": "",
    "code": "https://github.com/JIA-Lab-research/MGM"
  },
  {
    "title": "MM1: Methods, Analysis & Insights from Multimodal LLM Pre-training",
    "venue": "arXiv",
    "date": "2024",
    "link": "https://arxiv.org/pdf/2403.09611.pdf",
    "source": "auto-script",
    "tags": [
      "Instruction Tuning"
    ],
    "authors": "Brandon McKinzie, Zhe Gan, Jean-Philippe Fauconnier, Sam Dodge, Bowen Zhang, et al.",
    "description": "本文探讨了构建高性能多模态大语言模型（MLLM）的关键因素，重点研究了架构组件与数据选择的重要性。通过对图像编码器、视觉-语言连接器以及预训练数据选择进行全面且深入的消融实验，研究团队总结了几项核心设计经验：首先，在进行大规模多模态预训练时，混合使用图像描述（Image-caption）、图文交错（Interleaved image-text）和纯文本数据，对于在多个基准测试中取得最先进的少样本（Few-shot）学习效果至关重要；其次，图像编码器、图像分辨率及视觉标记（Token）数量对模型性能有显著影响，而视觉-语言连接器的设计影响则相对较小。基于这些发现，研究者构建了名为 MM1 的多模态模型系列，其参数规模最高达 30B，包含稠密模型与混合专家（MoE）变体。凭借大规模预训练，MM1 展现出增强的上下文学习和多图推理能力，并在监督微调后于多个主流多模态基准测试中达到了极具竞争力的水平。",
    "image": "",
    "code": ""
  },
  {
    "title": "MoAI: Mixture of All Intelligence for Large Language and Vision Models",
    "venue": "arXiv",
    "date": "2024",
    "link": "https://arxiv.org/pdf/2403.07508.pdf",
    "source": "auto-script",
    "tags": [
      "Instruction Tuning"
    ],
    "authors": "Byung-Kwan Lee, Beomchan Park, Chae Won Kim, Yong Man Ro",
    "description": "MoAI（全智混合模型）提出了一种全新的多模态大语言模型（LLVM）架构，旨在通过整合专业计算机视觉（CV）模型的辅助信息来增强对真实世界场景的深入理解。与现有主要依赖大语言模型（LLM）骨干网能力的模型不同，MoAI 巧妙利用了外部视觉模型在分割、检测、场景图生成（SGG）及光学字符识别（OCR）等任务中的输出。该模型通过两个新模块运行：MoAI-Compressor 负责将外部 CV 模型的口语化输出进行对齐与压缩，以高效提取相关的辅助视觉信息；MoAI-Mixer 则利用混合专家（MoE）的概念，将原始视觉特征、外部辅助特征以及语言特征进行深度融合。实验表明，在不扩大模型规模或额外策划视觉指令微调数据集的情况下，MoAI 在众多零样本视觉语言任务中表现卓越，尤其在物体存在性、位置、关系及 OCR 等真实场景理解任务上显著超越了现有的开源和闭源模型。",
    "image": "",
    "code": "https://github.com/ByungKwanLee/MoAI"
  },
  {
    "title": "DeepSeek-VL: Towards Real-World Vision-Language Understanding",
    "venue": "arXiv",
    "date": "2024",
    "link": "https://arxiv.org/pdf/2403.05525",
    "source": "auto-script",
    "tags": [
      "Instruction Tuning"
    ],
    "authors": "Haoyu Lu, Wen Liu, Bo Zhang, Bingxuan Wang, Kai Dong, et al.",
    "description": "DeepSeek-VL 是一款专为真实世界视觉与语言理解应用设计的开源多模态模型。该研究围绕三个核心维度展开：首先，在数据构建上，强调多样性与可扩展性，广泛覆盖了网页截图、PDF、OCR、图表及知识类内容，并基于真实用户场景构建了指令微调数据集，显著提升了实际应用中的用户体验；其次，在架构设计上，模型引入了混合视觉编码器，能够在保持较低计算开销的同时，高效处理 1024 x 1024 的高分辨率图像，从而精准捕捉各类视觉任务中的关键语义与细节信息；最后，在训练策略上，为了保持强大的语言能力，研究团队探索了一种在预训练初期就融入大语言模型（LLM）训练的策略，并平衡了视觉与语言模态之间的竞争动态。实验表明，DeepSeek-VL 系列（包括 1.3B 和 7B 模型）作为视觉语言聊天机器人在实际应用中表现出色，在同等参数规模下，不仅在多项视觉语言基准测试中达到了领先或极具竞争力的水平，同时依然保持了强劲的纯语言处理能力。",
    "image": "",
    "code": "https://github.com/deepseek-ai/DeepSeek-VL"
  },
  {
    "title": "TextMonkey: An OCR-Free Large Multimodal Model for Understanding Document",
    "venue": "arXiv",
    "date": "2024",
    "link": "https://arxiv.org/pdf/2403.04473.pdf",
    "source": "auto-script",
    "tags": [
      "Instruction Tuning"
    ],
    "authors": "Yuliang Liu, Biao Yang, Qiang Liu, Zhang Li, Zhiyin Ma, Shuo Zhang, Xiang Bai",
    "description": "TextMonkey 是一款专门针对以文本为中心的多模态任务而设计的大型多模态模型（LMM）。该模型在多个维度进行了创新：首先，通过采用零初始化的移位窗口注意力机制（Shifted Window Attention），实现了在更高输入分辨率下的跨窗口连接，并稳定了早期训练过程；其次，研究提出图像中存在冗余标记（Tokens）的假设，利用相似度过滤掉非显著标记，在精简序列长度的同时提升了模型性能。此外，TextMonkey 将能力扩展至文本检测与定位（Text Spotting and Grounding），并在回答中引入位置信息以增强可解释性，同时通过微调学会了处理屏幕截图任务。在 12 项基准测试的评估中，该模型表现出色，在场景文本任务、文档导向任务及关键信息提取任务中分别实现了 5.2%、6.9% 和 2.8% 的增长。特别是在 OCRBench 综合评估中，TextMonkey 以 561 分的成绩刷新了开源模型在文档理解领域的标准。",
    "image": "",
    "code": "https://github.com/Yuliang-Liu/Monkey"
  },
  {
    "title": "The All-Seeing Project V2: Towards General Relation Comprehension of the Open World",
    "venue": "arXiv",
    "date": "2024",
    "link": "https://arxiv.org/pdf/2402.19474.pdf",
    "source": "auto-script",
    "tags": [
      "Instruction Tuning"
    ],
    "authors": "Weiyun Wang, Yiming Ren, Haowen Luo, Tiantong Li, Chenxiang Yan, et al.",
    "description": "All-Seeing Project V2 提出了全新的 ASMv2 模型和 AS-V2 数据集，旨在增强多模态大语言模型（MLLM）对图像中物体关系的理解能力。ASMv2 将文本生成、物体定位和关系理解统一整合为“关系对话（ReC）”任务，通过这种统一的表征方式，模型不仅能识别图像中的所有物体，还能掌握它们之间复杂的关联图谱，从而有效缓解了 MLLM 常见的关系幻觉问题。为了支持该领域的训练与评估，研究团队构建了首个高质量的关系对话数据集 AS-V2，并设计了名为 CRPE 的新基准测试，用于全面衡量模型的关系探测能力。实验结果显示，ASMv2 在该基准测试中取得了 52.04% 的准确率，大幅领先于 LLaVA-1.5 的 43.14%，为迈向通用人工智能提供了新的研究思路。",
    "image": "",
    "code": "https://github.com/OpenGVLab/all-seeing"
  },
  {
    "title": "GROUNDHOG: Grounding Large Language Models to Holistic Segmentation",
    "venue": "CVPR",
    "date": "2024",
    "link": "https://arxiv.org/pdf/2402.16846.pdf",
    "source": "auto-script",
    "tags": [
      "Instruction Tuning"
    ],
    "authors": "Yichi Zhang, Ziqiao Ma, Xiaofeng Gao, Suhaila Shakiah, Qiaozi Gao, Joyce Chai",
    "description": "GROUNDHOG 是一款旨在实现整体分割对齐的多模态大语言模型（MLLM），解决了传统模型仅依靠边界框（Bounding Box）进行定位而缺乏像素级表示的问题。该模型引入了掩码特征提取器，并将提取的特征转换为视觉实体标记（Visual Entity Tokens），使模型能够通过检索和合并实体掩码，将文本中的可定位短语与统一的定位掩码精准关联。为了训练该模型，研究团队通过整合多项具有丰富标注的分割数据集，构建了名为 M3G2 的多模态多粒度定位指令微调数据集。实验结果表明，GROUNDHOG 在无需特定任务微调的情况下，在多种语言定位任务中表现优异，并显著减少了目标幻觉现象。此外，该模型在处理复杂视觉输入时展现出更强的定位能力，并能为失效案例提供直观易懂的诊断信息。",
    "image": "",
    "code": "https://groundhog-mllm.github.io/"
  },
  {
    "title": "AnyGPT: Unified Multimodal LLM with Discrete Sequence Modeling",
    "venue": "arXiv",
    "date": "2024",
    "link": "https://arxiv.org/pdf/2402.12226.pdf",
    "source": "auto-script",
    "tags": [
      "Instruction Tuning"
    ],
    "authors": "Jun Zhan, Junqi Dai, Jiasheng Ye, Yunhua Zhou, Dong Zhang, et al.",
    "description": "AnyGPT 是一款实现了“任意模态到任意模态（any-to-any）”转换的多模态语言模型，它采用离散表示技术，将语音、文本、图像和音乐统一在同一个框架下进行处理。该模型最大的特点在于其高度的兼容性：无需对现有大语言模型（LLM）的架构或训练范式进行任何修改，仅通过数据层面的预处理，即可像添加新语言一样将新模态无缝集成到模型中。研究团队构建了一个以文本为中心的多模态预训练数据集用于模态对齐，并利用生成模型合成首个大规模“任意到任意”多模态指令数据集，其中包含 10.8 万条复杂交织了多种模态的多轮对话样本。实验结果证明，AnyGPT 能够胜任任意组合的多模态输入与输出对话，且其在各模态任务中的性能均可比肩专门的单模态模型，有力证明了离散表示在统一多模态语言模型中的有效性与便捷性。",
    "image": "",
    "code": "https://junzhan2000.github.io/AnyGPT.github.io/"
  },
  {
    "title": "Momentor: Advancing Video Large Language Model with Fine-Grained Temporal Reasoning",
    "venue": "arXiv",
    "date": "2024",
    "link": "https://arxiv.org/pdf/2402.11435.pdf",
    "source": "auto-script",
    "tags": [
      "Instruction Tuning"
    ],
    "authors": "Long Qian, Juncheng Li, Yu Wu, Yaobo Ye, Hao Fei, et al.",
    "description": "Momentor 是一款专注于提升视频大语言模型（Video-LLM）细粒度时序理解能力的模型，旨在解决现有模型仅能捕捉粗粒度语义而无法有效处理视频片段定位与理解的问题。为了支持 Momentor 的训练，研究团队设计了一套自动化数据生成引擎，构建了包含千万级片段级指令数据的规模化数据集 Moment-10M。通过在该数据集上的训练，Momentor 获得了强大的片段级推理与定位能力。在多项任务的零样本（Zero-shot）评估中，Momentor 在细粒度时序对齐理解及定位方面表现卓越，显著提升了模型对视频特定片段的处理精度。",
    "image": "",
    "code": "https://github.com/DCDmllm/Momentor"
  },
  {
    "title": "ALLaVA: Harnessing GPT4V-synthesized Data for A Lite Vision-Language Model",
    "venue": "arXiv",
    "date": "2024",
    "link": "https://arxiv.org/pdf/2402.11684.pdf",
    "source": "auto-script",
    "tags": [
      "Instruction Tuning"
    ],
    "authors": "Guiming Hardy Chen, Shunian Chen, Ruifei Zhang, Junying Chen, Xiangbo Wu, et al.",
    "description": "ALLaVA 旨在通过采用高质量训练数据，缩小传统规模多模态大语言模型（LVLM）与资源友好型轻量级版本之间的性能差距。该研究提出了一个综合的合成数据生成流水线，核心思路是利用强大的闭源商业模型生成 130 万条高质量样本，包括用于视觉-语言对齐的细粒度图像注释，以及用于视觉指令微调的复杂推理视觉问答对。通过在该合成数据集（命名为 ALLaVA）上进行训练，4B 规模的轻量级模型在 17 项基准测试中展现出极强的竞争力，甚至在多项测试中达到了 7B 或 13B 规模模型的性能水平。这项工作不仅证明了高质量数据在构建高效 LVLM 中的可行性，还将其数据集开源，以促进资源受限场景下多模态模型的发展。",
    "image": "",
    "code": "https://github.com/FreedomIntelligence/ALLaVA"
  },
  {
    "title": "CoLLaVO: Crayon Large Language and Vision mOdel",
    "venue": "arXiv",
    "date": "2024",
    "link": "https://arxiv.org/pdf/2402.11248.pdf",
    "source": "auto-script",
    "tags": [
      "Instruction Tuning"
    ],
    "authors": "Byung-Kwan Lee, Beomchan Park, Chae Won Kim, Yong Man Ro",
    "description": "CoLLaVO（Crayon 多模态大模型）探讨了视觉语言模型（VLM）是否真正具备高质量的物体级图像理解能力，并指出这种基础理解能力与模型在视觉语言任务上的零样本表现强相关。为了强化物体级感知，研究者提出了一种基于全景颜色图（Panoptic Color Maps）的新型视觉提示微调方案——Crayon Prompt，使模型能够更精准地处理“图像中有什么物体”或“特定边界框对应哪个物体”等任务。此外，该模型引入了 Dual QLoRA 学习策略，旨在视觉指令微调过程中保持物体的级图像理解能力而不发生遗忘。实验证明，CoLLaVO 在多项视觉语言基准测试的零样本设置下取得了显著的性能飞跃。",
    "image": "",
    "code": "https://github.com/ByungKwanLee/CoLLaVO"
  },
  {
    "title": "Prismatic VLMs: Investigating the Design Space of Visually-Conditioned Language Models",
    "venue": "ICML",
    "date": "2024",
    "link": "https://arxiv.org/pdf/2402.07865",
    "source": "auto-script",
    "tags": [
      "Instruction Tuning"
    ],
    "authors": "Siddharth Karamcheti, Suraj Nair, Ashwin Balakrishna, Percy Liang, Thomas Kollar, Dorsa Sadigh",
    "description": "Prismatic VLMs对视觉语言模型（VLM）的设计空间进行了系统性研究，旨在破解图像预处理、架构设计及优化策略等关键决策对模型性能影响不透明的僵局。该研究首先构建了一套涵盖视觉问答、物体定位及幻觉探测等任务的标准评估套件，为衡量 VLM 能力提供了细粒度视角。通过对预训练视觉表示、基础与指令微调语言模型的效果等核心变量进行严格消融实验，研究发现：多阶段训练并非必要，单阶段优化即可在降低 20%-25% 计算成本的同时保持性能；此外，融合 DINOv2 和 SigLIP 等不同偏置的视觉后端能显著提升模型表现。基于这些发现，研究团队推出了 PRISMs 系列模型（7B-13B 规模），其性能全面超越了 InstructBLIP 和 LLaVA v1.5 等开源 SOTA 模型，并同步开源了统一的评估框架、优化的训练代码及全部模型权重。",
    "image": "",
    "code": "https://github.com/TRI-ML/prismatic-vlms"
  },
  {
    "title": "CogCoM: Train Large Vision-Language Models Diving into Details through Chain of Manipulations",
    "venue": "arXiv",
    "date": "2024",
    "link": "https://arxiv.org/pdf/2402.04236.pdf",
    "source": "auto-script",
    "tags": [
      "Instruction Tuning"
    ],
    "authors": "Ji Qi, Ming Ding, Weihan Wang, Yushi Bai, Qingsong Lv, et al.",
    "description": "CogCoM 针对现有视觉语言模型（VLM）因过度追求指令对齐而忽视中间推理过程、导致无法处理精细视觉问题的缺陷，提出了一种受人类认知启发的新型机制——操作链（Chain of Manipulations, CoM）。该机制允许模型像人类一样，通过一系列显式的视觉操作（如定位标注、局部放大、OCR 识别等）逐步收集证据并解决问题，且无需依赖外部工具。为了实现这一机制，研究团队设计了一套能够自动生成大规模操作链数据的流水线，并构建了包含 170 亿参数（17B）的模型架构，使其具备处理多轮对话和多图输入的能力。此外，研究者还针对极具挑战性的图形数学问题手动标注了 6,000 条高质量样本。实验证明，CogCoM 在视觉问答、物体定位等 4 大类、9 项基准测试中达到了领先（SOTA）水平，在显著减少幻觉的同时，为用户提供了可追溯错误原因的可解释推理路径。",
    "image": "",
    "code": "https://github.com/zai-org/CogCoM"
  },
  {
    "title": "MobileVLM V2: Faster and Stronger Baseline for Vision Language Model",
    "venue": "arXiv",
    "date": "2024",
    "link": "https://arxiv.org/pdf/2402.03766.pdf",
    "source": "auto-script",
    "tags": [
      "Instruction Tuning"
    ],
    "authors": "Xiangxiang Chu, Limeng Qiao, Xinyu Zhang, Shuang Xu, Fei Wei, et al.",
    "description": "MobileVLM V2 是一系列经过显著改进的视觉语言模型，它是对前作 MobileVLM 的深度升级。该研究证明，通过精心设计的架构、专门为移动端 VLM 定制的改进训练方案，以及高质量数据集的精细策划，可以大幅提升轻量化模型的性能。具体而言，MobileVLM V2 的 1.7B 版本在标准基准测试中达到了甚至超越了许多 3B 规模的模型；而其 3B 版本更是表现出色，性能超越了众多参数量在 7B 以上的大型视觉语言模型。这一成果为在移动端等资源受限设备上部署高性能多模态模型提供了强有力的技术支撑。",
    "image": "",
    "code": "https://github.com/Meituan-AutoML/MobileVLM"
  },
  {
    "title": "GITA: Graph to Visual and Textual Integration for Vision-Language Graph Reasoning",
    "venue": "NeurIPS",
    "date": "2024",
    "link": "https://arxiv.org/pdf/2402.02130",
    "source": "auto-script",
    "tags": [
      "Instruction Tuning"
    ],
    "authors": "Yanbin Wei, Shuai Fu, Weisen Jiang, Zejian Zhang, Zhixiong Zeng, et al.",
    "description": "GITA（图文视觉集成框架） 旨在解决大语言模型（LLM）在处理图结构任务时忽视视觉信息的问题。虽然现有模型可以通过文本格式处理图信息，但人类通常更倾向于通过直观的视觉图像来理解结构信息并进行图推理。为了填补这一空白，该研究创新性地提出了一个名为 GITA 的端到端框架，首次将“视觉图”（Visual Graphs）引入通用的图推理过程中。",
    "image": "",
    "code": "https://github.com/WEIYanbin1999/GITA/"
  },
  {
    "title": "Enhancing Multimodal Large Language Models with Vision Detection Models: An Empirical Study",
    "venue": "arXiv",
    "date": "2024",
    "link": "https://arxiv.org/pdf/2401.17981.pdf",
    "source": "auto-script",
    "tags": [
      "Instruction Tuning"
    ],
    "authors": "Qirui Jiao, Daoyuan Chen, Yilun Huang, Yaliang Li, Ying Shen",
    "description": "虽然将检测信息以文本格式输入 MLLM 已被证明简单有效，但现有研究大多直接使用，并未深入探讨自适应训练的潜力。研究团队系统地对比了免训练、重新训练和微调三种策略，旨在研究训练如何影响模型对注入信息的理解。实验结果表明，在 10 个主流基准测试中，对预训练好的 MLLM 进行微调以融入文本检测信息效果最佳，性能提升了 6.71%，且微调过程能让模型更智能地筛选冗余信息并专注于任务细节。此外，研究还发现经过微调后的模型具有良好的可置换性，即使更换底层的视觉检测模型，MLLM 依然能保持性能增强，这表明模型真正理解了格式化数据而非单纯记忆。该研究通过揭示训练策略的重要性，为提升 MLLM 的细粒度多模态能力提供了新的路径。",
    "image": "",
    "code": ""
  },
  {
    "title": "LLaVA-NeXT: Improved reasoning, OCR, and world knowledge",
    "venue": "Blog",
    "date": "2024",
    "link": "https://llava-vl.github.io/blog/2024-01-30-llava-next/",
    "source": "auto-script",
    "tags": [
      "Instruction Tuning"
    ],
    "authors": "Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, Yong Jae Lee",
    "description": "LLaVA-NeXT是多模态大模型 LLaVA 系列的一次重大更新，旨在提升模型在推理、OCR 能力以及世界知识方面的表现。相较于之前的 LLaVA-1.5，其核心改进体现在四个方面：首先是支持动态高分辨率输入，通过将图像像素增加至原来的 4 倍，使模型能够捕捉更精细的视觉细节，并有效减少了低分辨率带来的幻觉问题；其次是数据组合的优化，模型引入了更高质量的视觉指令遵循数据集，并针对文档、图表等特定场景进行了强化，甚至展现出了惊人的零样本中文处理能力；第三是骨干网络的扩展，除了原有的 Vicuna 外，还集成了 Mistral-7B 和 Yi-34B 等更强大的大语言模型，使其在多个基准测试中超越了 Gemini Pro；最后，LLaVA-NeXT 依然保持了极高的数据效率和低廉的训练成本，最大的 34B 版本仅需 32 张 A100 显卡训练约一天即可完成，且全套代码、模型和数据均向社区开源。",
    "image": "",
    "code": "https://github.com/haotian-liu/LLaVA"
  },
  {
    "title": "MoE-LLaVA: Mixture of Experts for Large Vision-Language Models",
    "venue": "arXiv",
    "date": "2024",
    "link": "https://arxiv.org/pdf/2401.15947.pdf",
    "source": "auto-script",
    "tags": [
      "Instruction Tuning"
    ],
    "authors": "Bin Lin, Zhenyu Tang, Yang Ye, Jinfa Huang, Junwu Zhang, et al.",
    "description": "本文介绍了 MoE-LLaVA，这是一种基于混合专家模型（MoE）的稀疏多模态大语言模型架构，旨在解决视觉语言模型在参数规模扩展时面临的高昂计算成本问题。该研究提出了一种名为 MoE-Tuning 的训练策略，创新性地解决了多模态稀疏学习中常见的性能退化问题，在构建拥有海量参数模型的同时，确保其计算成本保持恒定。MoE-LLaVA 在部署时通过路由器仅激活前 $k$ 个专家，使其余专家保持闲置状态，从而实现高效推理。实验结果显示，MoE-LLaVA 仅凭借约 3B 的激活参数，就在多项视觉理解任务上达到了与 LLaVA-1.5-7B 相当的水平，甚至在物体幻觉基准测试中超越了 LLaVA-1.5-13B。该工作不仅为稀疏多模态模型建立了性能基准，也为开发更高效、更具扩展性的多模态学习系统提供了宝贵的实证参考。",
    "image": "",
    "code": "https://github.com/PKU-YuanGroup/MoE-LLaVA"
  },
  {
    "title": "InternLM-XComposer2: Mastering Free-form Text-Image Composition and Comprehension in Vision-Language Large Model",
    "venue": "arXiv",
    "date": "2024",
    "link": "https://arxiv.org/pdf/2401.16420.pdf",
    "source": "auto-script",
    "tags": [
      "Instruction Tuning"
    ],
    "authors": "Xiaoyi Dong, Pan Zhang, Yuhang Zang, Yuhang Cao, Bin Wang, et al.",
    "description": "本文介绍了 InternLM-XComposer2，这是一款在自由格式图文创作与理解方面表现卓越的前沿视觉语言模型。该模型突破了传统的视觉理解局限，能够根据大纲、详细文本说明或参考图像等多样化输入，熟练地创作出图文交错的内容，实现了高度定制化的内容生成。InternLM-XComposer2 提出了一种局部 LoRA（PLoRA）方法，通过仅在图像标记上应用额外的 LoRA 参数，在保持预训练语言知识完整性的同时，实现了精准的视觉理解与富有文采的文本创作之间的平衡。实验结果表明，基于 InternLM2-7B 构建的 InternLM-XComposer2 在生成高质量长篇多模态内容方面具有显著优势，并在多项视觉语言理解基准测试中表现惊人，不仅大幅超越了现有的多模态模型，在某些评估中甚至达到或超过了 GPT-4V 和 Gemini Pro 的水平，彰显了其在多模态理解领域的极高造诣。",
    "image": "",
    "code": "https://github.com/InternLM/InternLM-XComposer"
  },
  {
    "title": "Yi-VL",
    "venue": "Research Community",
    "date": "2024",
    "link": "https://github.com/01-ai/Yi/tree/main/VL",
    "source": "auto-script",
    "tags": [
      "Instruction Tuning"
    ],
    "authors": "01.AI",
    "description": "Yi-VL 是基于 Yi 系列大语言模型开发的多模态版本，包含 Yi-VL-6B 和 Yi-VL-34B 两个版本。该模型架构由三个核心部分组成：视觉编码器（采用 ViT-H/14 预训练模型）、投影模块（实现视觉与语言特征的对齐）以及大语言模型（基于 Yi-6B/34B Chat 模型）。Yi-VL 采用了三阶段的训练策略，从最初的图像-文本对齐预训练，到多模态指令微调，再到最终的全参数指令微调，旨在建立强大的视觉理解与指令遵循能力。在性能表现上，Yi-VL 在 MMMU 等多模态基准测试中展现了卓越的水平，不仅在同等规模的开源模型中处于领先地位，甚至在部分测试中挑战了闭源模型的表现。该模型支持 448x448 的高分辨率图像输入，能够精准识别并理解图像中的复杂细节，是零一万物大模型生态中重要的多模态基础设施。",
    "image": "",
    "code": "https://github.com/01-ai/Yi/tree/main/VL"
  },
  {
    "title": "SpatialVLM: Endowing Vision-Language Models with Spatial Reasoning Capabilities",
    "venue": "arXiv",
    "date": "2024",
    "link": "https://arxiv.org/pdf/2401.12168.pdf",
    "source": "auto-script",
    "tags": [
      "Instruction Tuning"
    ],
    "authors": "Boyuan Chen, Zhuo Xu, Sean Kirmani, Brian Ichter, Danny Driess, et al.",
    "description": "本文介绍了 SpatialVLM，旨在解决现有视觉语言模型（VLM）在 3D 空间推理能力上的短板，特别是难以识别物体间的距离、尺寸差异等定量物理关系的问题。研究团队认为这一局限性源于训练数据中 3D 空间知识的匮乏，因此提出了一套自动化生成系统，通过在 1000 万张真实图像上构建包含 20 亿个 VQA 样本的互联网级空间推理数据集。该项工作首次在度量空间中实现了大规模 3D 空间数据采集，并深入探讨了数据质量、训练流程和模型架构对性能的影响。实验证明，在该数据上训练出的 VLM 在定性和定量空间推理任务中均取得了显著提升。此外，这种定量估计能力还赋予了模型在思维链空间推理以及机器人任务规划等下游应用中的全新潜力。",
    "image": "",
    "code": "https://spatial-vlm.github.io/"
  },
  {
    "title": "ChartAssisstant: A Universal Chart Multimodal Language Model via Chart-to-Table Pre-training and Multitask Instruction Tuning",
    "venue": "ACL",
    "date": "2024",
    "link": "https://arxiv.org/pdf/2401.02384",
    "source": "auto-script",
    "tags": [
      "Instruction Tuning"
    ],
    "authors": "Fanqing Meng, Wenqi Shao, Quanfeng Lu, Peng Gao, Kaipeng Zhang, et al.",
    "description": "本文介绍了 ChartAssistant，这是一款专为通用图表理解与推理而设计的视觉语言模型，旨在解决通用多模态模型在处理图表特有的图形元素（如条形、折线）与文本组件（如标签、图例）组合时面临的挑战。该模型利用大规模数据集 ChartSFT 进行训练，该数据集涵盖了从基础的柱状图、饼图到专业的雷达图、气泡图等多种图表类型，并包含了多样化的图表相关任务。ChartAssistant 采用两阶段训练方案：首先通过“图表转表格”的解析预训练实现图文对齐，随后进行多任务指令遵循微调。实验结果表明，ChartAssistant 在多项图表任务中表现出极强的竞争力，显著优于 UniChart 和 Chartllama 等现有最先进模型，尤其在真实世界图表数据的零样本设置下展现出了卓越的泛化能力。",
    "image": "",
    "code": "https://github.com/OpenGVLab/ChartAst"
  },
  {
    "title": "MobileVLM : A Fast, Reproducible and Strong Vision Language Assistant for Mobile Devices",
    "venue": "arXiv",
    "date": "2023",
    "link": "https://arxiv.org/pdf/2312.16886.pdf",
    "source": "auto-script",
    "tags": [
      "Instruction Tuning"
    ],
    "authors": "Xiangxiang Chu, Limeng Qiao, Xinyang Lin, Shuang Xu, Yang Yang, et al.",
    "description": "本文介绍了 MobileVLM，这是一款专门针对移动设备优化的高性能多模态视觉语言模型（MMVLM）。该模型融合了大量面向移动端的架构设计与技术方案，核心组件包括从零开始训练的 1.4B 和 2.7B 参数规模的轻量级语言模型、采用 CLIP 方式预训练的多模态视觉模型，以及一个用于跨模态交互的高效投影器。实验评估显示，MobileVLM 在多个典型的视觉语言模型基准测试中表现出色，性能可比肩规模更大的模型。更重要的是，在实际部署测试中，该模型在高通骁龙 888 CPU 和 NVIDIA Jetson Orin GPU 上分别实现了每秒 21.5 个和 65.3 个 Token 的推理速度，达到了目前业内领先的水平。",
    "image": "",
    "code": "https://github.com/Meituan-AutoML/MobileVLM"
  },
  {
    "title": "InternVL: Scaling up Vision Foundation Models and Aligning for Generic Visual-Linguistic Tasks",
    "venue": "CVPR",
    "date": "2023",
    "link": "https://arxiv.org/pdf/2312.14238.pdf",
    "source": "auto-script",
    "tags": [
      "Instruction Tuning"
    ],
    "authors": "Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, et al.",
    "description": "本文介绍了 InternVL，这是一个大规模视觉语言基座模型，旨在缩小视觉基座模型与大语言模型（LLM）之间的发展差距。研究团队将视觉基座模型的参数规模扩展至 60 亿（6B），并利用来自多种渠道的互联网级图文数据，将其分阶段地与大语言模型进行对齐。InternVL 具有极其强大的视觉感知能力，在涵盖图像级与像素级识别、零样本图文/视频分类与检索等 32 项通用视觉语言基准测试中均取得了领先（SOTA）的表现。该模型不仅可以作为 ViT-22B 的强力替代方案，还能与 LLM 无缝连接以构建高性能的多模态对话系统，为迈向多模态通用人工智能（AGI）提供了坚实的基础设施。",
    "image": "",
    "code": "https://github.com/OpenGVLab/InternVL"
  },
  {
    "title": "Osprey: Pixel Understanding with Visual Instruction Tuning",
    "venue": "CVPR",
    "date": "2023",
    "link": "https://arxiv.org/pdf/2312.10032.pdf",
    "source": "auto-script",
    "tags": [
      "Instruction Tuning"
    ],
    "authors": "Yuqian Yuan, Wentong Li, Jian Liu, Dongqi Tang, Xinjie Luo, et al.",
    "description": "本文介绍了 Osprey，这是一种基于掩码-文本指令微调（mask-text instruction tuning）的方法，旨在将多模态大语言模型（MLLM）的理解能力从图像级或框级（box-level）延伸到像素级。为了解决当前 MLLM 缺乏细粒度视觉语言对齐以及掩码类指令数据不足的问题，研究团队首先精心构建了一个包含 724K 样本的掩码区域-文本数据集，并设计了一个将像素级表示注入大语言模型的视觉语言架构。具体而言，Osprey 采用了卷积 CLIP 作为视觉编码器，并利用掩码感知视觉提取器从高分辨率输入中获取精确的视觉掩码特征。实验结果表明，Osprey 在多项区域理解任务中展现出卓越性能，能够实现精细的像素级指令遵循，并能与 Segment Anything Model (SAM) 无缝集成，从而获得多粒度的语义理解能力。",
    "image": "",
    "code": "https://github.com/CircleRadon/Osprey"
  },
  {
    "title": "CogAgent: A Visual Language Model for GUI Agents",
    "venue": "arXiv",
    "date": "2023",
    "link": "https://arxiv.org/pdf/2312.08914.pdf",
    "source": "auto-script",
    "tags": [
      "Instruction Tuning"
    ],
    "authors": "Wenyi Hong, Weihan Wang, Qingsong Lv, Jiazheng Xu, Wenmeng Yu, et al.",
    "description": "本文介绍了 CogAgent，这是一个拥有 180 亿参数、专为图形用户界面（GUI）理解与导航设计的视觉语言模型。针对大语言模型难以直接理解屏幕内容从而限制自动化水平的问题，CogAgent 创新性地采用了低分辨率与高分辨率双图像编码器设计，支持高达1120×1120像素的图像输入，使其能够精准识别页面上的微小元素和文字信息。作为一款通用的视觉语言模型，CogAgent 在 VQAv2、DocVQA、ChartQA 等 9 项涵盖文本丰富场景及通用问答的基准测试中均取得了领先成绩。特别是在 PC 端和安卓端的 GUI 导航任务中，CogAgent 仅凭借屏幕截图作为输入，其表现便超越了以往那些依赖提取 HTML 文本的传统方法，显著提升了自动化代理在理解和操作数字设备界面方面的技术前沿。",
    "image": "",
    "code": "https://github.com/zai-org/CogAgent"
  },
  {
    "title": "Pixel Aligned Language Models",
    "venue": "arXiv",
    "date": "2023",
    "link": "https://arxiv.org/pdf/2312.09237.pdf",
    "source": "auto-script",
    "tags": [
      "Instruction Tuning"
    ],
    "authors": "Jiarui Xu, Xingyi Zhou, Shen Yan, Xiuye Gu, Anurag Arnab, et al.",
    "description": "本文介绍了 Pixel-Aligned Language Model（PALM），旨在解决视觉语言模型如何有效处理定位任务（如词语对齐或指代定位）的问题。该模型创新性地实现了将位置信息（如点或边界框）作为输入或输出的能力：当位置作为输入时，模型执行受控区域描述任务，针对特定区域生成文本；当位置作为输出时，模型会为语言模型生成的每个单词回归其像素坐标，从而实现密集的词语级对齐。该模型在包含人类注意力对齐数据的 Localized Narrative 数据集上进行预训练，使其能够精准掌握视觉像素与文本单词之间的关联。实验证明，该模型在指代定位、特定位置描述和密集目标描述等多种位置感知任务中表现卓越，并在 RefCOCO 和 Visual Genome 等主流基准测试中取得了领先的性能。",
    "image": "",
    "code": "https://github.com/google-research/scenic/tree/main/scenic/projects/pixel_llm"
  },
  {
    "title": "VILA: On Pre-training for Visual Language Models",
    "venue": "CVPR",
    "date": "2023",
    "link": "https://arxiv.org/pdf/2312.07533",
    "source": "auto-script",
    "tags": [
      "Instruction Tuning"
    ],
    "authors": "Ji Lin, Hongxu Yin, Wei Ping, Yao Lu, Pavlo Molchanov, et al.",
    "description": "本文介绍了 VILA，这是一个通过系统性研究视觉语言预训练（VLP）流程而构建的视觉语言模型系列。针对目前多模态研究中对视觉-语言联合建模过程缺乏深入探讨的问题，研究团队通过受控的步骤对比，总结出了三个关键结论：首先，在预训练期间冻结大语言模型（LLM）虽然能获得不错的零样本性能，但会丧失语境学习（In-context Learning）能力，因此必须解除冻结；其次，交错式的图文预训练数据（Interleaved Data）对模型性能至关重要，仅依赖图文对数据并非最优解；最后，在指令微调阶段将纯文本指令数据重新混入图文数据中，不仅能防止模型在纯文本任务上的退化，还能提升视觉语言任务的准确率。凭借这套优化后的预训练方案，VILA 系列模型在无需复杂技巧的情况下，于多个主流基准测试中全面超越了 LLaVA-1.5 等先进模型。此外，多模态预训练还赋予了 VILA 强大的多图推理能力、增强的语境学习能力以及更丰富的世界知识。",
    "image": "",
    "code": "https://github.com/NVlabs/VILA"
  },
  {
    "title": "See, Say, and Segment: Teaching LMMs to Overcome False Premises",
    "venue": "arXiv",
    "date": "2023",
    "link": "https://arxiv.org/pdf/2312.08366.pdf",
    "source": "auto-script",
    "tags": [
      "Instruction Tuning"
    ],
    "authors": "Tsung-Han Wu, Giscard Biamby, David Chan, Lisa Dunlap, Ritwik Gupta, et al.",
    "description": "本文介绍了针对大型多模态模型中“虚假前提”识别难题的研究。现有的开源模型虽然在语言定位和分割任务上表现优异，但在面对暗示图像中存在实际不存在物体的错误查询时，往往会产生严重的幻觉，且现有的分割微调方法会导致模型在判断物体是否存在以及与人自然交互的能力上出现灾难性遗忘。为此，作者提出了一种级联与联合训练方法，使模型能够同时具备准确检测物体是否存在、在物体不存在时向用户提供纠正性语言反馈、以及在物体存在时输出精确掩码的三重能力。此外，研究团队还通过扩展 RefCOCO 系列数据集推出了全新的虚假前提纠正基准数据集。实验结果显示，该方法在虚假前提检测方面比现有方法提升了高达 55%，在错误前提条件下的分割性能相对提升超过 31%，且生成的自然语言反馈在 67% 的情况下被认为对用户有实际帮助。",
    "image": "",
    "code": "https://see-say-segment.github.io/"
  },
  {
    "title": "Vary: Scaling up the Vision Vocabulary for Large Vision-Language Models",
    "venue": "ECCV",
    "date": "2023",
    "link": "https://arxiv.org/pdf/2312.06109.pdf",
    "source": "auto-script",
    "tags": [
      "Instruction Tuning"
    ],
    "authors": "Haoran Wei, Lingyu Kong, Jinyue Chen, Liang Zhao, Zheng Ge, et al.",
    "description": "本文介绍了 Vary，这是一种旨在有效扩展大型视觉语言模型（LVLM）视觉词汇量的高效方法。针对目前主流模型普遍采用的 CLIP 视觉词汇在处理文档级 OCR、图表理解等细粒度视觉感知任务（尤其是非英语场景）时存在的编码效率低甚至词汇表溢出问题，Vary 提出了两阶段的改进方案：首先，通过设计一个专门的词汇网络结合微型解码器架构，利用自回归方式生成所需的全新视觉词汇；随后，将这一新词汇与原始的 CLIP 词汇进行融合，使模型能够在保留原有通用能力的基础上，快速获得新的视觉特征。实验结果表明，相较于 BLIP-2、MiniGPT-4 和 LLaVA 等热门模型，Vary 在保持基础能力的同时，显著增强了细粒度感知能力，尤其在文档解析（如 OCR 或 Markdown 转换）方面表现出色，其在 DocVQA 和 MMVet 基准测试中分别取得了 78.2% 的 ANLS 评分和 36.2% 的准确率。",
    "image": "",
    "code": "https://varybase.github.io/"
  },
  {
    "title": "Honeybee: Locality-enhanced Projector for Multimodal LLM",
    "venue": "CVPR",
    "date": "2023",
    "link": "https://arxiv.org/pdf/2312.06742.pdf",
    "source": "auto-script",
    "tags": [
      "Instruction Tuning"
    ],
    "authors": "Junbum Cha, Wooyoung Kang, Jonghwan Mun, Byungseok Roh",
    "description": "本文介绍了 Honeybee，这是一项专注于优化多模态大语言模型（MLLM）中视觉投影器（Visual Projector）设计的研究。研究者首先指出，视觉投影器在连接预训练视觉编码器与大语言模型中起着至关重要但常被忽视的作用，并确定了其两个核心属性：一是能够灵活管理视觉标记数量以确保模型的整体效率，二是能够保留视觉特征中的局部上下文信息以实现精准的空间理解。基于这些发现，研究团队提出了一种兼具灵活性与局部增强特性的新型投影器设计，同时配合了一套能够有效利用多元化指令数据集的综合策略。通过广泛的对比实验，该研究深入分析了各项设计决策的影响。最终，Honeybee 在 MME、MMBench、SEED-Bench 和 LLaVA-Bench 等多个权威基准测试中显著超越了以往的最先进方法，在大幅提升推理效率的同时实现了更强的多模态理解性能。",
    "image": "",
    "code": "https://github.com/khanrc/honeybee"
  },
  {
    "title": "Gemini: A Family of Highly Capable Multimodal Models",
    "venue": "Google",
    "date": "2023",
    "link": "https://storage.googleapis.com/deepmind-media/gemini/gemini_1_report.pdf",
    "source": "auto-script",
    "tags": [
      "Instruction Tuning",
      "Foundation Models"
    ],
    "authors": "Google Gemini Team",
    "description": "本文介绍了 Gemini 这一全新的多模态模型家族，该系列模型在图像、音频、视频及文本理解方面展现出了卓越的能力。Gemini 家族包含 Ultra、Pro 和 Nano 三种尺寸，能够满足从复杂推理任务到内存受限的端侧应用等多种场景需求。在广泛的基准测试评估中，最强悍的 Gemini Ultra 模型在 32 个基准测试中取得了 30 项领先成绩，不仅成为首个在 MMLU 测试中达到人类专家水平的模型，还在所有 20 个多模态基准测试中刷新了行业记录。",
    "image": "",
    "code": ""
  },
  {
    "title": "OneLLM: One Framework to Align All Modalities with Language",
    "venue": "arXiv",
    "date": "2023",
    "link": "https://arxiv.org/pdf/2312.03700.pdf",
    "source": "auto-script",
    "tags": [
      "Instruction Tuning"
    ],
    "authors": "Jiaming Han, Kaixiong Gong, Yiyuan Zhang, Jiaqi Wang, Kaipeng Zhang, et al.",
    "description": "本文介绍了 OneLLM，这是一个通过统一框架将图像、音频、视频、点云、深度图/法线图、惯性测量单元（IMU）以及 fMRI 脑活动等八种模态与语言进行对齐的多模态大语言模型。针对现有模型过度依赖特定模态编码器且架构各异的局限性，OneLLM 创新性地设计了一个统一的多模态编码器和渐进式对齐流水线：首先通过训练图像投影模块连接视觉编码器与大语言模型（LLM），随后利用多个投影模块的混合与动态路由构建出通用投影模块（UPM），并最终通过 UPM 将更多模态逐步对齐至 LLM。为了充分发挥模型在指令遵循方面的潜力，研究团队还精心策划了一个包含 200 万条目的综合多模态指令数据集。实验评估表明，OneLLM 在涵盖多模态描述、问答和推理等任务的 25 个不同基准测试中均表现出色，证明了其在处理多种非通用模态任务时的卓越性能。",
    "image": "",
    "code": "https://github.com/csuhan/OneLLM"
  },
  {
    "title": "Lenna: Language Enhanced Reasoning Detection Assistant",
    "venue": "arXiv",
    "date": "2023",
    "link": "https://arxiv.org/pdf/2312.02433.pdf",
    "source": "auto-script",
    "tags": [
      "Instruction Tuning"
    ],
    "authors": "Fei Wei, Xinyu Zhang, Ailing Zhang, Bo Zhang, Xiangxiang Chu",
    "description": "本文介绍了 Lenna，这是一款语言增强的推理检测助手，旨在挖掘和利用大语言模型（LLM）中蕴含的强大推理能力与世界知识来处理图像感知任务。针对现有模型在利用多模态特征的同时难以保留精确检测定位信息的问题，Lenna 通过在多模态大语言模型（MLLM）的词汇表中引入一个特殊的 <DET> 标记，该标记不包含显式的语义上下文，而是作为引导检测器识别对应位置的提示。为了评估 Lenna 的推理性能，研究团队构建了 ReasonDet 数据集，专门用于衡量基于推理的检测任务。实验结果表明，Lenna 在 ReasonDet 上表现优异，且具有极低的训练成本，在扩展至其他任务时也仅需极小的迁移开销。",
    "image": "",
    "code": "https://github.com/Meituan-AutoML/Lenna"
  },
  {
    "title": "VaQuitA: Enhancing Alignment in LLM-Assisted Video Understanding",
    "venue": "arXiv",
    "date": "2023",
    "link": "https://arxiv.org/pdf/2312.02310.pdf",
    "source": "auto-script",
    "tags": [
      "Instruction Tuning"
    ],
    "authors": "Yizhou Wang, Ruiyi Zhang, Haoliang Wang, Uttaran Bhattacharya, Yun Fu, Gang Wu",
    "description": "本文介绍了 VaQuitA，这是一个旨在优化视频与文本信息协同效应的前沿多模态框架。针对以往研究仅简单将视频特征映射为标记（tokens）导致效率低下的问题，VaQuitA 在数据和特征两个层面进行了创新。在数据层面，该框架弃用了传统的均匀采样，转而采用基于 CLIP 分数排名的引导采样方法，从而能够选出与所提问题契合度更高的视频帧；在特征层面，它集成了可训练的视频感知器（Video Perceiver）与视觉查询变换器（VQ-Former），显著增强了输入问题与视频特征之间的交互能力。此外，研究发现只需在模型输入中加入简单的提示词“请保持挑剔”（Please be critical），即可大幅提升模型的视频理解表现。实验结果表明，VaQuitA 在零样本视频问答任务中屡创新高，并能与用户进行高质量的多轮视频对话。",
    "image": "",
    "code": ""
  },
  {
    "title": "TimeChat: A Time-sensitive Multimodal Large Language Model for Long Video Understanding",
    "venue": "arXiv",
    "date": "2023",
    "link": "https://arxiv.org/pdf/2312.02051.pdf",
    "source": "auto-script",
    "tags": [
      "Instruction Tuning"
    ],
    "authors": "Shuhuai Ren, Linli Yao, Shicheng Li, Xu Sun, Lu Hou",
    "description": "本文介绍了 TimeChat，这是一个专门为长视频理解设计的具有时间感知能力的多模态大语言模型。针对长视频处理的挑战，该模型在架构上做出了两项核心贡献：一是引入了时间戳感知帧编码器，将视觉内容与每一帧的时间戳进行绑定；二是设计了滑动视频 Q-Former，能够产生变长的视频标记序列以适应不同时长的视频需求。此外，研究团队还构建了一个包含 6 类任务、总计 12.5 万个实例的指令微调数据集，以增强模型的指令遵循性能。实验结果表明，TimeChat 在密集视频描述、时序定位和高光检测等任务中展现出强大的零样本时序定位与推理能力，在 YouCook2、QVHighlights 和 Charades-STA 等基准测试中的表现均显著优于现有的最先进视频大模型。作为一款多功能视频助手，TimeChat 在长视频理解任务中具有极大的应用潜力，能够更好地满足现实场景下的用户需求。",
    "image": "",
    "code": "https://github.com/RenShuhuai-Andy/TimeChat"
  },
  {
    "title": "Making Large Multimodal Models Understand Arbitrary Visual Prompts",
    "venue": "CVPR",
    "date": "2023",
    "link": "https://arxiv.org/pdf/2312.00784.pdf",
    "source": "auto-script",
    "tags": [
      "Instruction Tuning"
    ],
    "authors": "Mu Cai, Haotian Liu, Dennis Park, Siva Karthik Mustikovela, Gregory P. Meyer, et al.",
    "description": "本文介绍了 ViP-LLM（基于视觉提示的多模态大模型），旨在解决现有大模型在特定区域理解能力上的缺失，以及空间坐标或编码方式对用户不够友好的问题。研究团队提出了一种能够解码任意“视觉提示”（Visual Prompts）的新型模型，使用户可以通过在图像上直接标记红色框、箭头等直观自然的方式与模型交互。该设计采用简单而高效的方案，直接将视觉标记叠加在原始 RGB 图像上，无需复杂的区域编码技术，却在 Visual7W、PointQA 和视觉常识推理（VCR）等区域理解基准测试中取得了领先性能。此外，该研究还推出了 ViP-Bench 基准测试，用于从多个维度全面评估模型对视觉提示的理解能力，为该领域的后续研究提供了重要工具。",
    "image": "",
    "code": "https://vip-llava.github.io/"
  },
  {
    "title": "Dolphins: Multimodal Language Model for Driving",
    "venue": "arXiv",
    "date": "2023",
    "link": "https://arxiv.org/pdf/2312.00438.pdf",
    "source": "auto-script",
    "tags": [
      "Instruction Tuning"
    ],
    "authors": "Yingzi Ma, Yulong Cao, Jiachen Sun, Marco Pavone, Chaowei Xiao",
    "description": "本文介绍了 Dolphins，这是一款创新的视觉语言模型，旨在通过模拟人类的理解和反应能力，构建一个对话式自动驾驶助手。Dolphins 能够处理由视频或图像数据、文本指令及历史控制信号组成的多模态输入，并根据指令生成明智的输出结果。该研究基于开源的预训练模型 OpenFlamingo，首先通过创新的接地思维链过程增强了模型的推理能力，随后利用专门构建的驾驶领域指令数据进行微调，使其更好地适应驾驶环境。通过整合 BDD-X 数据集，研究者在 Dolphins 中设计并巩固了四项不同的自动驾驶任务，从而提升其对复杂驾驶场景的整体理解。Dolphins 的独特优势主要体现在两个维度：首先，它能够深入理解复杂且长尾的开放世界驾驶场景，并解决一系列自动驾驶任务；其次，它展现出了类似人类的进阶能力，包括通过语境学习实现无梯度的即时任务适应，以及通过自我反思实现错误恢复。",
    "image": "",
    "code": "https://vlm-driver.github.io/"
  },
  {
    "title": "LL3DA: Visual Interactive Instruction Tuning for Omni-3D Understanding, Reasoning, and Planning",
    "venue": "arXiv",
    "date": "2023",
    "link": "https://arxiv.org/pdf/2311.18651.pdf",
    "source": "auto-script",
    "tags": [
      "Instruction Tuning"
    ],
    "authors": "Sijin Chen, Xin Chen, Chi Zhang, Mingsheng Li, Gang Yu, et al.",
    "description": "本文介绍了 LL3DA，这是一款旨在解决复杂 3D 环境下多模态理解与规划难题的大型语言 3D 助手。针对现有研究主要依赖多视图图像映射导致计算开销巨大且性能下降的问题，LL3DA 直接将点云作为输入，能够同时响应文本指令和视觉提示，从而帮助多模态大模型（LMM）更好地理解人类交互，并有效消除凌乱 3D 场景中的歧义。实验结果表明，LL3DA 在 3D 密集描述和 3D 问答任务上均取得了显著成果，性能超越了现有的各类 3D 视觉语言模型。",
    "image": "",
    "code": "https://ll3da.github.io/"
  },
  {
    "title": "VTimeLLM: Empower LLM to Grasp Video Moments",
    "venue": "arXiv",
    "date": "2023",
    "link": "https://arxiv.org/pdf/2311.18445.pdf",
    "source": "auto-script",
    "tags": [
      "Instruction Tuning"
    ],
    "authors": "Bin Huang, Xin Wang, Hong Chen, Zihan Song, Wenwu Zhu",
    "description": "本文介绍了 VTimeLLM，这是一种专为精细化视频片段理解和时序边界推理而设计的新型视频大语言模型，旨在解决现有模型仅能提供粗略描述而无法捕捉特定事件精确起止时间的问题。该模型采用了一种边界感知的阶段式训练策略：第一阶段利用图文对进行特征对齐；第二阶段通过多事件视频增强模型对时序边界的感知；第三阶段利用高质量视频指令微调提升时序理解能力并对齐人类意图。实验结果表明，在视频时序定位和密集视频描述等精细化时序理解任务中，VTimeLLM 的表现显著优于现有的视频大语言模型。此外，凭借对视频精细时序的深度理解，VTimeLLM 在视频对话基准测试中也超越了现有模型，展现出卓越的跨模态理解与推理能力。",
    "image": "",
    "code": "https://github.com/huangb23/VTimeLLM"
  },
  {
    "title": "mPLUG-PaperOwl: Scientific Diagram Analysis with the Multimodal Large Language Model",
    "venue": "arXiv",
    "date": "2023",
    "link": "https://arxiv.org/pdf/2311.18248.pdf",
    "source": "auto-script",
    "tags": [
      "Instruction Tuning"
    ],
    "authors": "Anwen Hu, Yaya Shi, Haiyang Xu, Jiabo Ye, Qinghao Ye, et al.",
    "description": "本文介绍了旨在提升多模态大语言模型图表分析能力的科研写作助手研究，通过解析高质量论文的 LaTeX 源文件，构建了多模态图表理解数据集 M-Paper。该数据集通过将论文图表与相关段落对齐，构建了专业的图表分析样本，是首个支持对包括图片、表格或 LaTeX 代码在内的多个科学图表进行联合理解的数据集。此外，为使助手更符合用户意图，研究引入了“大纲”作为控制信号，用户可以直接提供或基于自动生成的版本进行修改。实验表明，在该数据集上训练的模型在图表描述、图表分析和大纲推荐等科学图表理解任务中表现出更强的性能。",
    "image": "",
    "code": "https://github.com/X-PLUG/mPLUG-DocOwl/tree/main/PaperOwl"
  },
  {
    "title": "LLaMA-VID: An Image is Worth 2 Tokens in Large Language Models",
    "venue": "arXiv",
    "date": "2023",
    "link": "https://arxiv.org/pdf/2311.17043.pdf",
    "source": "auto-script",
    "tags": [
      "Instruction Tuning"
    ],
    "authors": "Yanwei Li, Chengyao Wang, Jiaya Jia",
    "description": "本文介绍了 LLaMA-VID，这是一种旨在解决视觉语言模型在处理视频和图像理解时面临的标记生成挑战的新方法。现有模型在处理长视频时因视觉标记过多而面临巨大的计算负担，LLaMA-VID 通过用两个独特的标记（即上下文标记和内容标记）表示每一帧来解决这一问题。上下文标记根据用户输入编码图像的整体语境，而内容标记则封装了每一帧的视觉线索。这种双标记策略在显著减轻长视频处理负荷的同时保留了关键信息，使现有框架能够支持长达一小时的视频，并通过额外的上下文标记提升了模型性能上限。实验证明，该方法在大多数视频或图像基准测试中均超越了以往的方法。",
    "image": "",
    "code": "https://github.com/JIA-Lab-research/LLaMA-VID"
  },
  {
    "title": "LLMGA: Multimodal Large Language Model based Generation Assistant",
    "venue": "arXiv",
    "date": "2023",
    "link": "https://arxiv.org/pdf/2311.16500.pdf",
    "source": "auto-script",
    "tags": [
      "Instruction Tuning"
    ],
    "authors": "Bin Xia, Shiyin Wang, Yingfan Tao, Yitong Wang, Jiaya Jia",
    "description": "本文介绍了 LLMGA，这是一款基于多模态大语言模型（MLLM）的生成助手，旨在利用大语言模型在推理、理解和响应方面的深厚知识储备来辅助用户进行图像生成与编辑。不同于现有模型生成固定尺寸嵌入来控制 Stable Diffusion（SD）的方法，LLMGA 能够提供详细的语言生成提示词，从而实现对 SD 的精确控制，这不仅增强了模型对上下文的理解，还减少了提示词中的噪点，使生成的图像内容更丰富、更准确，并提高了网络的可解释性。为此，研究团队构建了一个涵盖提示词优化、相似图像生成、图像内外修补及基于指令编辑的综合数据集，并提出了两阶段训练方案：第一阶段训练 MLLM 掌握图像生成与编辑属性以生成详细提示词；第二阶段优化 SD 以对齐 MLLM 的生成提示词。此外，针对图像修补任务，该研究还提出了一种基于参考的恢复网络，以缓解生成区域与保留区域之间的纹理、亮度和对比度差异。实验结果表明，LLMGA 具备出色的生成与编辑能力，能够以交互方式实现更灵活、更广泛的应用。",
    "image": "",
    "code": "https://github.com/JIA-Lab-research/LLMGA"
  },
  {
    "title": "ChartLlama: A Multimodal LLM for Chart Understanding and Generation",
    "venue": "arXiv",
    "date": "2023",
    "link": "https://arxiv.org/pdf/2311.16483.pdf",
    "source": "auto-script",
    "tags": [
      "Instruction Tuning"
    ],
    "authors": "Yucheng Han, Chi Zhang, Xin Chen, Xu Yang, Zhibin Wang, et al.",
    "description": "本文介绍了 ChartLlama，这是一个旨在解决多模态大语言模型在图表解析领域理解能力不足问题的模型。针对现有研究缺乏相关多模态指令微调数据集的现状，研究团队利用 GPT-4 开发了一套多步数据生成流程，分别负责生成表格数据、创建图表以及设计指令微调数据，从而能够以较低的资源消耗持续、高效地生成包含更多图表类型和任务的高质量数据集。通过在该数据集上进行训练，ChartLlama 在 ChartQA、Chart-to-text 和 Chart-extraction 等评估基准上均超越了以往所有方法。此外，在包含新图表和新任务类型的特制数据集中，ChartLlama 较基线模型也有显著提升，其实验结果充分验证了该数据生成方法在增强图表理解方面的价值与潜力。",
    "image": "",
    "code": "https://tingxueronghua.github.io/ChartLlama/"
  },
  {
    "title": "ShareGPT4V: Improving Large Multi-Modal Models with Better Captions",
    "venue": "arXiv",
    "date": "2023",
    "link": "https://arxiv.org/pdf/2311.12793.pdf",
    "source": "auto-script",
    "tags": [
      "Instruction Tuning"
    ],
    "authors": "Lin Chen, Jinsong Li, Xiaoyi Dong, Pan Zhang, Conghui He, et al.",
    "description": "本文介绍了 ShareGPT4V 数据集，这是一个包含 120 万条高度描述性字幕的大规模资源，旨在解决大型多模态模型（LMM）中高质量图文数据稀缺导致的模态对齐瓶颈。该数据集在多样性和信息量上超越了现有数据集，涵盖了世界知识、物体属性、空间关系及审美评价等内容。ShareGPT4V 最初源自利用 GPT4-Vision 收集的 10 万条高质量字幕，随后通过在该子集上训练的优秀描述模型扩展至 120 万条。实验证明，在监督微调（SFT）阶段，用该数据集的子集替换现有 SFT 数据集中的同等数量详细字幕，能显著提升 LLaVA-7B、LLaVA-1.5-13B 和 Qwen-VL-Chat-7B 等模型在 MME 和 MMBench 基准测试中的表现。此外，研究团队将 ShareGPT4V 数据进一步整合到预训练和 SFT 阶段，构建了基于简单架构的 ShareGPT4V-7B 模型，该模型在大多数多模态基准测试中均展现出了卓越的性能。",
    "image": "",
    "code": "https://sharegpt4v.github.io/"
  },
  {
    "title": "LION : Empowering Multimodal Large Language Model with Dual-Level Visual Knowledge",
    "venue": "arXiv",
    "date": "2023",
    "link": "https://arxiv.org/pdf/2311.11860.pdf",
    "source": "auto-script",
    "tags": [
      "Instruction Tuning"
    ],
    "authors": "Gongwei Chen, Leyang Shen, Rui Shao, Xiang Deng, Liqiang Nie",
    "description": "本文介绍了 LION，这是一种双级视觉知识增强的多模态大语言模型，旨在解决现有模型因仅采用粗粒度对齐的预训练视觉编码器而导致的视觉知识提取与推理能力不足的问题。该模型通过两个层面注入视觉知识：首先，通过设计视觉聚合器并结合区域级视觉语言（VL）任务，渐进式地引入细粒度的空间感知视觉知识；同时，为了缓解图像级与区域级任务之间的冲突，研究者设计了带有适配器混合（mixture-of-adapters）的专用分阶段指令微调策略，实现了两类任务的相互促进。其次，该模型利用多样化的图像标签为大语言模型提供高层的语义视觉证据，并提出一种软提示方法，通过在定制指令中嵌入可学习标记来减轻预测标签不准确带来的影响。实验结果显示，LION 在多个基准测试中表现卓越，例如在 VSR、TextCaps 和 RefCOCOg 等任务上均显著优于 InstructBLIP 和 Kosmos-2 等模型。",
    "image": "",
    "code": "https://github.com/JiuTian-VL/JiuTian-LION"
  },
  {
    "title": "An Embodied Generalist Agent in 3D World",
    "venue": "arXiv",
    "date": "2023",
    "link": "https://arxiv.org/pdf/2311.12871.pdf",
    "source": "auto-script",
    "tags": [
      "Instruction Tuning"
    ],
    "authors": "Jiangyong Huang, Silong Yong, Xiaojian Ma, Xiongkun Linghu, Puhao Li, et al.",
    "description": "本文介绍了 LEO，这是一款具身多模态通用智能体，旨在解决现有大语言模型在 3D 视觉输入处理以及 3D 空间内任务（如 3D 定位、具身推理与动作执行）能力有限的问题。LEO 采用统一的任务接口、模型架构和目标函数，并经历了两个训练阶段：首先是 3D 视觉-语言（VL）对齐，随后是 3D 视觉-语言-动作（VLA）指令微调。为了支持训练，研究团队收集了包含多样化物体级和场景级任务的大规模数据集，并设计了由大语言模型辅助的流程来生成高质量 3D 视觉语言数据。广泛的实验证明，LEO 在 3D 描述、问答、具身推理、导航和操作等一系列任务中展现出卓越的能力，其消融实验和规模化分析也为未来开发具身通用智能体提供了宝贵见解。",
    "image": "",
    "code": "https://embodied-generalist.github.io/"
  },
  {
    "title": "Video-LLaVA: Learning United Visual Representation by Alignment Before Projection",
    "venue": "arXiv",
    "date": "2023",
    "link": "https://arxiv.org/pdf/2311.10122.pdf",
    "source": "auto-script",
    "tags": [
      "Instruction Tuning"
    ],
    "authors": "Bin Lin, Yang Ye, Bin Zhu, Jiaxi Cui, Munan Ning, Peng Jin, Li Yuan",
    "description": "本文介绍了 Video-LLaVA，这是一个旨在通过将图像和视频表示统一到语言特征空间，从而推动大语言模型向统一的大型视觉语言模型发展的基准模型。针对现有方法中图像与视频特征空间分离、在投影前缺乏统一标记化而导致多模态交互学习困难的问题，Video-LLaVA 采用了统一的视觉表示，并利用图像和视频的混合数据集进行学习，实现了两者的相互增强。实验结果显示，Video-LLaVA 在 9 项图像基准测试中表现卓越，并在 MSRVTT、MSVD、TGIF 和 ActivityNet 等视频任务上分别以 5.8%、9.9%、18.6% 和 10.1% 的优势大幅超越了 Video-ChatGPT。广泛的实验进一步证明，这种统一的视觉表示使模型在处理图像和视频时能够互为裨益，其性能甚至超过了专门针对单一模态设计的模型。",
    "image": "",
    "code": "https://github.com/PKU-YuanGroup/Video-LLaVA"
  },
  {
    "title": "Chat-UniVi: Unified Visual Representation Empowers Large Language Models with Image and Video Understanding",
    "venue": "CVPR",
    "date": "2023",
    "link": "https://arxiv.org/pdf/2311.08046",
    "source": "auto-script",
    "tags": [
      "Instruction Tuning"
    ],
    "authors": "Peng Jin, Ryuichi Takanobu, Wancai Zhang, Xiaochun Cao, Li Yuan",
    "description": "本文介绍了 Chat-UniVi，这是一个能够通过统一视觉表示来理解并进行图像和视频对话的统一视觉语言模型。针对现有方法在处理有限视觉标记下的图像和视频理解时面临的挑战，Chat-UniVi 采用了一组动态视觉标记来统一表示图像和视频，使其能以有限的标记同时捕捉图像所需的空间细节和视频所需的完整时序关系。此外，该模型利用多尺度表示，使其能够兼顾高层语义概念和底层视觉细节。Chat-UniVi 在包含图像和视频的混合数据集上进行训练，无需任何修改即可直接应用于两种媒介的任务。广泛的实验结果表明，Chat-UniVi 的表现始终优于现有的专门针对单一图像或视频模态设计的模型。",
    "image": "",
    "code": "https://github.com/PKU-YuanGroup/Chat-UniVi"
  },
  {
    "title": "To See is to Believe: Prompting GPT-4V for Better Visual Instruction Tuning",
    "venue": "arXiv",
    "date": "2023",
    "link": "https://arxiv.org/pdf/2311.07574.pdf",
    "source": "auto-script",
    "tags": [
      "Instruction Tuning"
    ],
    "authors": "Junke Wang, Lingchen Meng, Zejia Weng, Bo He, Zuxuan Wu, Yu-Gang Jiang",
    "description": "本文介绍了 LVIS-Instruct4V，这是一个包含 22 万条视觉对齐且具备上下文感知能力的精细化视觉指令数据集。针对现有视觉指令微调方法中，利用粗粒度图像注释生成的文本描述往往缺乏细节且易与实际视觉内容冲突的问题，该研究通过利用强大的 GPT-4V 模型对 LVIS 数据集中的图像进行提示，生成了高质量的指令数据。实验验证和案例研究表明，这些高质量的视觉教学数据能显著提升当前先进多模态大模型 LLaVA-1.5 在多项基准测试中的表现。值得注意的是，仅通过将原有的 LLaVA-Instruct 替换为 LVIS-Instruct4V，模型在极具挑战性的 LMM 基准测试中便取得了优于 LLaVA 的结果，例如在 LLaVA-Bench 上由 70.7 提升至 76.7，在 MM-Vet 上由 35.4 提升至 40.2。",
    "image": "",
    "code": "https://github.com/X2FD/LVIS-INSTRUCT4V"
  },
  {
    "title": "SPHINX: The Joint Mixing of Weights, Tasks, and Visual Embeddings for Multi-modal Large Language Models",
    "venue": "arXiv",
    "date": "2023",
    "link": "https://arxiv.org/pdf/2311.07575.pdf",
    "source": "auto-script",
    "tags": [
      "Instruction Tuning"
    ],
    "authors": "Ziyi Lin, Chris Liu, Renrui Zhang, Peng Gao, Longtian Qiu, et al.",
    "description": "本文介绍了 SPHINX，这是一个通过模型权重、微调任务和视觉嵌入三者混合构建的多功能多模态大语言模型（MLLM）。首先，为了实现更强的视觉-语言对齐，研究者在预训练期间解冻了大语言模型（LLM），并引入了真实数据与合成数据训练模型之间的权重混合策略，使模型能有效整合多样化的语义并具备良好的鲁棒性。其次，为了实现多用途能力，研究者混合了多种任务进行联合视觉指令微调，并设计了特定任务指令以避免冲突，涵盖了区域级理解、描述定位、文档布局检测和人体姿态估计等挑战性任务，促进了不同场景下的相互增强。此外，该模型从多种网络架构、预训练范式和信息粒度中提取全面的视觉嵌入，为语言模型提供了更稳健的图像表示。在此基础上，SPHINX 还提出了一种高效策略，通过混合不同比例和高分辨率子图像来捕捉细粒度外观，从而在现有的评估基准上实现了卓越的视觉解析和推理性能。",
    "image": "",
    "code": "https://github.com/Alpha-VLLM/LLaMA2-Accessory"
  },
  {
    "title": "Monkey: Image Resolution and Text Label Are Important Things for Large Multi-modal Models",
    "venue": "CVPR",
    "date": "2023",
    "link": "https://arxiv.org/pdf/2311.06607.pdf",
    "source": "auto-script",
    "tags": [
      "Instruction Tuning"
    ],
    "authors": "Zhang Li, Biao Yang, Qiang Liu, Zhiyin Ma, Shuo Zhang, et al.",
    "description": "本文介绍了 Monkey，这是一款旨在增强大型多模态模型（LMM）处理高分辨率输入和详细场景理解能力的新型模型。针对现有模型在复杂视觉信息捕捉方面的局限性，Monkey 首先通过将输入图像分割成与预训练视觉编码器匹配的均匀分块（如 448x448），并为每个分块配备独立的适配器，使其能够处理高达 1344x896 像素的分辨率，从而捕捉更细致的视觉特征。其次，它采用多层级描述生成方法，丰富了场景与物体关联的语境信息。这种双重策略确保了模型能更有效地从生成数据中学习：高分辨率提供了更详尽的视觉输入，进而增强了综合描述的有效性。广泛的消融实验验证了该设计的有效性，且在 18 个数据集上的实验进一步证明，Monkey 在图像描述及多种视觉问答任务中超越了现有的多模态大模型。特别是在针对密集文本问答的定性测试中，Monkey 展现出了足以媲美 GPT-4V 的优异表现。",
    "image": "",
    "code": "https://github.com/Yuliang-Liu/Monkey"
  },
  {
    "title": "LLaVA-Plus: Learning to Use Tools for Creating Multimodal Agents",
    "venue": "arXiv",
    "date": "2023",
    "link": "https://arxiv.org/pdf/2311.05437.pdf",
    "source": "auto-script",
    "tags": [
      "Instruction Tuning"
    ],
    "authors": "Shilong Liu, Hao Cheng, Haotian Liu, Hao Zhang, Feng Li, et al.",
    "description": "LLaVA-Plus 是一款通用多模态助手，通过扩展大型多模态模型的能力，构建了一个由预训练视觉及视觉语言模型组成的技能库，并能根据用户输入激活相关工具以完成现实任务。该模型在涵盖视觉理解、生成、外部知识检索及任务组合的多模态指令遵循数据上进行了训练，从而获得了使用工具的能力。实验结果表明，LLaVA-Plus 在现有能力上优于 LLaVA，并展现出了一系列新功能。其独特之处在于图像查询在整个人机交互过程中被直接定位并积极参与，这显著提升了工具的使用性能并赋能了全新的应用场景。",
    "image": "",
    "code": "https://llava-vl.github.io/llava-plus/"
  },
  {
    "title": "NExT-Chat: An LMM for Chat, Detection and Segmentation",
    "venue": "arXiv",
    "date": "2023",
    "link": "https://arxiv.org/pdf/2311.04498.pdf",
    "source": "auto-script",
    "tags": [
      "Instruction Tuning"
    ],
    "authors": "Ao Zhang, Yuan Yao, Wei Ji, Zhiyuan Liu, Tat-Seng Chua",
    "description": "本文介绍了 NExT-Chat，这是一种用于目标定位建模的新型范式。针对以往大型多模态模型（LMM）通过将目标边界框坐标表示为文本序列（pix2seq）来实现区域级理解的局限性，研究团队提出了 pix2emb 方法，即要求模型输出定位嵌入（location embeddings），并利用不同的解码器对其进行解码。这一范式使模型能够在多模态对话中使用边界框和掩码等多种定位格式，从而具备了处理视觉定位、区域描述和定位推理等多种任务的能力。广泛的实验证明了 NExT-Chat 在多项任务中的有效性，例如在 POPE-Random 基准测试中达到 87.7（优于 Shikra 的 86.9），在指代对象分割任务中达到 68.9（优于 LISA 的 67.9），以及在区域描述任务中达到 79.6（显著优于 Kosmos-2 的 62.3）。",
    "image": "",
    "code": "https://github.com/NExT-ChatV/NExT-Chat"
  },
  {
    "title": "mPLUG-Owl2: Revolutionizing Multi-modal Large Language Model with Modality Collaboration",
    "venue": "arXiv",
    "date": "2023",
    "link": "https://arxiv.org/pdf/2311.04257.pdf",
    "source": "auto-script",
    "tags": [
      "Instruction Tuning"
    ],
    "authors": "Qinghao Ye, Haiyang Xu, Jiabo Ye, Ming Yan, Anwen Hu, et al.",
    "description": "本文介绍了 mPLUG-Owl2，这是一个通用的多模态大语言模型，旨在通过模态协作有效提升纯文本和多模态任务的性能。该模型采用模块化网络设计，将语言解码器作为管理不同模态的通用接口。具体而言，mPLUG-Owl2 引入了共享功能模块以促进模态间的协作，并加入了模态自适应模块以保留各模态的特有特征。广泛的实验表明，mPLUG-Owl2 能够泛化至多种纯文本及多模态任务，并凭借单一通用模型实现行业领先的性能。值得注意的是，mPLUG-Owl2 是首个在纯文本和多模态场景中均展现出模态协作现象的模型，为未来多模态基础模型的发展开辟了先驱路径。",
    "image": "",
    "code": "https://github.com/X-PLUG/mPLUG-Owl/tree/main/mPLUG-Owl2"
  },
  {
    "title": "OtterHD: A High-Resolution Multi-modality Model",
    "venue": "arXiv",
    "date": "2023",
    "link": "https://arxiv.org/pdf/2311.04219.pdf",
    "source": "auto-script",
    "tags": [
      "Instruction Tuning"
    ],
    "authors": "Bo Li, Peiyuan Zhang, Jingkang Yang, Yuanhan Zhang, Fanyi Pu, Ziwei Liu",
    "description": "本文介绍了 OtterHD-8B，这是一款基于 Fuyu-8B 演进的创新多模态模型，专门设计用于高精度地解析高分辨率视觉输入。与受限于固定尺寸视觉编码器的传统模型不同，OtterHD-8B 能够处理灵活的输入维度，从而确保其在各种推理需求中的通用性。伴随该模型，研究团队还推出了 MagnifierBench 评估框架，旨在考察模型辨别微小物体细节及其空间关系的能力。对比分析显示，虽然目前领先的模型在这一基准测试中表现不佳，但 OtterHD-8B 在直接处理高分辨率输入时，其表现显著优于同类模型。研究结果揭示了不同模型在视觉信息处理上的结构性差异，以及视觉编码器预训练分辨率差异对模型效能的影响。该研究强调了大型多模态模型中灵活性和高分辨率输入能力的关键作用，并展示了 Fuyu 架构在处理复杂视觉数据方面的潜力。",
    "image": "",
    "code": "https://github.com/EvolvingLMMs-Lab/Otter"
  },
  {
    "title": "CoVLM: Composing Visual Entities and Relationships in Large Language Models Via Communicative Decoding",
    "venue": "arXiv",
    "date": "2023",
    "link": "https://arxiv.org/pdf/2311.03354.pdf",
    "source": "auto-script",
    "tags": [
      "Instruction Tuning"
    ],
    "authors": "Junyan Li, Delin Chen, Yining Hong, Zhenfang Chen, Peihao Chen, et al.",
    "description": "本文介绍了 CoVLM，旨在解决现有视觉语言基础模型（VLM）由于“词袋”行为导致的组合推理能力不足，以及无法准确表示视觉实体及其相互关系的问题。研究者提出了一套全新的通信标记（communication tokens），使大语言模型（LLM）能够与视觉检测系统进行动态双向通信：LLM 在生成视觉实体或关系后产生通信标记，通知检测网络提取相关的感兴趣区域（ROIs），随后这些区域信息被反馈给 LLM 以生成更精准的后续文本。通过这种视觉到语言、语言到视觉的迭代解码过程，CoVLM 成功弥合了视觉感知与 LLM 之间的鸿沟，在组合推理基准测试中（如 HICO-DET、Cola 和 ARO）显著优于以往模型，并在指代对象理解和视觉问答等传统视觉语言任务中达到了行业领先水平。",
    "image": "",
    "code": ""
  },
  {
    "title": "GLaMM: Pixel Grounding Large Multimodal Model",
    "venue": "CVPR",
    "date": "2023",
    "link": "https://arxiv.org/pdf/2311.03356.pdf",
    "source": "auto-script",
    "tags": [
      "Instruction Tuning"
    ],
    "authors": "Hanoona Rasheed, Muhammad Maaz, Sahal Shaji Mullappilly, Abdelrahman Shaker, Salman Khan, et al.",
    "description": "本文介绍了 GLaMM，这是首个能够生成与目标分割掩码无缝衔接的自然语言回复的大型多模态模型。针对现有区域级多模态模型仅能引用单一类别、依赖用户指定区域或缺乏细粒度像素级对齐的局限性，GLaMM 不仅能对对话中出现的物体进行定位，还支持文本和可选视觉提示（感兴趣区域）作为输入，使用户能在不同粒度下进行交互。由于该领域缺乏标准基准，研究团队提出了“视觉定位对话生成（GCG）”任务及其评估协议，并利用自动化标注流水线构建了大规模的 GranD 数据集，其中包含 8.1 亿个带有分割掩码的区域及 750 万个独特概念。实验证明，GLaMM 除了在 GCG 任务上表现出色外，在指代对象分割、图像及区域级描述以及视觉语言对话等多个下游任务中也同样有效。",
    "image": "",
    "code": "https://github.com/mbzuai-oryx/groundingLMM"
  },
  {
    "title": "What Makes for Good Visual Instructions? Synthesizing Complex Visual Reasoning Instructions for Visual Instruction Tuning",
    "venue": "arXiv",
    "date": "2023",
    "link": "https://arxiv.org/pdf/2311.01487.pdf",
    "source": "auto-script",
    "tags": [
      "Instruction Tuning"
    ],
    "authors": "Yifan Du, Hangyu Guo, Kun Zhou, Wayne Xin Zhao, Jinpeng Wang, et al.",
    "description": "本文通过全面实证研究探讨了“什么是优质视觉指令”这一核心问题，发现侧重于复杂视觉推理任务的指令在提升多模态大语言模型（MLLM）零样本泛化能力方面尤为有效。基于这一发现，研究者提出了一种自动创建高质量复杂视觉推理指令的系统性方法，即“合成-复杂化-重构”范式，通过多个阶段逐步提升指令复杂性并确保质量。利用该方法，研究团队构建了包含 3.2 万个样本的 ComVint 数据集，并在其上对四种 MLLM 进行了微调。实验结果一致表明，该数据集显著增强了所有对比模型的性能，例如使 LLaVA 模型在 MME-Perception 和 MME-Cognition 指标上分别提升了 27.86% 和 27.60%。",
    "image": "",
    "code": "https://github.com/RUCAIBox/ComVint"
  },
  {
    "title": "MiniGPT-v2: large language model as a unified interface for vision-language multi-task learning",
    "venue": "arXiv",
    "date": "2023",
    "link": "https://arxiv.org/pdf/2310.09478.pdf",
    "source": "auto-script",
    "tags": [
      "Instruction Tuning"
    ],
    "authors": "Jun Chen, Deyao Zhu, Xiaoqian Shen, Xiang Li, Zechun Liu, et al.",
    "description": "本文介绍了 MiniGPT-v2，这是一个旨在为图像描述、视觉问答及视觉定位等多种视觉语言任务提供统一接口的模型。为了解决在单一模型中利用简单多模态指令有效处理多样化任务的难题，MiniGPT-v2 在训练过程中为不同任务引入了独特的标识符。这些标识符使模型能够轻松区分各类任务指令，并提高了各任务的学习效率。经过三个阶段的训练，实验结果表明，与现有的其他视觉语言通用模型相比，MiniGPT-v2 在多项视觉问答和视觉定位基准测试中均展现出了强大的性能。",
    "image": "",
    "code": "https://minigpt-v2.github.io/"
  },
  {
    "title": "SALMONN: Towards Generic Hearing Abilities for Large Language Models",
    "venue": "ICLR",
    "date": "2023",
    "link": "https://arxiv.org/pdf/2310.13289",
    "source": "auto-script",
    "tags": [
      "Instruction Tuning"
    ],
    "authors": "Changli Tang, Wenyi Yu, Guangzhi Sun, Xianzhao Chen, Tian Tan, et al.",
    "description": "本文介绍了 SALMONN，这是一个通过将预训练的文本大语言模型（LLM）与语音及音频编码器整合而成的多模态开放神经网络。该模型旨在使人工智能智能体具备感知和理解包括语音、音频事件和音乐在内的通用听觉信息的能力。SALMONN 能够直接处理通用音频输入，在自动语音识别、语音翻译、基于听觉信息的问答、情感识别、说话人验证以及音乐和音频描述等训练任务中表现出色。此外，该模型还展现出一系列在训练中未曾出现的涌现能力，如向未训练语言的语音翻译、语音插槽填充、基于音频的故事创作以及语音音频协同推理等。研究还探讨了跨模态涌现现象，并提出了一种新型少样本激活微调方法来开启这些能力。作为同类模型中的首创，SALMONN 被认为是迈向具备通用听觉能力人工智能的重要一步。",
    "image": "",
    "code": "https://github.com/bytedance/SALMONN"
  },
  {
    "title": "Ferret: Refer and Ground Anything Anywhere at Any Granularity",
    "venue": "arXiv",
    "date": "2023",
    "link": "https://arxiv.org/pdf/2310.07704.pdf",
    "source": "auto-script",
    "tags": [
      "Instruction Tuning"
    ],
    "authors": "Haoxuan You, Haotian Zhang, Zhe Gan, Xianzhi Du, Bowen Zhang, et al.",
    "description": "本文介绍了 Ferret，这是一款新型多模态大语言模型（MLLM），能够理解图像中任何形状或粒度的空间指代，并能准确对齐开放词汇的描述。为了在 LLM 范式中统一指代（referring）与定位（grounding），Ferret 采用了一种创新的混合区域表示方法，将离散坐标与连续特征相结合来共同表示图像区域。为了提取多变区域的连续特征，研究者提出了一种空间感知视觉采样器，能够处理不同形状的稀疏性，从而使 Ferret 可以接受点、边界框和任意形状等多样化的区域输入。此外，研究团队构建了包含 110 万个样本的大规模指令微调数据集 GRIT，其中包含丰富的层级空间知识及 9.5 万条困难负样本以增强模型鲁棒性。实验结果表明，Ferret 不仅在经典指代和定位任务中性能卓越，在基于区域和强调定位的多模态对话中也显著优于现有的 MLLM，同时在图像细节描述能力及缓解物体幻觉方面均有显著提升。",
    "image": "",
    "code": "https://github.com/apple/ml-ferret"
  },
  {
    "title": "CogVLM: Visual Expert For Large Language Models",
    "venue": "arXiv",
    "date": "2023",
    "link": "https://arxiv.org/pdf/2311.03079.pdf",
    "source": "auto-script",
    "tags": [
      "Instruction Tuning"
    ],
    "authors": "Weihan Wang, Qingsong Lv, Wenmeng Yu, Wenyi Hong, Ji Qi, et al.",
    "description": "本文介绍了 CogVLM，这是一个强大的开源视觉语言基础模型。不同于将图像特征映射到语言模型输入空间的流行浅层对齐方法，CogVLM 通过在注意力层和前馈网络（FFN）层中引入可训练的“视觉专家”模块，弥补了冻结的预训练语言模型与图像编码器之间的差距。这种设计实现了视觉和语言特征的深度融合，且不会损害模型在纯自然语言处理（NLP）任务上的性能。实验结果显示，CogVLM-17B 在 NoCaps、RefCOCO、GQA 和 ScienceQA 等 10 个经典跨模态基准测试中达到了行业领先水平，并在 VQAv2 和 COCO captioning 等多项任务中排名第二，其表现足以媲美或超越参数量更大的 PaLI-X 55B 模型。",
    "image": "",
    "code": "https://github.com/zai-org/CogVLM"
  },
  {
    "title": "Improved Baselines with Visual Instruction Tuning",
    "venue": "arXiv",
    "date": "2023",
    "link": "https://arxiv.org/pdf/2310.03744.pdf",
    "source": "auto-script",
    "tags": [
      "Instruction Tuning"
    ],
    "authors": "Haotian Liu, Chunyuan Li, Yuheng Li, Yong Jae Lee",
    "description": "本文介绍了 LLaVA-1.5，证明了 LLaVA 架构中简单的全连接视觉-语言跨模态连接器具有极强的性能和数据效率。通过对 LLaVA 进行简单的改进——包括采用 CLIP-ViT-L-336px 视觉编码器、使用 MLP 投影层，以及加入面向学术任务的视觉问答（VQA）数据并配合简洁的回复格式提示，研究团队建立了一个更强大的基线模型，在 11 个基准测试中达到了最先进水平。最终的 13B 版本模型仅使用了 120 万条公开数据，在单个 8 卡 A100 节点上约一天内即可完成全部训练。",
    "image": "",
    "code": "https://llava-vl.github.io/"
  },
  {
    "title": "LanguageBind: Extending Video-Language Pretraining to N-modality by Language-based Semantic Alignment",
    "venue": "ICLR",
    "date": "2023",
    "link": "https://arxiv.org/pdf/2310.01852.pdf",
    "source": "auto-script",
    "tags": [
      "Instruction Tuning"
    ],
    "authors": "Bin Zhu, Bin Lin, Munan Ning, Yang Yan, Jiaxi Cui, et al.",
    "description": "本文介绍了 LanguageBind，这是一种以语言为核心连接多种模态的预训练框架。针对现有视觉-语言（VL）预训练框架难以扩展至三门及以上多模态（N>=3）的问题，研究者提出利用语义丰富且研究成熟的语言模态作为不同模态间的纽带。具体而言，该方法冻结了通过 VL 预训练获得的语言编码器，并利用对比学习训练其他模态的编码器，从而将所有模态映射到共享特征空间，实现多模态语义对齐。为配合此框架，研究团队推出了包含视频、红外、深度、音频及对应文本描述的高质量数据集 VIDAL-10M，其视频均源自语义完整的短视频平台。实验结果显示，LanguageBind 在涵盖四种模态的 15 个基准测试中表现卓越，并有效证明了其在实现不同模态间间接对齐与互补方面的有效性。",
    "image": "",
    "code": "https://github.com/PKU-YuanGroup/LanguageBind"
  },
  {
    "title": "Reformulating Vision-Language Foundation Models and Datasets Towards Universal Multimodal Assistants",
    "venue": "arXiv",
    "date": "2023",
    "link": "https://arxiv.org/pdf/2310.00653.pdf",
    "source": "auto-script",
    "tags": [
      "Instruction Tuning"
    ],
    "authors": "Tianyu Yu, Jinyi Hu, Yuan Yao, Haoye Zhang, Yue Zhao, et al.",
    "description": "本文介绍了 Muffin，这是一种旨在提升多模态大语言模型（MLLM）性能的框架，并配套推出了 UniMM-Chat 数据集。针对现有模型架构通常依赖额外的外部桥接模块且需要专门的对齐预训练这一问题，研究者发现紧凑的预训练视觉语言模型本身即可作为“开箱即用”的桥梁，并以此提出了直接利用这类预训练模型提供视觉信号的 Muffin 框架。在数据集方面，针对现有方法简单混合不同任务数据集而忽略互补性的不足，研究者通过整合来自不同来源、描述同一图像的信息，将其转化为更具知识密集型的对话数据，构建了包含 110 万条高质量指令的 UniMM-Chat 数据集。实验结果表明，Muffin 框架在多项视觉语言任务中达到了行业领先水平，性能显著超越了 LLaVA 和 InstructBLIP 等先进模型。",
    "image": "",
    "code": "https://github.com/thunlp/muffin"
  },
  {
    "title": "AnyMAL: An Efficient and Scalable Any-Modality Augmented Language Model",
    "venue": "arXiv",
    "date": "2023",
    "link": "https://arxiv.org/pdf/2309.16058.pdf",
    "source": "auto-script",
    "tags": [
      "Instruction Tuning"
    ],
    "authors": "Seungwhan Moon, Andrea Madotto, Zhaojiang Lin, et al.",
    "description": "本文介绍了 Any-Modality Augmented Language Model (AnyMAL)，这是一种统一的多模态增强语言模型，能够对包括文本、图像、视频、音频和 IMU 运动传感器在内的多种输入模态信号进行推理，并生成文本回复。AnyMAL 继承了 LLaMA-2 (70B) 等最先进大语言模型强大的文本推理能力，并通过预训练的对齐模块将特定模态的信号转换为统一的文本空间。为了进一步增强模型的多模态处理能力，研究团队使用了人工收集的多模态指令集进行微调，该指令集涵盖了除简单问答之外的各种主题和任务。通过结合人工评估和自动评估的综合实证分析，AnyMAL 在多种多模态任务中展现出了最先进的性能水平。",
    "image": "",
    "code": ""
  },
  {
    "title": "InternLM-XComposer: A Vision-Language Large Model for Advanced Text-image Comprehension and Composition",
    "venue": "arXiv",
    "date": "2023",
    "link": "https://arxiv.org/pdf/2309.15112.pdf",
    "source": "auto-script",
    "tags": [
      "Instruction Tuning"
    ],
    "authors": "Pan Zhang, Xiaoyi Dong, Bin Wang, Yuhang Cao, Chao Xu, et al.",
    "description": "本文介绍了 InternLM-XComposer，这是一个具备先进图文理解与创作能力的视觉语言大模型。该模型具有三大核心特性：1）交错式图文创作：模型能够根据写作指令生成连贯且图文并茂的文章，通过智能识别文本中适合插入图片的位置并匹配视觉素材，提供沉浸式的阅读体验。2）丰富的多语言知识理解：通过在海量多模态多语言数据库上进行精心设计的策略训练，模型实现了对视觉内容的深度理解。3）卓越的性能表现：模型在 MME、MMBench、Seed-Bench 以及针对中国文化的 CCBench 等多个主流基准测试中均达到了行业领先水平。由于缺乏定量评估图文创作的标准，研究团队设计了一套结合人工与 GPT-4V 的评估流程，结果显示 InternLM-XComposer 在图文创作任务上相比 GPT-4V 和 GPT-3.5 等方案具有极强的竞争力。综上所述，该模型通过融合理解与创作能力，为视觉语言交互领域提供了新的研究思路。",
    "image": "",
    "code": "https://github.com/InternLM/InternLM-XComposer"
  },
  {
    "title": "DreamLLM: Synergistic Multimodal Comprehension and Creation",
    "venue": "ICLR",
    "date": "2023",
    "link": "https://arxiv.org/pdf/2309.11499.pdf",
    "source": "auto-script",
    "tags": [
      "Instruction Tuning"
    ],
    "authors": "Runpei Dong, Chunrui Han, Yuang Peng, Zekun Qi, Zheng Ge, et al.",
    "description": "本文介绍了 DreamLLM，这是一个首次实现了多模态理解与创作之间协同效应的学习框架。DreamLLM 遵循两个基本原则：首先，它通过在原始多模态空间直接采样，对语言和图像的后验分布进行生成式建模，从而规避了 CLIP 等外部特征提取器固有的局限性和信息损失，获得了更深入的多模态理解；其次，DreamLLM 能够生成包含文本、图像内容及非结构化布局的原始交错文档，有效学习了所有条件、边缘及联合多模态分布。因此，DreamLLM 成为首个能够生成自由形式交错内容的多模态大模型。综合实验表明，得益于增强的学习协同效应，DreamLLM 作为零样本多模态通用模型展现出了卓越的性能。",
    "image": "",
    "code": "https://dreamllm.github.io/"
  },
  {
    "title": "An Empirical Study of Scaling Instruction-Tuned Large Multimodal Models",
    "venue": "arXiv",
    "date": "2023",
    "link": "https://arxiv.org/pdf/2309.09958.pdf",
    "source": "auto-script",
    "tags": [
      "Instruction Tuning"
    ],
    "authors": "Yadong Lu, Chunyuan Li, Haotian Liu, Jianwei Yang, Jianfeng Gao, Yelong Shen",
    "description": "本文对开源大型多模态模型（LMM）的规模化进行了实证研究，将 LLaVA 模型扩展到了 33B 和 65B/70B 参数量级，并探讨了图像分辨率、数据混合以及 LoRA/QLoRA 等参数高效微调方法对模型性能的影响。研究发现，扩大模型规模能持续提升多模态处理能力和纯语言能力；同时，LoRA/QLoRA 微调的效果与全参数微调相当。研究还强调了提高图像分辨率以及混合多模态与纯语言数据对于提升模型表现的重要性，并指出视觉指令微调有时甚至能增强模型的纯语言能力。",
    "image": "",
    "code": "https://github.com/haotian-liu/LLaVA"
  },
  {
    "title": "TextBind: Multi-turn Interleaved Multimodal Instruction-following",
    "venue": "arXiv",
    "date": "2023",
    "link": "https://arxiv.org/pdf/2309.08637.pdf",
    "source": "auto-script",
    "tags": [
      "Instruction Tuning"
    ],
    "authors": "Huayang Li, Siheng Li, Deng Cai, Longyue Wang, Lemao Liu, et al.",
    "description": "本文介绍了 TextBind，这是一个几乎无需人工标注的框架，旨在赋予大语言模型处理多轮交错多模态指令的能力。针对高质量多模态指令数据难以获取的挑战，该方法仅利用“图像-描述”对，通过语言模型自动生成多轮多模态指令对话数据。为了支持交错的图文输入与输出，研究团队设计了以语言模型为核心的 MIM 架构，实现了图像编码器与解码器模型的无缝集成。该研究通过发布数据集、模型及演示，为多模态指令遵循领域的后续研究提供了有力支持。",
    "image": "",
    "code": ""
  },
  {
    "title": "NExT-GPT: Any-to-Any Multimodal LLM",
    "venue": "arXiv",
    "date": "2023",
    "link": "https://arxiv.org/pdf/2309.05519.pdf",
    "source": "auto-script",
    "tags": [
      "Instruction Tuning"
    ],
    "authors": "Shengqiong Wu, Hao Fei, Leigang Qu, Wei Ji, Tat-Seng Chua",
    "description": "本文介绍了 NExT-GPT，这是一个端到端的通用任意模态（any-to-any）多模态大语言模型系统。针对现有模型大多局限于输入端多模态理解而缺乏多模态内容输出能力的问题，NExT-GPT 通过将大语言模型与多模态适配器及不同的扩散解码器相结合，实现了文本、图像、视频和音频任意组合的输入感知与输出生成。该系统利用现有的预训练高性能编码器和解码器，仅需对约 1% 的特定投影层参数进行微调，在实现低成本训练的同时，也便于扩展更多潜在模态。此外，研究团队引入了模态切换指令微调（MosIT），并为此手动构建了高质量数据集，赋予了 NExT-GPT 复杂的跨模态语义理解与内容生成能力。这项研究展示了构建能够建模通用模态的 AI 智能体的可能性，为类人人工智能的研究奠定了基础。",
    "image": "",
    "code": "https://next-gpt.github.io/"
  },
  {
    "title": "Sight Beyond Text: Multi-Modal Training Enhances LLMs in Truthfulness and Ethics",
    "venue": "arXiv",
    "date": "2023",
    "link": "https://arxiv.org/pdf/2309.07120.pdf",
    "source": "auto-script",
    "tags": [
      "Instruction Tuning"
    ],
    "authors": "Haoqin Tu, Bingchen Zhao, Chen Wei, Cihang Xie",
    "description": "本文研究了多模态大语言模型（MLLM）在纯自然语言处理（NLP）领域中常被忽视的能力，揭示了视觉指令微调这一常用训练策略对模型真实性（Truthfulness）和伦理对齐（Ethical Alignment）的意外提升作用。初步实验结果表明，经过视觉指令微调的 LLaMA2 7B 模型在 TruthfulQA-mc 和 Ethics 基准测试中的表现，甚至超过了使用超过 100 万条人工标注进行微调的 LLaMA2-chat 7B 模型。进一步分析显示，这种对齐能力的提升归功于视觉-文本数据所具有的高质量指令特性。",
    "image": "",
    "code": "https://github.com/UCSC-VLAA/Sight-Beyond-Text"
  },
  {
    "title": "ImageBind-LLM: Multi-modality Instruction Tuning",
    "venue": "arXiv",
    "date": "2023",
    "link": "https://arxiv.org/pdf/2309.03905.pdf",
    "source": "auto-script",
    "tags": [
      "Instruction Tuning"
    ],
    "authors": "Jiaming Han, Renrui Zhang, Wenqi Shao, Peng Gao, Peng Xu, et al.",
    "description": "本文介绍了 ImageBind-LLM，这是一种通过 ImageBind 实现的大语言模型多模态指令微调方法。不同于以往主要关注图文指令微调的研究，ImageBind-LLM 仅通过图像-文本对齐训练，即可响应包括音频、3D 点云、视频及其嵌入空间运算在内的多种模态指令。在训练阶段，该方法采用可学习的绑定网络（bind network）来对齐 LLaMA 与 ImageBind 图像编码器的嵌入空间，并通过一种无注意力机制且零初始化的门控机制，将转换后的图像特征注入 LLaMA 的各层词标记中。得益于 ImageBind 的联合嵌入特性，这种简单的训练方式赋予了模型卓越的多模态指令遵循能力。在推理阶段，不同模态的输入经由 ImageBind 编码器处理后，通过提出的视觉缓存模型（visual cache model）进行跨模态嵌入增强；该缓存模型利用 ImageBind 提取的 300 万个图像特征进行非训练式检索，有效缓解了训练与推理间的模态差异。实验表明，ImageBind-LLM 能够高质量地响应多种模态的指令并生成语言。",
    "image": "",
    "code": "https://github.com/OpenGVLab/LLaMA-Adapter"
  },
  {
    "title": "Scaling Autoregressive Multi-Modal Models: Pretraining and Instruction Tuning",
    "venue": "arXiv",
    "date": "2023",
    "link": "https://arxiv.org/pdf/2309.02591.pdf",
    "source": "auto-script",
    "tags": [
      "Instruction Tuning"
    ],
    "authors": "Lili Yu, Bowen Shi, Ramakanth Pasunuru, Benjamin Muller, Olga Golovneva, et al.",
    "description": "本文介绍了 CM3Leon（发音同“Chameleon”），这是一个基于检索增强、基于标记且仅包含解码器的多模态语言模型，具备生成及填充文本和图像的能力。CM3Leon 沿用了 CM3 的多模态架构，并展示了规模扩展以及在多样化指令式数据上进行微调的显著优势。它是首个完全适配纯文本语言模型训练方案的多模态模型，其流程包括大规模检索增强预训练阶段以及随后的多任务有监督微调（SFT）阶段。作为一个通用模型，它同时支持文本到图像和图像到文本的生成，并引入了能够产生高质量输出的自包含对比解码方法。广泛的实验表明，该训练方案对多模态模型极为有效：在文本到图像生成方面，CM3Leon 以低于同类方法 5 倍的训练计算量达到了领先水平（MS-COCO 零样本 FID 为 4.88）。经过 SFT 后，CM3Leon 在语言引导的图像编辑、图像控制生成及分割等任务中展现出了前所未有的可控性。",
    "image": "",
    "code": ""
  },
  {
    "title": "PointLLM: Empowering Large Language Models to Understand Point Clouds",
    "venue": "arXiv",
    "date": "2023",
    "link": "https://arxiv.org/pdf/2308.16911.pdf",
    "source": "auto-script",
    "tags": [
      "Instruction Tuning"
    ],
    "authors": "Runsen Xu, Xiaolong Wang, Tai Wang, Yilun Chen, Jiangmiao Pang, Dahua Lin",
    "description": "本文介绍了 PointLLM，这是一项旨在填补大语言模型（LLM）在 3D 理解领域空白的初步尝试，使其能够理解点云数据并提供超越 2D 视觉数据的交互新途径。PointLLM 能够根据人类指令理解彩色物体点云，并生成符合语境的响应，展示了其对点云特征和常识的掌握。具体而言，该模型结合了点云编码器与强大的 LLM，有效地融合了几何、外观和语言信息。研究团队收集了一个包含 66 万条简单和 7 万条复杂“点云-文本”指令对的新型数据集，并采用两阶段训练策略：先进行潜空间对齐，随后对统一模型进行指令微调。为了严谨评估 PointLLM 的感知与泛化能力，研究者建立了生成式 3D 物体分类和 3D 物体描述两个基准测试，并利用人工评估、GPT-4/ChatGPT 评估以及传统指标进行衡量。实验结果显示，PointLLM 的表现优于现有的 2D 和 3D 基线模型，尤其在人工评估的物体描述任务中，其在超过 50% 的样本中超越了人类标注者的水平。",
    "image": "",
    "code": "https://github.com/InternRobotics/PointLLM"
  },
  {
    "title": "✨Sparkles: Unlocking Chats Across Multiple Images for Multimodal Instruction-Following Models",
    "venue": "arXiv",
    "date": "2023",
    "link": "https://arxiv.org/pdf/2308.16463.pdf",
    "source": "auto-script",
    "tags": [
      "Evaluation",
      "Instruction Tuning"
    ],
    "authors": "Yupan Huang, Zaiqiao Meng, Fangyu Liu, Yixuan Su, Nigel Collier, Yutong Lu",
    "description": "本文介绍了 SparklesChat 及其配套资源，旨在解决 MiniGPT-4 和 LLaVA 等现有模型在多图交互场景中难以保持对话连贯性的问题。针对相关专用数据集匮乏的现状，研究团队首先推出了 SparklesDialogue，这是首个专为词级交错的多图文本交互设计的机器生成对话数据集。此外，研究者构建了由 GPT 辅助的量化基准测试 SparklesEval，用于评估模型在多图及多轮对话中的交际能力。基于此数据集和评估体系，研究团队开发了能够进行多图开放式对话的多模态指令遵循模型 SparklesChat。实验结果表明，在 MiniGPT-4 和 LLaVA-v1.5 基础上使用 SparklesDialogue 进行训练，能显著增强模型对多图及多轮对话的理解能力，且不会损害其单图理解性能。定性评估进一步证明了 SparklesChat 在处理现实世界应用中的通用性。",
    "image": "",
    "code": "https://github.com/HYPJUDY/Sparkles"
  },
  {
    "title": "MLLM-DataEngine: An Iterative Refinement Approach for MLLM",
    "venue": "arXiv",
    "date": "2023",
    "link": "https://arxiv.org/pdf/2308.13566.pdf",
    "source": "auto-script",
    "tags": [
      "Instruction Tuning"
    ],
    "authors": "Zhiyuan Zhao, Linke Ouyang, Bin Wang, Siyuan Huang, Pan Zhang, et al.",
    "description": "本文提出了 MLLM-DataEngine，这是一个将数据生成、模型训练与评估相结合的新型闭环系统。针对当前多模态大语言模型（MLLM）中训练与评估相互独立、难以根据评估结果低成本提升模型能力的痛点，该系统在每次循环迭代中，首先根据评估结果分析模型弱点，随后生成相应的增量数据集进行针对性训练，从而迭代增强模型能力。相比以往脱离基准测试的数据收集方法，MLLM-DataEngine 生成的数据在针对性、质量和准确性方面表现更佳：在针对性方面，通过“自适应坏例采样”模块根据基准测试结果调整增量数据比例；在质量方面，利用 GPT-4 生成高质量数据；在准确性方面，提出“交互式提示优化”策略，通过人工与 GPT 的多轮交互优化提示词，显著提升生成数据的正确率。广泛的实验表明，MLLM-DataEngine 能在极少人工参与的情况下，以自动化且具针对性的方式提升模型性能，为构建未来的多模态大模型提供了一种通用解决方案。",
    "image": "",
    "code": "https://github.com/opendatalab/MLLM-DataEngine"
  },
  {
    "title": "Position-Enhanced Visual Instruction Tuning for Multimodal Large Language Models",
    "venue": "arXiv",
    "date": "2023",
    "link": "https://arxiv.org/pdf/2308.13437.pdf",
    "source": "auto-script",
    "tags": [
      "Instruction Tuning"
    ],
    "authors": "Chi Chen, Ruoyu Qin, Fuwen Luo, Xiaoyue Mi, Peng Li, et al.",
    "description": "本文介绍了位置增强视觉指令微调（PVIT），这是一种旨在提升多模态大语言模型（MLLM）细粒度理解能力的框架。针对现有视觉指令微调方法仅利用图像-语言对齐、缺乏更精细跨模态对齐的局限性，PVIT 通过整合一个额外的区域级视觉编码器扩展了模型功能，从而实现对图像内容的详细理解。此外，为了高效实现视觉模块与大语言模型之间的细粒度对齐，研究者设计了多种数据生成策略，构建了一个“图像-区域-语言”指令数据集。定量实验与定性分析结果均证明了该模型在图像理解任务中的优越性。",
    "image": "",
    "code": "https://github.com/PVIT-official/PVIT"
  },
  {
    "title": "Qwen-VL: A Frontier Large Vision-Language Model with Versatile Abilities",
    "venue": "arXiv",
    "date": "2023",
    "link": "https://arxiv.org/pdf/2308.12966.pdf",
    "source": "auto-script",
    "tags": [
      "Instruction Tuning"
    ],
    "authors": "Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, et al.",
    "description": "本文介绍了 Qwen-VL 系列，这是一组旨在感知和理解文本与图像的大型视觉语言模型（LVLM）。该系列以 Qwen-LM 为基础，通过精心设计的视觉接收器、输入输出接口、三阶段训练流水线以及多语言多模态清洗语料库，使模型具备了强大的视觉能力。除了传统的图像描述和问答外，Qwen-VL 还能通过对齐“图像-描述-检测框”元组来实现定位和文本阅读能力。实验结果显示，Qwen-VL 和 Qwen-VL-Chat 在同等模型规模下，于图像描述、视觉问答、视觉定位等一系列以视觉为中心的基准测试中，以及零样本和少样本等不同设置下均创下了通用模型的新纪录。此外，经过指令微调的 Qwen-VL-Chat 在真实对话基准测试中，相较于现有的视觉语言聊天机器人也展现出了优越性。",
    "image": "",
    "code": "https://github.com/QwenLM/Qwen-VL"
  },
  {
    "title": "Large Multilingual Models Pivot Zero-Shot Multimodal Learning across Languages",
    "venue": "ICLR",
    "date": "2023",
    "link": "https://arxiv.org/pdf/2308.12038.pdf",
    "source": "auto-script",
    "tags": [
      "Instruction Tuning"
    ],
    "authors": "Jinyi Hu, Yuan Yao, Chongyi Wang, Shan Wang, Yinxu Pan, et al.",
    "description": "本文介绍了 MPM，这是一种用于训练非英语多模态大模型的有效范式。针对非英语多模态数据稀缺（缺乏大规模、高质量图文对）的挑战，MPM 证明了多语言语言模型能够作为枢纽，实现跨语言的零样本多模态学习。具体而言，基于强大的多语言大语言模型，仅在英语图文数据上预训练的多模态模型能够以（类）零样本的方式很好地泛化到其他语言，其表现甚至优于在母语图文数据上训练的模型。研究团队以中文作为 MPM 的实践，构建了具备图文双向生成能力的 VisCPM 模型，并在中文开源模型中达到了领先水平。",
    "image": "",
    "code": "https://github.com/OpenBMB/VisCPM"
  },
  {
    "title": "StableLLaVA: Enhanced Visual Instruction Tuning with Synthesized Image-Dialogue Data",
    "venue": "arXiv",
    "date": "2023",
    "link": "https://arxiv.org/pdf/2308.10253.pdf",
    "source": "auto-script",
    "tags": [
      "Instruction Tuning"
    ],
    "authors": "Yanda Li, Chi Zhang, Gang Yu, Zhibin Wang, Bin Fu, et al.",
    "description": "本文介绍了一种旨在提升多模态大语言模型（LLM）性能的新型数据采集方法。针对现有视觉指令微调数据集因过度依赖基准测试标注而导致的领域偏差及生成能力受限等问题，研究者提出了一种同步合成图像与对话的方案。该方法结合了 ChatGPT 的文本生成能力与文本到图像生成模型，构建出一个内容多样且高度可控的数据集。相比传统方法，该方法不仅具备极高的扩展灵活性，还能根据需求任意扩大数据规模。实验结果表明，该方法显著增强了模型在十余项常见评估能力上的表现，并使模型在多个广受认可的多模态基准测试中达到了最先进水平。",
    "image": "",
    "code": "https://github.com/icoz69/StableLLAVA"
  },
  {
    "title": "BLIVA: A Simple Multimodal LLM for Better Handling of Text-rich Visual Questions",
    "venue": "arXiv",
    "date": "2023",
    "link": "https://arxiv.org/pdf/2308.09936.pdf",
    "source": "auto-script",
    "tags": [
      "Instruction Tuning"
    ],
    "authors": "Wenbo Hu, Yifan Xu, Yi Li, Weiyue Li, Zeyuan Chen, Zhuowen Tu",
    "description": "本文介绍了 BLIVA，这是一款在 InstructBLIP 基础上通过引入“视觉辅助”机制增强的多模态大语言模型（VLM）。针对现有视觉语言模型在处理富含文本的现实场景图像时，因受限于固定查询嵌入（Query Embeddings）数量而导致信息丢失的问题，BLIVA 结合了 InstructBLIP 的查询嵌入技术与 LLaVA 直接投影编码特征图块（Patch Embeddings）的方法。这种双重路径设计使模型能够捕捉到查询解码过程中可能遗漏的微观细节。实验结果表明，BLIVA 在处理富文本视觉问答（VQA）基准测试时表现显著提升（在 OCR-VQA 基准上提升高达 17.76%），在通用空间推理任务及 MME 综合测评中也分别实现了 7.9% 和 17.72% 的性能增长。为了验证其在工业界的广泛应用潜力，研究团队还推出了一个涵盖 11 个类别的 YouTube 缩略图问答数据集进行评估，结果证明 BLIVA 无论在图像是否包含文本的情况下均具备强大的解码能力。",
    "image": "",
    "code": "https://github.com/mlpc-ucsd/BLIVA"
  },
  {
    "title": "Fine-tuning Multimodal LLMs to Follow Zero-shot Demonstrative Instructions",
    "venue": "arXiv",
    "date": "2023",
    "link": "https://arxiv.org/pdf/2308.04152.pdf",
    "source": "auto-script",
    "tags": [
      "Instruction Tuning"
    ],
    "authors": "Juncheng Li, Kaihang Pan, Zhiqi Ge, Minghe Gao, Wei Ji, et al.",
    "description": "本文介绍了 VPG-C（视觉提示生成器补全模块），这是一种旨在解决多模态大语言模型（MLLM）在理解演示性指令（demonstrative instructions）时表现不足的通用轻量化模块。研究指出，现有的视觉提示生成器（VPG）通常在数百万个图像-描述对上进行训练，这使其倾向于仅关注足以生成简短描述的主要视觉内容，而忽略了对于完成复杂、交错任务至关重要的视觉细节。为克服这一缺陷，VPG-C 能够推断并补全缺失的视觉细节，以更好地理解演示性指令。此外，研究者提出了一种合成判别式训练策略来微调 VPG-C，从而消除了对有监督演示性指令数据的依赖。为了评估模型性能，研究团队构建了综合基准测试 DEMON。实验结果表明，经过合成训练的 VPG-C 在 DEMON 的所有任务中均展现出更强的零样本性能，并在 MME 和 OwlEval 基准测试中进一步证明了其优越性。",
    "image": "",
    "code": "https://github.com/DCDmllm/Cheetah"
  },
  {
    "title": "The All-Seeing Project: Towards Panoptic Visual Recognition and Understanding of the Open World",
    "venue": "ICLR",
    "date": "2023",
    "link": "https://arxiv.org/pdf/2308.01907.pdf",
    "source": "auto-script",
    "tags": [
      "Instruction Tuning"
    ],
    "authors": "Weiyun Wang, Min Shi, Qingyun Li, Wenhai Wang, Zhenhang Huang, et al.",
    "description": "本文介绍了 All-Seeing (AS) 项目，这是一个旨在识别和理解开放世界中万物的大规模数据集与模型体系。通过结合人类反馈和高效模型构建的可扩展数据引擎，研究团队创建了全新的 AS-1B 数据集，其中包含超过 10 亿个标注了语义标签、问答对及详细描述的图像区域。该数据集涵盖了现实世界中 350 万个常见及罕见概念，包含共计 1322 亿个用于描述这些概念及其属性的文本标记。基于此数据集，研究者开发了 All-Seeing 模型（ASM），这是一个用于全景视觉识别与理解的统一框架。该模型通过开放式语言提示和位置信息进行训练，在区域-文本检索、区域识别、图像描述和问答等多种视觉语言任务中展现出卓越的零样本泛化性能。该项目旨在为视觉语言通用人工智能的研究提供基础支持。",
    "image": "",
    "code": "https://github.com/OpenGVLab/all-seeing"
  },
  {
    "title": "LISA: Reasoning Segmentation via Large Language Model",
    "venue": "arXiv",
    "date": "2023",
    "link": "https://arxiv.org/pdf/2308.00692.pdf",
    "source": "auto-script",
    "tags": [
      "Instruction Tuning"
    ],
    "authors": "Xin Lai, Zhuotao Tian, Yukang Chen, Yanwei Li, Yuhui Yuan, et al.",
    "description": "本文提出了一项名为“推理分割”（reasoning segmentation）的新型图像分割任务，旨在根据复杂且含蓄的查询文本输出分割掩码，以解决现有感知系统无法主动推理用户意图的问题。为此，研究团队构建了一个包含 1000 多个“图像-指令-掩码”样本的基准数据集，其中融入了复杂的推理过程和世界知识。同时，本文介绍了 LISA（大语言指令分割助手），该模型继承了多模态大语言模型（LLM）的文本生成能力，并具备生成分割掩码的功能。通过扩展词表引入 <SEG> 标记并采用“嵌入即掩码”（embedding-as-mask）范式，LISA 成功解锁了分割能力，能够处理涉及复杂推理和世界知识的案例。实验表明，该模型在仅使用无推理数据集训练时便展现出强大的零样本能力，而通过仅 239 条推理分割数据的微调，性能可进一步提升。定量和定性实验一致证明，该方法有效赋予了多模态大语言模型全新的推理分割能力。",
    "image": "",
    "code": "https://github.com/JIA-Lab-research/LISA"
  },
  {
    "title": "MovieChat: From Dense Token to Sparse Memory for Long Video Understanding",
    "venue": "arXiv",
    "date": "2023",
    "link": "https://arxiv.org/pdf/2307.16449.pdf",
    "source": "auto-script",
    "tags": [
      "Instruction Tuning"
    ],
    "authors": "Enxin Song, Wenhao Chai, Guanhong Wang, Yucheng Zhang, Haoyang Zhou, et al.",
    "description": "本文介绍了 MovieChat，这是一个旨在克服现有视频理解系统在处理长视频时面临的计算复杂度高、内存开销大以及长程时序关联难等挑战的系统。该模型借鉴了 Atkinson-Shiffrin 记忆模型，利用 Transformer 的标记（token）作为记忆载体，并结合专门设计的记忆机制，实现了对超长视频的有效处理。MovieChat 在长视频理解任务中达到了领先性能。此外，研究团队还发布了包含 1000 个长视频及 1.4 万条人工标注的 MovieChat-1K 基准数据集，用以验证该方法的有效性。",
    "image": "",
    "code": "https://github.com/rese1f/MovieChat"
  },
  {
    "title": "3D-LLM: Injecting the 3D World into Large Language Models",
    "venue": "arXiv",
    "date": "2023",
    "link": "https://arxiv.org/pdf/2307.12981.pdf",
    "source": "auto-script",
    "tags": [
      "Instruction Tuning"
    ],
    "authors": "Yining Hong, Haoyu Zhen, Peihao Chen, Shuhong Zheng, Yilun Du, et al.",
    "description": "本文介绍了 3D-LLM 系列模型，旨在将 3D 物理世界信息注入大语言模型（LLM），使其能够理解空间关系、交互层级、物理特性及布局等丰富概念。3D-LLM 以 3D 点云及其特征作为输入，能够执行包括场景描述、密集描述、3D 问答、任务分解、3D 定位、3D 辅助对话及导航等在内的多种任务。研究者通过设计三种提示机制，收集了超过 30 万条涵盖上述任务的 3D 语言数据。在训练策略上，该模型首先利用 3D 特征提取器从渲染的多视角图像中获取特征，并以 2D 视觉语言模型（VLM）为骨干网络进行高效训练。通过引入 3D 定位机制，模型能够更好地捕捉空间信息。实验结果显示，在 ScanQA 任务中，3D-LLM 的 BLEU-1 分数比现有最先进模型高出 9%；在 3D 描述、任务分解和辅助对话等数据集上的表现也优于 2D 视觉语言模型。定性评估进一步证明，该模型具备处理超出传统模型能力范围的任务潜力。",
    "image": "",
    "code": "https://vis-www.cs.umass.edu/3dllm/"
  },
  {
    "title": "ChatSpot: Bootstrapping Multimodal LLMs via Precise Referring Instruction Tuning",
    "venue": "arXiv",
    "date": "2023",
    "link": "https://arxiv.org/pdf/2307.09474.pdf",
    "source": "auto-script",
    "tags": [
      "Instruction Tuning"
    ],
    "authors": "Liang Zhao, En Yu, Zheng Ge, Jinrong Yang, Haoran Wei, et al.",
    "description": "本文介绍了 ChatSpot，这是一个统一的端到端多模态大语言模型（MLLM），旨在解决现有模型仅支持语言指令交互而导致的准确性与效率受限问题。ChatSpot 引入了精确指代指令（precise referring instructions），支持用户通过点击、拖拽和画框等多种参考表示（如点和框）作为提示来指代特定区域，从而使模型能够关注感兴趣的区域并实现更细粒度的交互。此外，研究团队结合现有数据集并利用 GPT-4 生成技术，构建了一个多粒度视觉语言指令遵循数据集，并设计了一系列评估任务来检测区域识别与交互的有效性。实验结果表明，ChatSpot 在提供灵活、无缝的交互体验方面展现出良好的性能。",
    "image": "",
    "code": ""
  },
  {
    "title": "BuboGPT: Enabling Visual Grounding in Multi-Modal LLMs",
    "venue": "arXiv",
    "date": "2023",
    "link": "https://arxiv.org/pdf/2307.08581.pdf",
    "source": "auto-script",
    "tags": [
      "Instruction Tuning"
    ],
    "authors": "Yang Zhao, Zhijie Lin, Daquan Zhou, Zilong Huang, Jiashi Feng, Bingyi Kang",
    "description": "本文介绍了 BuboGPT，这是一种具备视觉定位（visual grounding）能力的多模态大语言模型，能够实现视觉、音频和语言之间的跨模态交互，并提供对视觉对象及其他模态的细粒度理解。针对现有模型（如 MiniGPT-4、LLaVA 等）在生成描述时缺乏对输入特定部分进行精准定位、仅能构建粗粒度映射的问题，BuboGPT 在生成响应或描述时能够指出图像中物体的具体位置。本文的主要贡献包括：1) 开发了一个基于 SAM 的即插即用视觉定位模块，用于提取句子中的实体并寻找图像中对应的掩码；2) 提出了一个两阶段训练方案及指令数据集，以赋予模型对文本-图像-音频的联合理解能力。实验表明，BuboGPT 在与人交互过程中展现了出色的多模态理解和视觉定位能力，且在面对任意模态组合（无论对齐与否）时均表现出良好的一致性。",
    "image": "",
    "code": "https://bubo-gpt.github.io/"
  },
  {
    "title": "SVIT: Scaling up Visual Instruction Tuning",
    "venue": "arXiv",
    "date": "2023",
    "link": "https://arxiv.org/pdf/2307.04087.pdf",
    "source": "auto-script",
    "tags": [
      "Instruction Tuning"
    ],
    "authors": "Bo Zhao, Boya Wu, Muyang He, Tiejun Huang",
    "description": "本文介绍了 SVIT，这是一个通过规模化视觉指令微调（SVIT）来提升多模态能力的项目。针对高质量指令微调数据匮乏导致的多模态模型能力受限问题，研究团队构建了一个包含 4.2 万条数据的视觉指令数据集，其中包括 160 万条对话问答对、160 万条复杂推理问答对、100 万条指代问答对以及 10.6 万条详细图像描述。该数据集不仅规模庞大，还具有高质量和丰富的多样性，其生成方式是利用 GPT-4 对丰富的图像人工标注进行提示。此外，本文提出了一种新的数据配方，通过筛选出具备更佳多样性和平衡性的子集，有效激发了模型的卓越性能。广泛的实验证明，在本项目数据集上训练的 SVIT-v1.5 模型在多个热门基准测试中均超越了现有的先进多模态大语言模型。",
    "image": "",
    "code": "https://github.com/BAAI-DCAI/Visual-Instruction-Tuning"
  },
  {
    "title": "GPT4RoI: Instruction Tuning Large Language Model on Region-of-Interest",
    "venue": "arXiv",
    "date": "2023",
    "link": "https://arxiv.org/pdf/2307.03601.pdf",
    "source": "auto-script",
    "tags": [
      "Instruction Tuning"
    ],
    "authors": "Shilong Zhang, Peize Sun, Shoufa Chen, Min Xiao, Wenqi Shao, et al.",
    "description": "本文提出了空间指令微调（spatial instruction tuning），旨在解决现有视觉指令微调模型因缺乏“区域-文本”对训练而导致的细粒度多模态理解能力不足的问题。该方法在指令中引入对感兴趣区域（RoI）的引用，并在输入大语言模型（LLM）前将引用替换为 RoI 特征，使其与语言嵌入交错形成序列。基于 7 个区域-文本对数据集训练的 GPT4RoI 模型，实现了超越以往图像级模型的交互与对话体验：(1) 超越语言的交互：用户可以通过语言和绘制边界框的方式与模型交互，灵活调整指代粒度；(2) 全面的多模态能力：模型能够挖掘 RoI 内的颜色、形状、材料、动作等多种属性信息，并能基于常识对多个区域进行推理。在视觉常识推理（VCR）数据集上，GPT4RoI 达到了 81.6% 的准确率，大幅领先现有模型（第二名为 75.6%），并接近人类 85.0% 的表现水平。",
    "image": "",
    "code": "https://github.com/jshilong/GPT4RoI"
  },
  {
    "title": "What Matters in Training a GPT4-Style Language Model with Multimodal Inputs?",
    "venue": "arXiv",
    "date": "2023",
    "link": "https://arxiv.org/pdf/2307.02469.pdf",
    "source": "auto-script",
    "tags": [
      "Instruction Tuning"
    ],
    "authors": "Yan Zeng, Hanbo Zhang, Jiani Zheng, Jiangnan Xia, Guoqiang Wei, et al.",
    "description": "本文对多模态大语言模型的训练进行了系统且全面的定量与定性研究。针对现有研究在网络结构、训练数据和训练策略等设计选择上缺乏深入讨论的问题，作者通过控制变量法实现了 20 多种模型变体：在网络结构方面，对比了不同的大语言模型（LLM）骨干网络及设计方案；在训练数据方面，探究了数据量与采样策略的影响；在指令方面，考察了多样化提示词对模型指令遵循能力的作用；在基准测试方面，通过众包方式贡献了首个涵盖图像和视频任务的综合评估集。基于研究发现，作者推出了 Lynx 模型。与现有的开源 GPT4 风格模型相比，Lynx 在保持最强多模态生成能力的同时，实现了最准确的多模态理解。",
    "image": "",
    "code": "https://lynx-llm.github.io/"
  },
  {
    "title": "mPLUG-DocOwl: Modularized Multimodal Large Language Model for Document Understanding",
    "venue": "arXiv",
    "date": "2023",
    "link": "https://arxiv.org/pdf/2307.02499.pdf",
    "source": "auto-script",
    "tags": [
      "Instruction Tuning"
    ],
    "authors": "Jiabo Ye, Anwen Hu, Haiyang Xu, Qinghao Ye, Ming Yan, et al.",
    "description": "本文介绍了基于 mPLUG-Owl 开发的 mPLUG-DocOwl，旨在实现无 OCR（OCR-free）的文档理解。针对现有模型在缺乏领域内训练时容易忽略复杂表格、大段文本等细粒度 OCR 特征的问题，研究团队首先构建了一个涵盖广泛视觉文本理解任务的指令微调数据集。随后，通过统一的指令微调策略，将模型在纯语言、通用视觉语言以及文档指令数据集上进行联合训练，从而增强其无 OCR 文档理解能力。此外，研究者还构建了评估集 LLMDoc，用以衡量模型在指令遵循和文档理解方面的表现。实验结果表明，mPLUG-DocOwl 的性能优于现有的多模态模型，展现出强大的文档理解能力，且在未经特定微调的情况下，能很好地泛化至多种下游任务。",
    "image": "",
    "code": "https://github.com/X-PLUG/mPLUG-DocOwl"
  },
  {
    "title": "Visual Instruction Tuning with Polite Flamingo",
    "venue": "arXiv",
    "date": "2023",
    "link": "https://arxiv.org/pdf/2307.01003.pdf",
    "source": "auto-script",
    "tags": [
      "Instruction Tuning"
    ],
    "authors": "Delong Chen, Jianfeng Liu, Wenliang Dai, Baoyuan Wang",
    "description": "本文研究了多模态大语言模型（LLM）在多任务微调过程中产生的负面效应，即“多模态对齐税”（multi-modal alignment tax）。该问题源于原始标注数据过于简略且缺乏格式化，导致模型生成的响应在礼貌程度等方面表现欠佳，降低了人类偏好。为此，本文引入了 Polite Flamingo，这是一个能够将原始标注转化为更具吸引力和礼貌格式的多模态响应重写器。Polite Flamingo 通过从自动扭曲的文本中重建高质量响应进行训练，并被应用于大规模视觉语言数据集的重写工作。在经过严格筛选后，研究团队构建了 PF-1M 数据集，并利用该数据集微调出了多模态大模型 Clever Flamingo。结合 U 型多阶段微调和多轮对话增强等创新方法，Clever Flamingo 在自动化评估和人工评价中均展现出在多模态理解与响应礼貌性方面的显著优势。",
    "image": "",
    "code": "https://github.com/ChenDelong1999/polite-flamingo"
  },
  {
    "title": "LLaVAR: Enhanced Visual Instruction Tuning for Text-Rich Image Understanding",
    "venue": "arXiv",
    "date": "2023",
    "link": "https://arxiv.org/pdf/2306.17107.pdf",
    "source": "auto-script",
    "tags": [
      "Instruction Tuning"
    ],
    "authors": "Yanzhe Zhang, Ruiyi Zhang, Jiuxiang Gu, Yufan Zhou, Nedim Lipka, et al.",
    "description": "本文介绍了 LLaVAR，这是一款旨在增强多模态大语言模型对图像中文字细节理解能力的模型。针对现有视觉指令微调模型在处理富含文本的图像时表现不足的问题，研究团队首先利用 OCR 工具从 LAION 数据集中提取了 42.2 万张富文本图像（如电影海报、书籍封面等）的文字信息；随后，通过向纯文本 GPT-4 提供识别出的文字和图像描述，生成了 1.6 万条针对这些图像的多轮对话数据。通过将这些新数据与现有的多模态指令遵循数据相结合，LLaVAR 在基于文本的视觉问答（VQA）数据集上的准确率较 LLaVA 提升了高达 20%，并在 ScienceQA 数据集上达到了 91.42% 的准确率。基于 GPT-4 的评估以及定性分析表明，LLaVAR 在处理自然图像和富文本图像方面均有显著进步，并能围绕包含图文的现实世界在线内容与人类进行出色的互动（如推理、写作和详述）。",
    "image": "",
    "code": "https://llavar.github.io/"
  },
  {
    "title": "Shikra: Unleashing Multimodal LLM's Referential Dialogue Magic",
    "venue": "arXiv",
    "date": "2023",
    "link": "https://arxiv.org/pdf/2306.15195.pdf",
    "source": "auto-script",
    "tags": [
      "Instruction Tuning",
      "Chain-of-Thought"
    ],
    "authors": "Keqin Chen, Zhao Zhang, Weili Zeng, Richong Zhang, Feng Zhu, et al.",
    "description": "本文介绍了 Shikra，这是一款能够像人类对话一样在自然语言中处理空间坐标输入与输出的多模态大语言模型（MLLM）。针对现有 MLLM 缺乏在对话中进行空间参考能力的现状，Shikra 采用由视觉编码器、对齐层和大语言模型组成的简洁架构，无需额外词表、位置编码器、预/后检测模块或外部插件，所有输入输出均采用自然语言形式。由于参考对话（Referential Dialogue）涵盖了多种视觉语言（VL）任务，Shikra 能够自然地处理 REC、PointQA 等位置相关任务，以及图像描述和视觉问答等传统任务。实验结果证明了 Shikra 的卓越性能。此外，它还解锁了许多创新应用，例如在思维链（CoT）中提供所提及物体的坐标，以及对比用户指定区域的相似性。",
    "image": "",
    "code": "https://github.com/shikras/shikra"
  },
  {
    "title": "MotionGPT: Human Motion as a Foreign Language",
    "venue": "arXiv",
    "date": "2023",
    "link": "https://arxiv.org/pdf/2306.14795.pdf",
    "source": "auto-script",
    "tags": [
      "Instruction Tuning"
    ],
    "authors": "Biao Jiang, Xin Chen, Wen Liu, Jingyi Yu, Gang Yu, Tao Chen",
    "description": "本文介绍了 MotionGPT，这是一个统一、通用且用户友好的动作-语言大模型，旨在处理多种与人体动作相关的任务。针对构建统一语言与动作等多模态数据模型的挑战，研究者受人体动作具有类语言语义特性的启发，提出将人体动作视为一种特定的“身体语言”。MotionGPT 采用离散矢量量化技术将 3D 人体动作转换为动作标记（motion tokens），并在此基础上构建“动作词表”，从而以统一的方式对动作和文本进行语言建模。此外，借鉴提示学习（prompt learning）的思想，MotionGPT 在混合的动作-语言数据上进行预训练，并针对基于提示的问答任务进行微调。广泛的实验证明，MotionGPT 在文本驱动动作生成、动作描述、动作预测和动作插值等多个任务中均达到了领先水平。",
    "image": "",
    "code": "https://github.com/OpenMotionLab/MotionGPT"
  },
  {
    "title": "Macaw-LLM: Multi-Modal Language Modeling with Image, Audio, Video, and Text Integration",
    "venue": "arXiv",
    "date": "2023",
    "link": "https://arxiv.org/pdf/2306.09093.pdf",
    "source": "auto-script",
    "tags": [
      "Instruction Tuning"
    ],
    "authors": "Chenyang Lyu, Minghao Wu, Longyue Wang, Xinting Huang, Bingshuai Liu, et al.",
    "description": "本文介绍了 Macaw-LLM，这是一种将视觉、音频和文本信息无缝整合的新型多模态大语言模型（LLM）。该模型由三个核心组件构成：用于编码多模态数据的数据模态模块、利用预训练大语言模型能力的认知模块，以及用于协调不同表征的对齐模块。其中，创新的对齐模块能够将多模态特征无缝桥接至文本特征，从而简化了从模态模块到认知模块的适配过程。此外，研究团队还构建了一个包含 6.9 万个图像实例和 5 万个视频实例的大规模多轮对话多模态指令数据集。通过公开数据、代码和模型，该项目旨在为多模态大语言模型的未来研究奠定基础，并扩展模型处理多样化数据模态及复杂现实场景的能力。",
    "image": "",
    "code": "https://github.com/lyuchenyang/Macaw-LLM"
  },
  {
    "title": "LAMM: Language-Assisted Multi-Modal Instruction-Tuning Dataset, Framework, and Benchmark",
    "venue": "arXiv",
    "date": "2023",
    "link": "https://arxiv.org/pdf/2306.06687.pdf",
    "source": "auto-script",
    "tags": [
      "Evaluation",
      "Instruction Tuning"
    ],
    "authors": "Zhenfei Yin, Jiong Wang, Jianjian Cao, Zhelun Shi, Dingning Liu, et al.",
    "description": "本文介绍了 LAMM 项目，这是多模态大语言模型（MLLM）领域最早的开源尝试之一，包含了一个语言辅助的多模态指令微调数据集、框架及基准测试。针对 GPT-4V 和 Bard 等模型透明度不足、难以支持学术研究的问题，LAMM 旨在构建一个持续发展的生态系统，重点培养能够弥合想法与执行之间鸿沟、实现无缝人机交互的 AI 智能体。本文的主要贡献包括：1) 提出了一个涵盖 2D 和 3D 视觉任务的综合数据集与基准测试，并通过实验验证了其有效性；2) 阐述了构建 MLLM 指令微调数据集和基准测试的详细方法论，支持研究快速扩展至不同领域、任务和模态；3) 提供了一个针对模态扩展进行优化的模型训练框架、基线模型以及详尽的实验观察与分析。",
    "image": "",
    "code": "https://openlamm.github.io/"
  },
  {
    "title": "Video-ChatGPT: Towards Detailed Video Understanding via Large Vision and Language Models",
    "venue": "arXiv",
    "date": "2023",
    "link": "https://arxiv.org/pdf/2306.05424.pdf",
    "source": "auto-script",
    "tags": [
      "Instruction Tuning"
    ],
    "authors": "Muhammad Maaz, Hanoona Rasheed, Salman Khan, Fahad Shahbaz Khan",
    "description": "本文介绍了 Video-ChatGPT，这是一款旨在解决视频对话领域研究不足问题的多模态大模型。该模型通过将适配视频的视觉编码器与大语言模型（LLM）相结合，实现了对视频内容的理解以及生成详细对话的能力。研究团队通过人工与半自动化相结合的流水线，构建了一个包含 10 万条“视频-指令”对的新型数据集，该流水线具有易扩展和对标签噪声鲁棒的特点。此外，本文还开发了一套针对视频对话模型的定量评估框架，用于客观分析此类模型在实际应用中的优劣势。",
    "image": "",
    "code": "https://github.com/mbzuai-oryx/Video-ChatGPT"
  },
  {
    "title": "MIMIC-IT: Multi-Modal In-Context Instruction Tuning",
    "venue": "arXiv",
    "date": "2023",
    "link": "https://arxiv.org/pdf/2306.05425.pdf",
    "source": "auto-script",
    "tags": [
      "In-Context Learning",
      "Instruction Tuning"
    ],
    "authors": "Bo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang, Fanyi Pu, et al.",
    "description": "本文介绍了多模态上下文指令微调数据集 MIMIC-IT，该数据集包含 280 万个多模态指令-响应对，其中 220 万个指令源自图像和视频。针对现有视觉语言指令数据在数量、多样性和创意方面的不足，MIMIC-IT 为每个数据对配备了多模态上下文信息，旨在增强视觉语言模型（VLM）在感知、推理和规划方面的能力。该数据集通过名为 Syphus 的自动化标注流水线构建，结合了人类专业知识与 GPT 的生成能力。基于 MIMIC-IT 数据集，研究团队训练了大型视觉语言模型 Otter。基准测试评估显示，Otter 在多模态感知、推理及上下文学习方面表现卓越；人工评估则证实其能有效对齐用户意图。目前，MIMIC-IT 数据集、标注流水线、基准测试及 Otter 模型均已开源。",
    "image": "",
    "code": "https://github.com/EvolvingLMMs-Lab/Otter"
  },
  {
    "title": "M<sup>3</sup>IT: A Large-Scale Dataset towards Multi-Modal Multilingual Instruction Tuning",
    "venue": "arXiv",
    "date": "2023",
    "link": "https://arxiv.org/pdf/2306.04387.pdf",
    "source": "auto-script",
    "tags": [
      "Instruction Tuning"
    ],
    "authors": "Lei Li, Yuwei Yin, Shicheng Li, Liang Chen, Peiyi Wang, et al.",
    "description": "本文介绍了多模态、多语言指令微调（M³IT）数据集，旨在优化视觉语言模型（VLM）与人类指令的对齐。针对高质量视觉语言指令数据集匮乏的挑战，M³IT 包含了 40 个精心挑选的数据集，涵盖 240 万个实例和 400 条人工编写的任务指令，并统一转换为“视觉转文本”结构。为了增强通用性，该研究利用先进的翻译系统将核心任务翻译成 80 种语言。在任务覆盖范围、指令数量和实例规模上，M³IT 均超越了以往的数据集。此外，研究团队还开发了在 M³IT 上训练的 Ying-VLM 模型，该模型展现出回答涉及世界知识的复杂问题、泛化至未见过的视频任务以及理解中文指令的强大能力。该数据集已正式开源，以推动相关领域的进一步研究。",
    "image": "",
    "code": "https://huggingface.co/datasets/MMInstruction/M3IT"
  },
  {
    "title": "Video-LLaMA: An Instruction-tuned Audio-Visual Language Model for Video Understanding",
    "venue": "arXiv",
    "date": "2023",
    "link": "https://arxiv.org/pdf/2306.02858.pdf",
    "source": "auto-script",
    "tags": [
      "Instruction Tuning"
    ],
    "authors": "Hang Zhang, Xin Li, Lidong Bing",
    "description": "本文介绍了 Video-LLaMA，这是一个旨在赋予大语言模型（LLM）理解视频中视觉和审计内容能力的多模态框架。该框架在冻结的预训练视觉/音频编码器以及冻结的 LLM 基础上进行跨模态引导训练。针对视频理解中的两大挑战——捕捉视觉场景的时序变化以及整合视听信号，Video-LLaMA 提出了相应的解决方案：首先，通过引入 Video Q-former 将预训练图像编码器整合至视频编码器中，并利用视频转文本生成任务学习视频与语言的对应关系；其次，利用 ImageBind 作为预训练音频编码器，并在其基础上引入 Audio Q-former，为 LLM 学习合理的音频查询嵌入。为了将视听编码器的输出与 LLM 的嵌入空间对齐，该模型先在海量图文/视文对上进行预训练，随后在高质量的视觉指令数据集上进行微调。实验发现，Video-LLaMA 能够有效感知和理解视频内容，并基于视频中的视听信息生成有意义的响应。",
    "image": "",
    "code": "https://github.com/DAMO-NLP-SG/Video-LLaMA"
  },
  {
    "title": "LLaVA-Med: Training a Large Language-and-Vision Assistant for Biomedicine in One Day",
    "venue": "arXiv",
    "date": "2023",
    "link": "https://arxiv.org/pdf/2306.00890.pdf",
    "source": "auto-script",
    "tags": [
      "Instruction Tuning"
    ],
    "authors": "Chunyuan Li, Cliff Wong, Sheng Zhang, Naoto Usuyama, Haotian Liu, et al.",
    "description": "本文介绍了 LLaVA-Med，这是一款专为生物医学领域设计的视觉语言对话助手，能够针对生物医学图像回答开放式研究问题。针对现有通用多模态模型在生物医学图像理解方面深度不足的问题，研究者提出了一种高性价比的训练方法。其核心方案是利用从 PubMed Central 提取的大规模生物医学图片-说明文字数据集，并通过 GPT-4 基于说明文字自发生成开放式的指令遵循数据。在训练策略上，该模型采用了一种创新的课程学习方法：首先利用原始的图片-说明对来对齐医学词汇，随后利用 GPT-4 生成的对话数据掌握开放式语义，模拟外行逐渐习得医学知识的过程。借助该方法，LLaVA-Med 仅需不到 15 小时（使用 8 张 A100 显卡）即可完成训练。实验表明，LLaVA-Med 具备出色的多模态对话能力，并在三个标准生物医学视觉问答（VQA）基准测试中，在特定指标上超越了此前最先进的有监督模型。",
    "image": "",
    "code": "https://github.com/microsoft/LLaVA-Med"
  },
  {
    "title": "GPT4Tools: Teaching Large Language Model to Use Tools via Self-instruction",
    "venue": "arXiv",
    "date": "2023",
    "link": "https://arxiv.org/pdf/2305.18752.pdf",
    "source": "auto-script",
    "tags": [
      "Visual Reasoning",
      "Instruction Tuning"
    ],
    "authors": "Rui Yang, Lin Song, Yanwei Li, Sijie Zhao, Yixiao Ge, et al.",
    "description": "本文介绍了 GPT4Tools，这是一种基于自指令（self-instruct）的方法，旨在高效地使开源大语言模型（如 LLaMA 和 OPT）具备使用多模态工具的能力。针对先进私有模型（如 GPT-4）计算成本高昂且数据不公开的问题，GPT4Tools 通过向先进的教师模型提供多种多模态上下文来生成指令遵循数据集。该方法采用低秩自适应（LoRA）优化技术，使开源模型能够调用工具来解决包括视觉理解和图像生成在内的一系列视觉问题。此外，本文还提供了一个评估模型工具使用能力的基准测试，涵盖了零样本和微调两种评估方式。广泛的实验证明，该方法在多种语言模型上均具有显著效果，不仅大幅提升了模型调用已知工具的准确率，还赋予了模型对未见工具的零样本调用能力。",
    "image": "",
    "code": "https://github.com/AILab-CVC/GPT4Tools"
  },
  {
    "title": "PandaGPT: One Model To Instruction-Follow Them All",
    "venue": "arXiv",
    "date": "2023",
    "link": "https://arxiv.org/pdf/2305.16355.pdf",
    "source": "auto-script",
    "tags": [
      "Instruction Tuning"
    ],
    "authors": "Yixuan Su, Tian Lan, Huayang Li, Jialu Xu, Yan Wang, Deng Cai",
    "description": "本文介绍了 PandaGPT，这是一种赋予大语言模型视觉和音频指令遵循能力的方法。初步实验表明，PandaGPT 能够执行生成详细图像描述、根据视频创作故事以及回答音频相关问题等复杂任务。更具特色的是，PandaGPT 能够同时接收多种模态的输入并自然地组合其语义，例如将图像/视频中的物体外观与其在音频中的声音联系起来。在架构上，PandaGPT 结合了 ImageBind 的多模态编码器与 Vicuna 大语言模型。值得注意的是，该模型在训练过程中仅需对齐的图文数据，得益于 ImageBind 将不同模态映射到统一空间的能力，PandaGPT 对图像和文本以外的模态（如视频、音频、深度、热成像和惯性测量单元 IMU）表现出了涌现性的零样本跨模态处理能力。该项目旨在迈向通用人工智能（AGI），使其能够像人类一样整体性地感知和理解不同模态的输入。",
    "image": "",
    "code": "https://panda-gpt.github.io/"
  },
  {
    "title": "ChatBridge: Bridging Modalities with Large Language Model as a Language Catalyst",
    "venue": "arXiv",
    "date": "2023",
    "link": "https://arxiv.org/pdf/2305.16103.pdf",
    "source": "auto-script",
    "tags": [
      "Instruction Tuning"
    ],
    "authors": "Zijia Zhao, Longteng Guo, Tongtian Yue, Sihan Chen, Shuai Shao, et al.",
    "description": "本文介绍了 ChatBridge，这是一种新型多模态语言模型，它利用语言的表达能力作为催化剂，旨在弥合不同模态之间的鸿沟。该研究表明，仅需与语言对齐的双模态数据，就足以将所有模态连接起来。ChatBridge 基于先进的大语言模型（LLM），并将其零样本能力扩展至多种多模态输入。该模型的训练分为两个阶段：第一阶段将各模态与语言对齐，从而激发出模态间的关联与协作能力；第二阶段利用新构建的多模态指令微调数据集 MULTIS 对模型进行微调，使其与用户意图对齐，该数据集涵盖了涉及文本、图像、视频和音频的 16 种多模态任务。实验结果在涵盖上述四种模态的零样本多模态任务中展示了强大的定量与定性表现。",
    "image": "",
    "code": "https://iva-chatbridge.github.io/"
  },
  {
    "title": "Cheap and Quick: Efficient Vision-Language Instruction Tuning for Large Language Models",
    "venue": "arXiv",
    "date": "2023",
    "link": "https://arxiv.org/pdf/2305.15023.pdf",
    "source": "auto-script",
    "tags": [
      "Instruction Tuning"
    ],
    "authors": "Gen Luo, Yiyi Zhou, Tianhe Ren, Shengxin Chen, Xiaoshuai Sun, Rongrong Ji",
    "description": "本文介绍了 MMA（Mixture-of-Modality Adaptation），这是一种针对大语言模型（LLM）进行视觉语言（VL）适配的高效且低成本解决方案。针对现有方案因参数量巨大且在指令微调前需要大规模预训练而导致成本高昂的问题，MMA 弃用了重型连接网络，转而采用轻量化适配器（adapters）来桥接 LLM 与视觉语言任务，从而实现了图像编码器与语言模型的联合优化。同时，MMA 配备了一种路由算法，使模型能够在单模态和多模态指令之间自动切换，且不损害其原有的自然语言理解能力。研究团队将 MMA 应用于 LLaMA 模型，构建了名为 LaVIN 的大型视觉语言指令模型。在多模态科学问答和多模态对话实验中，LaVIN 不仅展现出优于现有模型的性能和训练效率，还证明了其作为通用聊天机器人的巨大潜力。值得关注的是，LaVIN 的训练成本极低，仅需 1.4 小时训练时间和 3.8M 可训练参数，有力验证了 MMA 的有效性。",
    "image": "",
    "code": ""
  },
  {
    "title": "DetGPT: Detect What You Need via Reasoning",
    "venue": "arXiv",
    "date": "2023",
    "link": "https://arxiv.org/pdf/2305.14167.pdf",
    "source": "auto-script",
    "tags": [
      "Instruction Tuning"
    ],
    "authors": "Renjie Pi, Jiahui Gao, Shizhe Diao, Rui Pan, Hanze Dong, et al.",
    "description": "本文介绍了一种名为“基于推理的对象检测”的新范式，并据此开发了 DetGPT。与依赖特定类别名称的传统检测方法不同，该范式允许用户通过自然语言指令与系统交互。DetGPT 结合了先进的多模态模型与开集（open-vocabulary）对象检测器，能够根据用户指令和视觉场景进行推理。这使得模型即便在用户未直接提及目标物体的情况下，也能根据其需求自动定位相关物体。例如，当用户表示想喝冷饮时，DetGPT 能分析图像、识别冰箱，并基于常识定位其中的饮料。这种灵活性使其在机器人技术、自动化及自动驾驶等领域具有广泛的应用潜力。DetGPT 展现了更高级且直观的人机交互可能，为构建更具互动性和通用性的对象检测系统提供了新思路。",
    "image": "",
    "code": "https://detgpt.github.io/"
  },
  {
    "title": "Pengi: An Audio Language Model for Audio Tasks",
    "venue": "NeurIPS",
    "date": "2023",
    "link": "https://arxiv.org/pdf/2305.11834.pdf",
    "source": "auto-script",
    "tags": [
      "Instruction Tuning"
    ],
    "authors": "Soham Deshmukh, Benjamin Elizalde, Rita Singh, Huaming Wang",
    "description": "本文介绍了 Pengi，这是一种新型音频语言模型，它通过将所有音频任务转化为文本生成任务，利用迁移学习来解决音频处理领域的问题。针对现有模型在音频描述（Audio Captioning）或音频问答（Audio Q&A）等开放式任务中缺乏语言生成能力的问题，Pengi 采用了一种统一的架构：它接收音频录制和文本作为输入，并生成自由格式的文本作为输出。在模型内部，音频编码器和文本编码器分别将输入转化为连续嵌入序列，并将这些序列作为前缀来提示（prompt）一个预训练且冻结的语言模型。这种设计使 Pengi 无需任何额外的微调或特定任务扩展，即可同时处理开放式和封闭式任务。在 22 个下游任务的评估中，该方法在多个任务上达到了领先水平。实验结果表明，将语言模型与音频模型相结合是实现通用音频理解的关键一步。",
    "image": "",
    "code": "https://github.com/microsoft/Pengi"
  },
  {
    "title": "VisionLLM: Large Language Model is also an Open-Ended Decoder for Vision-Centric Tasks",
    "venue": "arXiv",
    "date": "2023",
    "link": "https://arxiv.org/pdf/2305.11175.pdf",
    "source": "auto-script",
    "tags": [
      "Instruction Tuning"
    ],
    "authors": "Wenhai Wang, Zhe Chen, Xiaokang Chen, Jiannan Wu, Xizhou Zhu, et al.",
    "description": "本文介绍了 VisionLLM，这是一个基于大语言模型（LLM）的视觉任务框架，旨在解决现有视觉基础模型（VFM）受限于预定义任务形式、难以像 LLM 那样处理开放式任务的问题。该框架通过将图像视为一种“外语”，并将以视觉为中心的任务与语言任务进行对齐，提供了一个统一的视觉和语言任务视角。在这种视角下，视觉任务可以通过语言指令进行灵活定义和管理，并由基于 LLM 的解码器根据指令对开放式任务做出预测。广泛的实验表明，VisionLLM 能够根据语言指令实现从细粒度物体级到粗粒度任务级的不同程度的定制化。值得注意的是，作为一个通用框架，该模型在 COCO 数据集上达到了超过 60% 的 mAP，表现与特定的检测模型相当。研究团队希望该模型能为通用视觉和语言模型建立新的基准。",
    "image": "",
    "code": "https://github.com/OpenGVLab/VisionLLM"
  },
  {
    "title": "Listen, Think, and Understand",
    "venue": "arXiv",
    "date": "2023",
    "link": "https://arxiv.org/pdf/2305.10790.pdf",
    "source": "auto-script",
    "tags": [
      "Instruction Tuning"
    ],
    "authors": "Yuan Gong, Hongyin Luo, Alexander H. Liu, Leonid Karlinsky, James Glass",
    "description": "本文介绍了新型音频基础模型 LTU（Listen, Think, and Understand），旨在使人工智能系统同时具备音频感知与推理能力。针对现有音频模型大多局限于将输入映射到预定义标签集、缺乏像人类一样理解声音细节及进行逻辑推理的问题，研究团队结合了大语言模型（LLM）的推理优势与音频感知技术。为此，研究者构建了包含 190 万个封闭式和 370 万个开放式“音频-问题-答案”元组的大规模数据集 OpenAQA-5M，并采用“从感知到理解”的课程学习方法进行自回归训练。实验表明，LTU 不仅在音频分类和描述等传统任务上表现强劲，还展现出了现有模型所缺失的音频推理与理解能力。LTU 是首批专注于通用音频（而非仅限于语音）理解的多模态大语言模型之一。",
    "image": "",
    "code": "https://github.com/YuanGongND/ltu"
  },
  {
    "title": "PMC-VQA: Visual Instruction Tuning for Medical Visual Question Answering",
    "venue": "arXiv",
    "date": "2023",
    "link": "https://arxiv.org/pdf/2305.10415.pdf",
    "source": "auto-script",
    "tags": [
      "Instruction Tuning"
    ],
    "authors": "Xiaoman Zhang, Chaoyi Wu, Ziheng Zhao, Weixiong Lin, Ya Zhang, et al.",
    "description": "本文将医疗视觉问答（MedVQA）重构为符合人机交互习惯的生成式任务，并提出了一种通过将预训练视觉编码器与大语言模型进行对齐的医疗视觉理解模型。研究团队建立了一套可扩展的流水线，构建了名为 PMC-VQA 的大规模医疗视觉问答数据集，包含 14.9 万张图像和 22.7 万个涵盖多种模态及疾病的问答对。该模型在 PMC-VQA 上进行训练，并在 VQA-RAD、SLAKE 和 Image-Clef-2019 等多个公共基准测试中进行了微调，在生成相关且准确的自由格式回答方面显著优于现有的 MedVQA 模型。此外，本文还提出了一个经过人工校验且更具挑战性的测试集，以更好地监测生成式 MedVQA 方法的发展，并设立了排行榜作为跟踪前沿进展的集中资源。PMC-VQA 数据集和 MedVInT 模型分别为该研究领域提供了核心资源和技术突破。",
    "image": "",
    "code": "https://github.com/xiaoman-zhang/PMC-VQA"
  },
  {
    "title": "InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning",
    "venue": "arXiv",
    "date": "2023",
    "link": "https://arxiv.org/pdf/2305.06500.pdf",
    "source": "auto-script",
    "tags": [
      "Instruction Tuning"
    ],
    "authors": "Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, et al.",
    "description": "本文对基于预训练 BLIP-2 模型的视觉语言指令微调进行了系统而全面的研究。针对视觉输入带来的输入分布丰富及任务多样性挑战，研究团队收集了 26 个公开数据集，涵盖了广泛的任务与能力，并将其统一转换为指令微调格式。此外，本文引入了一种“指令感知查询转换器”（instruction-aware Query Transformer），能够根据给定指令提取关键特征。在 13 个内部数据集上训练后，InstructBLIP 在全部 13 个保留（held-out）测试数据集上均取得了领先的零样本（zero-shot）性能，显著优于 BLIP-2 和规模更大的 Flamingo 模型。在单个下游任务的微调中，该模型同样表现卓越（例如在 ScienceQA 视觉问答中达到 90.7% 的准确率）。定性分析进一步证实了 InstructBLIP 相比于同期多模态模型的优势。",
    "image": "",
    "code": "https://github.com/salesforce/LAVIS/tree/main/projects/instructblip"
  },
  {
    "title": "VideoChat: Chat-Centric Video Understanding",
    "venue": "arXiv",
    "date": "2023",
    "link": "https://arxiv.org/pdf/2305.06355.pdf",
    "source": "auto-script",
    "tags": [
      "Instruction Tuning"
    ],
    "authors": "KunChang Li, Yinan He, Yi Wang, Yizhuo Li, Wenhai Wang, et al.",
    "description": "本文介绍了 VideoChat，这是一个端到端的以聊天为中心的视频理解系统。该系统通过可学习的神经接口将视频基础模型与大语言模型相结合，在时空推理、事件定位和因果关系推断方面表现出色。为了对该系统进行指令微调，研究团队构建了一个以视频为中心的指令数据集，包含数千个配有详细描述和对话的视频；该数据集重点突出了时空推理能力并捕捉了因果关系，为训练此类系统提供了宝贵资源。初步的定性实验展示了该系统在广泛视频应用场景中的潜力，可作为未来以聊天为中心的视频理解研究的简易原型系统。",
    "image": "",
    "code": "https://github.com/OpenGVLab/Ask-Anything"
  },
  {
    "title": "MultiModal-GPT: A Vision and Language Model for Dialogue with Humans",
    "venue": "arXiv",
    "date": "2023",
    "link": "https://arxiv.org/pdf/2305.04790.pdf",
    "source": "auto-script",
    "tags": [
      "Instruction Tuning"
    ],
    "authors": "Tao Gong, Chengqi Lyu, Shilong Zhang, Yudong Wang, Miao Zheng, et al.",
    "description": "本文介绍了名为 MultiModal-GPT 的视觉语言模型，旨在与人类进行多轮对话。MultiModal-GPT 能够遵循人类的各种指令，如生成详细说明、计算特定目标数量以及回答通用问题。该模型基于 OpenFlamingo 进行了参数高效微调，在大语言模型的交叉注意力和自注意力部分均引入了低秩适配器（LoRA）。研究团队首先构建了包含视觉和语言数据的指令模板进行多模态指令微调，使模型能够理解并遵循人类指令。研究发现，训练数据的质量对对话性能至关重要，少量包含简短回答的数据就可能导致模型对任何指令都给出简短响应。为了进一步增强 MultiModal-GPT 的人机聊天能力，研究者采用相同的指令模板，将纯语言指令遵循数据与视觉语言指令数据进行联合训练，有效提升了对话表现。多项演示证明了 MultiModal-GPT 与人类进行持续对话的能力。",
    "image": "",
    "code": "https://github.com/open-mmlab/Multimodal-GPT"
  },
  {
    "title": "X-LLM: Bootstrapping Advanced Large Language Models by Treating Multi-Modalities as Foreign Languages",
    "venue": "arXiv",
    "date": "2023",
    "link": "https://arxiv.org/pdf/2305.04160.pdf",
    "source": "auto-script",
    "tags": [
      "Instruction Tuning"
    ],
    "authors": "Feilong Chen, Minglun Han, Haozhi Zhao, Qingyang Zhang, Jing Shi, et al.",
    "description": "本文介绍了 X-LLM，这是一种旨在赋予大语言模型（LLM）多模态能力的框架。通过使用 X2L 接口将图像、语音、视频等多种模态（X）转化为 LLM 能够理解的“外语”（L），并输入到 ChatGLM 等大型语言模型中。X-LLM 的训练分为三个阶段：首先，分别训练各个 X2L 接口，使其与各自冻结的单模态编码器对齐以转换信息；其次，通过 X2L 接口独立将各单模态编码器与 LLM 进行对齐；最后，通过联合训练实现多模态能力的整合。实验表明，X-LLM 展现出色的多模态对话能力，在未见过的图像和指令上表现出类似 GPT-4 的行为，并在合成的多模态指令遵循数据集上达到了相对于 GPT-4 84.5% 的得分。此外，该研究还对基于 LLM 的自动语音识别（ASR）及多模态 ASR 进行了定量测试，旨在推动基于 LLM 的语音识别技术发展。",
    "image": "",
    "code": "https://x-llm.github.io/"
  },
  {
    "title": "LMEye: An Interactive Perception Network for Large Language Models",
    "venue": "arXiv",
    "date": "2023",
    "link": "https://arxiv.org/pdf/2305.03701.pdf",
    "source": "auto-script",
    "tags": [
      "Instruction Tuning"
    ],
    "authors": "Yunxin Li, Baotian Hu, Xinyu Chen, Lin Ma, Yong Xu, Min Zhang",
    "description": "本文介绍了 LMEye，这是一个为大语言模型（LLM）设计的类人眼插件式交互感知网络。针对以往方法使用简单的视觉映射网络或 Q-former 仅对图像特征进行单次投影，而忽略了图像与用户查询之间交互（即“静态视觉信息”）的问题，LMEye 实现了“动态视觉信息交互”，允许 LLM 根据不同的人类指令主动请求所需的视觉信息。在架构上，LMEye 包含一个提供基础图像感知的映射网络，以及负责接收 LLM 请求、执行基于请求的视觉信息交互并回传交互结果的专用模块。在这种机制下，LLM 负责理解用户查询，向交互模块发出相应请求，并基于交织的多模态信息生成响应。实验表明，LMEye 在多个多模态基准测试中显著提升了零样本性能，且参数量较以往方法更少。",
    "image": "",
    "code": "https://github.com/YunxinLi/LingCloud"
  },
  {
    "title": "LLaMA-Adapter V2: Parameter-Efficient Visual Instruction Model",
    "venue": "arXiv",
    "date": "2023",
    "link": "https://arxiv.org/pdf/2304.15010.pdf",
    "source": "auto-script",
    "tags": [
      "Instruction Tuning"
    ],
    "authors": "Peng Gao, Jiaming Han, Renrui Zhang, Ziyi Lin, Shijie Geng, et al.",
    "description": "本文介绍了 LLaMA-Adapter V2，这是一种参数高效的视觉指令模型，旨在提升大语言模型（LLM）的多模态推理和开放式指令遵循能力。针对第一代模型在处理开放式视觉指令方面的局限性，V2 版本进行了多项核心改进：首先，通过解锁更多可学习参数（如 norm、bias 和 scale），将指令遵循能力分布至整个 LLaMA 模型而非仅限于适配器；其次，采用早期融合策略，仅将视觉 Token 输入 LLM 的浅层网络，以实现更好的视觉知识整合；第三，引入了图像-文本对与指令遵循数据的联合训练范式，通过优化互不重叠的可学习参数组，有效缓解了任务间的干扰。在推理阶段，该模型还可接入字幕生成或 OCR 等专家模型，在不增加训练成本的前提下增强图像理解能力。LLaMA-Adapter V2 仅在 LLaMA 基础上新增 14M 参数，便展现出强大的多模态推理、纯文本指令遵循以及聊天交互能力。",
    "image": "",
    "code": "https://github.com/ZrrSkywalker/LLaMA-Adapter"
  },
  {
    "title": "mPLUG-Owl: Modularization Empowers Large Language Models with Multimodality",
    "venue": "arXiv",
    "date": "2023",
    "link": "https://arxiv.org/pdf/2304.14178.pdf",
    "source": "auto-script",
    "tags": [
      "Instruction Tuning"
    ],
    "authors": "Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, et al.",
    "description": "本文介绍了 mPLUG-Owl，这是一种通过对基础大语言模型（LLM）、视觉知识模块和视觉抽象器模块进行模块化学习，赋予 LLM 多模态能力的新型训练范式。该方法支持多模态协作，并采用两阶段对齐策略：第一阶段，在冻结 LLM 的情况下，训练视觉知识模块和抽象器模块以实现图文对齐；第二阶段，利用纯文本和多模态指令数据集，在冻结视觉知识模块的同时，对 LLM 的低秩自适应（LoRA）模块及抽象器模块进行联合微调。这种训练方式在学习视觉知识的同时，保持并增强了 LLM 原有的生成能力。研究团队还构建了视觉相关指令评估集 OwlEval。实验结果显示，mPLUG-Owl 在指令遵循、视觉理解、多轮对话及知识推理方面均优于现有模型。此外，该模型还表现出多图关联和场景文本理解等涌现能力，使其在纯视觉文档理解等复杂现实场景中具备应用潜力。",
    "image": "",
    "code": "https://github.com/X-PLUG/mPLUG-Owl"
  },
  {
    "title": "MiniGPT-4: Enhancing Vision-Language Understanding with Advanced Large Language Models",
    "venue": "arXiv",
    "date": "2023",
    "link": "https://arxiv.org/pdf/2304.10592.pdf",
    "source": "auto-script",
    "tags": [
      "Instruction Tuning"
    ],
    "authors": "Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, Mohamed Elhoseiny",
    "description": "本文介绍了 MiniGPT-4，该模型通过一个投影层将冻结的视觉编码器与先进的冻结大语言模型 Vicuna 相结合。这项研究首次揭示了通过将视觉特征与先进的大语言模型进行妥善对齐，即可使模型具备类似于 GPT-4 的多种高级多模态能力，例如生成详细的图像描述以及根据手绘草图创建网站。此外，MiniGPT-4 还展现出其他涌现能力，包括根据图片创作故事和诗歌、基于食物照片教用户烹饪等。实验发现，仅在简短的图文对上训练会导致模型输出重复或断裂等不自然语言，为此，研究团队在第二阶段构建了一个详细的图像描述数据集对模型进行微调，从而显著提升了生成的可靠性和整体可用性。",
    "image": "",
    "code": "https://minigpt-4.github.io/"
  },
  {
    "title": "Visual Instruction Tuning",
    "venue": "NeurIPS",
    "date": "2023",
    "link": "https://arxiv.org/pdf/2304.08485.pdf",
    "source": "auto-script",
    "tags": [
      "Instruction Tuning"
    ],
    "authors": "Haotian Liu, Chunyuan Li, Qingyang Wu, Yong Jae Lee",
    "description": "本文介绍了 LLaVA (Large Language and Vision Assistant)，这是首次尝试利用纯文本 GPT-4 生成多模态语言-图像指令遵循数据的研究。LLaVA 是一个端到端训练的大型多模态模型，通过连接视觉编码器和大语言模型（LLM），实现了通用的视觉与语言理解能力。初步实验表明，LLaVA 展现了出色的多模态对话能力，在未见过的图像或指令上表现出类似于多模态 GPT-4 的行为，并在合成的多模态指令遵循数据集上达到了相对于 GPT-4 85.1% 的得分。此外，当在 Science QA 数据集上进行微调时，LLaVA 与 GPT-4 协同达到了 92.53% 的准确率，创下了新的性能纪录。",
    "image": "",
    "code": "https://llava-vl.github.io/"
  },
  {
    "title": "LLaMA-Adapter: Efficient Fine-tuning of Language Models with Zero-init Attention",
    "venue": "ICLR",
    "date": "2023",
    "link": "https://arxiv.org/pdf/2303.16199.pdf",
    "source": "auto-script",
    "tags": [
      "Instruction Tuning"
    ],
    "authors": "Renrui Zhang, Jiaming Han, Chris Liu, Peng Gao, Aojun Zhou, et al.",
    "description": "本文介绍了 LLaMA-Adapter，这是一种轻量级的适配方法，旨在将 LLaMA 高效地微调为指令遵循模型。基于 5.2 万条自指令数据，LLaMA-Adapter 仅在冻结的 LLaMA 7B 模型基础上引入了 120 万个可学习参数，且在 8 张 A100 GPU 上的微调时间不到一小时。具体而言，该方法采用了一组可学习的适配提示，并将其添加至 Transformer 高层架构的单词标记之前。同时，本文提出了一种零初始化注意力和零门控机制，能够在自适应地将新指令线索注入 LLaMA 的同时，有效保留其预训练知识。得益于这种高效的训练方式，LLaMA-Adapter 生成的回答质量可与全参数微调 7B 参数的 Alpaca 模型相媲美。除了语言指令外，该方法还可以简单地扩展到多模态指令，训练出以图像为条件的 LLaMA 模型，并在 ScienceQA 和 COCO Caption 基准测试中展现出卓越的推理性能。此外，研究团队还在传统视觉和语言任务上评估了零初始化注意力机制对其他预训练模型（如 ViT、RoBERTa）的微调效果，证明了该方法强大的泛化能力。",
    "image": "",
    "code": "https://github.com/OpenGVLab/LLaMA-Adapter"
  },
  {
    "title": "MultiInstruct: Improving Multi-Modal Zero-Shot Learning via Instruction Tuning",
    "venue": "ACL",
    "date": "2022",
    "link": "https://arxiv.org/pdf/2212.10773.pdf",
    "source": "auto-script",
    "tags": [
      "Instruction Tuning"
    ],
    "authors": "Zhiyang Xu, Ying Shen, Lifu Huang",
    "description": "本文介绍了 MULTIINSTRUCT，这是首个多模态指令微调基准数据集，包含 10 大类、62 种不同的多模态任务，并统一采用序列到序列（seq-to-seq）格式。该数据集源自 21 个开源数据集，每项任务均配备了 5 条由专家编写的指令。研究者以 OFA 作为基础预训练模型进行多模态指令微调，并探索了多种迁移学习策略，旨在利用大规模纯文本数据集 NATURAL INSTRUCTIONS 进一步提升模型的零样本性能。实验结果证明，该模型在多种未见过的多模态任务中展现出强大的零样本能力，同时也验证了从纯文本指令数据集中进行迁移学习的有效性。此外，本文还设计了一种名为“敏感度”（Sensitivity）的新评估指标，用于衡量模型对不同指令变体的反应。研究结果表明，在多样化的任务和指令集上对模型进行微调，可以有效降低模型对每项任务中指令变化的敏感度。",
    "image": "",
    "code": "https://github.com/PLUM-Lab/MultiInstruct"
  },
  {
    "title": "Look Twice Before You Answer: Memory-Space Visual Retracing for Hallucination Mitigation in Multimodal Large Language Models",
    "venue": "arXiv",
    "date": "2024",
    "link": "https://arxiv.org/pdf/2410.03577",
    "source": "auto-script",
    "tags": [
      "Hallucination"
    ],
    "authors": "Xin Zou, Yizhou Wang, Yibo Yan, Yuanhuiyi Lyu, Kening Zheng, et al.",
    "description": "本文介绍了 MemVR，这是一种旨在缓解多模态大语言模型（MLLM）幻觉问题的新型解码范式。针对 MLLM 中由于文本解码器对视觉 Token 敏感度不足而导致的视觉信息“健忘”现象，研究团队受人类认知启发，提出了“再次查看”机制：当人们忘记刚看到的图像细节时，会重新观察图像以寻找事实答案。基于这一原理，MemVR 将视觉 Token 视为补充证据，并在推理过程中模型表现出高不确定性时，通过中层的触发层将这些视觉信息作为“键值记忆”（key-value memory）重新注入前馈网络（FFN）中。这种机制能够有效增强生成内容与事实的对齐程度。广泛的实验评估证明，MemVR 在不增加额外时间开销的情况下，显著减轻了多种 MLLM 的幻觉问题，并在通用基准测试中表现卓越。",
    "image": "",
    "code": "https://github.com/1zhou-Wang/MemVR"
  },
  {
    "title": "Interpreting and Editing Vision-Language Representations to Mitigate Hallucinations",
    "venue": "arXiv",
    "date": "2024",
    "link": "https://arxiv.org/pdf/2410.02762",
    "source": "auto-script",
    "tags": [
      "Hallucination"
    ],
    "authors": "Nick Jiang, Anish Kachinthaya, Suzie Petryk, Yossi Gandelsman",
    "description": "本文通过研究多模态视觉语言模型（VLM）的内部表征，提出了一种缓解幻觉问题的新方法。研究发现，将模型的内部图像表征映射到语言词汇表时，模型对真实物体的输出概率置信度明显高于幻觉物体，且利用这些概率可以实现对真实物体的空间定位。基于此观察，作者引入了一种知识擦除算法，通过将图像特征相对于幻觉物体特征进行线性正交化处理来消除幻觉。实验表明，这种针对模型潜变量表征的定向编辑方法在 COCO2014 数据集上可将幻觉减少高达 25.7%，同时保持原有的任务性能。研究结果证明，深入理解 VLM 的潜在表征不仅能增强模型的可靠性，还能赋予其零样本分割等新型能力。",
    "image": "",
    "code": "https://anishk23733.github.io/vl-interp/"
  },
  {
    "title": "FIHA: Autonomous Hallucination Evaluation in Vision-Language Models with Davidson Scene Graphs",
    "venue": "arXiv",
    "date": "2024",
    "link": "https://arxiv.org/pdf/2409.13612",
    "source": "auto-script",
    "tags": [
      "Hallucination"
    ],
    "authors": "Bowen Yan, Zhengsong Zhang, Liqiang Jing, Eftekhar Hossain, Xinya Du",
    "description": "本文介绍了 FIHA，一种针对大型视觉语言模型（LVLMs）细粒度幻觉进行自主评估的方法。针对现有评估方法依赖高成本人工标注且无法全面衡量物体关系、属性及各方面依赖性的问题，FIHA 提出了一种无需大语言模型（LLM-free）且无需标注（annotation-free）的评估框架。该方法能够在任何图像数据集上以极低成本生成问答对，从图像内容和文本说明两个维度评估幻觉。基于此方法，研究团队构建了名为 FIHA-v1 的基准测试，涵盖了来自 MSCOCO 和 Foggy 数据集的多种图像及问题。此外，FIHA 利用戴维森场景图（DSG）来组织问答对之间的结构，从而增强了评估结果的可靠性。通过对多种代表性模型的测试，该方法揭示了当前模型在处理复杂场景时的局限性与挑战。",
    "image": "",
    "code": "https://github.com/confidentzzzs/FIHA"
  },
  {
    "title": "Alleviating Hallucination in Large Vision-Language Models with Active Retrieval Augmentation",
    "venue": "arXiv",
    "date": "2024",
    "link": "https://arxiv.org/pdf/2408.00555",
    "source": "auto-script",
    "tags": [
      "Hallucination"
    ],
    "authors": "Xiaoye Qu, Qiyuan Chen, Wei Wei, Jishuo Sun, Jianfeng Dong",
    "description": "本文介绍了主动检索增强大型视觉语言模型（ARA）框架，旨在解决大型视觉语言模型（LVLMs）中频繁出现的幻觉问题。尽管检索增强技术在大语言模型（LLMs）中缓解幻觉成效显著，但在多模态领域的应用相对滞后，且直接迁移时甚至可能加剧幻觉。针对这一现状，ARA 框架从三个维度进行了创新设计：首先，基于图像的内在层次结构对检索目标进行解构；其次，确定最有效的检索方法并过滤出可靠的检索结果；最后，通过精准把握检索时机，仅在模型置信度较低时触发检索，而在高置信度时避免不必要的调用。研究团队在 LLaVA-1.5、Qwen-VL 和 mPLUG-Owl2 三种代表性模型上进行了四项基准测试，实验结果表明，通过适配的检索机制和合理的时机判断，该框架能有效减轻幻觉现象。本研究为如何利用更高效且极简的检索操作来优化 LVLMs 的可靠性提供了深刻见解。",
    "image": "",
    "code": ""
  },
  {
    "title": "Paying More Attention to Image: A Training-Free Method for Alleviating Hallucination in LVLMs",
    "venue": "ECCV",
    "date": "2024",
    "link": "https://arxiv.org/pdf/2407.21771",
    "source": "auto-script",
    "tags": [
      "Hallucination"
    ],
    "authors": "Shi Liu, Kecheng Zheng, Wei Chen",
    "description": "本文介绍了针对大型视觉语言模型（LVLMs）中“文本惯性”现象的一种无需训练的改进算法，旨在解决由于视觉编码器与语言模型规模不对等导致的幻觉问题。研究发现，LVLMs 在理解过程中往往由大语言模型（LLMs）占据主导地位，甚至在没有视觉输入的情况下也能生成一致的描述，表明其输出有时仅受上下文文本影响而非视觉内容。为平衡图像理解与语言推理，该算法通过自适应地调整和放大图像标记（tokens）的注意力权重，显著提升了视觉元素的显著性。同时，模型从多模态输入的对数概率（logits）中减去纯文本输入的对数概率，以此抵消 LLMs 的输出偏好。通过这种增强视觉信号并削弱语言模型固有偏向的方法，LVLMs 能够更多地关注图像内容，从而有效缓解文本惯性并降低各类幻觉指标。在多种 LVLMs 上的广泛实验证明，该方法在不同评价维度下均能显著减少幻觉输出的频率。",
    "image": "",
    "code": "https://lalbj.github.io/projects/PAI/"
  },
  {
    "title": "Evaluating and Analyzing Relationship Hallucinations in LVLMs",
    "venue": "ICML",
    "date": "2024",
    "link": "https://arxiv.org/pdf/2406.16449",
    "source": "auto-script",
    "tags": [
      "Hallucination"
    ],
    "authors": "Mingrui Wu, Jiayi Ji, Oucheng Huang, Jiale Li, Yuhang Wu, et al.",
    "description": "本文介绍了 R-Bench，这是一个专门用于评估大型视觉语言模型（LVLMs）中视觉关系幻觉的新型基准测试。针对以往研究大多集中于容易通过物体检测器缓解的物体幻觉，而忽略了对于视觉理解至关重要的物体间关系幻觉这一现状，R-Bench 设定了关注关系存在性的图像级问题以及评估局部视觉理解的实例级问题。研究识别出导致关系幻觉的三类共现关系：关系-关系、主体-关系以及关系-客体。分析表明，视觉指令微调数据集中的长尾分布显著影响了模型对视觉关系的理解。此外，实验揭示了当前 LVLMs 往往忽视视觉内容而过度依赖大语言模型的常识知识，且在基于上下文信息进行空间关系推理方面表现欠佳。",
    "image": "",
    "code": "https://github.com/mrwu-mac/R-Bench"
  },
  {
    "title": "AGLA: Mitigating Object Hallucinations in Large Vision-Language Models with Assembly of Global and Local Attention",
    "venue": "arXiv",
    "date": "2024",
    "link": "https://arxiv.org/pdf/2406.12718",
    "source": "auto-script",
    "tags": [
      "Hallucination"
    ],
    "authors": "Wenbin An, Feng Tian, Sicong Leng, Jiahao Nie, Haonan Lin, et al.",
    "description": "本文介绍了 AGLA（Assembly of Global and Local Attention），这是一种旨在缓解大型视觉语言模型（LVLMs）中物体幻觉问题的无需训练、即插即用的新方法。研究通过对不同 LVLMs 的分析指出，物体幻觉的根源之一在于模型对具有辨别力的图像特征关注不足，即模型往往过度依赖与提示词无关的全局特征，而忽视了与提示词相关的局部特征，从而削弱了其视觉定位能力。AGLA 通过同时汇集用于响应生成的全局特征和用于视觉辨别的局部特征来解决这一问题。具体而言，该方法引入了一种图像-提示词匹配方案，从图像中捕捉与提示词相关的局部特征，并生成一张突出相关内容、抑制无关干扰的增强视图。通过综合原始图像的生成性全局特征和增强图像的辨别性局部特征来校准逻辑概率（logit）分布，从而有效减少幻觉的产生。广泛的实验表明，AGLA 在缓解 LVLM 幻觉方面具有优越性，且在辨别性和生成性任务中均展现出广泛的适用性。",
    "image": "",
    "code": "https://github.com/Lackel/AGLA"
  },
  {
    "title": "CODE: Contrasting Self-generated Description to Combat Hallucination in Large Multi-modal Models",
    "venue": "arXiv",
    "date": "2024",
    "link": "https://arxiv.org/pdf/2406.01920",
    "source": "auto-script",
    "tags": [
      "Hallucination"
    ],
    "authors": "Junho Kim, Hyunjun Kim, Yeonju Kim, Yong Man Ro",
    "description": "本文介绍了 CODE（COuntering DEscription Contrastive Decoding），这是一种针对大型多模态模型（LMMs）中幻觉问题提出的新型对比解码方法。尽管 LMMs 在视觉理解和响应生成方面取得了巨大进步，但经常会产生与视觉内容无关的错误响应。CODE 利用模型在解码阶段自生成的详细描述作为对比参考，通过将这些自我生成的描述视为视觉副本，来纠正并增强响应与实际视觉内容的一致性。该方法通过动态调整 LMM 词汇表中下一标记预测的信息流和分布，显著提升了生成响应的连贯性和信息量。广泛的实验表明，CODE 在不增加额外训练的情况下，能够显著减少各种前沿 LMMs 在不同基准测试中的幻觉现象，并提高跨模态一致性，为现有 LMM 框架提供了一种简单且有效的即插即用解码策略。",
    "image": "",
    "code": "https://ivy-lvlm.github.io/CODE/"
  },
  {
    "title": "Mitigating Object Hallucination via Data Augmented Contrastive Tuning",
    "venue": "arXiv",
    "date": "2024",
    "link": "https://arxiv.org/pdf/2405.18654",
    "source": "auto-script",
    "tags": [
      "Hallucination"
    ],
    "authors": "Pritam Sarkar, Sayna Ebrahimi, Ali Etemad, Ahmad Beirami, Sercan Ö. Arık, Tomas Pfister",
    "description": "本文介绍了 DPA（Data-augmented Phrase-level Alignment），这是一种旨在缓解多模态大语言模型（MLLMs）中物体幻觉问题的创新损失函数。针对模型生成输入图像中并不存在的物体信息这一挑战，DPA 可直接应用于经过指令微调的现成 MLLM，在减少幻觉的同时保留其通用的视觉语言能力。在微调过程中，研究团队首先通过生成式数据增强手段，在短语级别选择性地修改正确响应中的事实信息，从而构建出一系列“幻觉-正确”响应对；随后，利用 DPA 损失函数训练模型，使其在生成时相比于幻觉短语更倾向于选择正确的短语。在多个基准测试上的深入评估验证了该方法的有效性，经 DPA 微调后的模型（被称为 HALVA）在幻觉视觉问答任务上的 F1 分数最高提升了 13.4%，并在图像描述任务中将幻觉率降低了多达 4.2%，且能保持模型在通用任务上的即插即用性能。",
    "image": "",
    "code": "https://github.com/pritamqu/HALVA"
  },
  {
    "title": "VDGD: Mitigating LVLM Hallucinations in Cognitive Prompts by Bridging the Visual Perception Gap",
    "venue": "arXiv",
    "date": "2024",
    "link": "https://arxiv.org/pdf/2405.15683",
    "source": "auto-script",
    "tags": [
      "Hallucination"
    ],
    "authors": "Sreyan Ghosh, Chandra Kiran Reddy Evuru, Sonal Kumar, Utkarsh Tyagi, Oriol Nieto, et al.",
    "description": "本文研究了大型视觉语言模型（LVLMs）产生幻觉的根本原因，指出虽然现有技术能有效减少视觉识别类指令（即简单的视觉元素描述）中的幻觉，但在需要深层推理的认知类指令上表现不佳。研究发现，LVLMs 幻觉的核心症结在于缺乏真正的视觉感知：尽管模型能准确识别视觉元素，却难以在输入指令的上下文中充分解读这些元素，并将其与内部知识有效关联以支持推理。为填补这一空白，本文提出了一种简单、鲁棒且无需训练的方法——基于视觉描述锚定的解码（VDGD）。该方法首先生成图像的详细描述并将其作为前缀添加到指令中，在生成响应时，通过计算备选标记与描述之间的 KL 散度进行采样，优先选择散度较低的候选标记，从而强化视觉感知并提升推理能力。在多个视觉推理基准和模型上的实验表明，VDGD 的性能始终优于现有基准方法 2% 至 33%。最后，本文还引入了 VaLLu 基准，用于全面评估 LVLMs 的认知能力。",
    "image": "",
    "code": "https://sreyan88.github.io/VDGD/"
  },
  {
    "title": "Detecting and Mitigating Hallucination in Large Vision Language Models via Fine-Grained AI Feedback",
    "venue": "arXiv",
    "date": "2024",
    "link": "https://arxiv.org/pdf/2404.14233.pdf",
    "source": "auto-script",
    "tags": [
      "Hallucination"
    ],
    "authors": "Wenyi Xiao, Ziwei Huang, Leilei Gan, Wanggui He, Haoyuan Li, et al.",
    "description": "本文介绍了一种通过细粒度人工智能反馈来检测和缓解大型视觉语言模型（LVLMs）幻觉的新方法。针对以往研究多集中于粗粒度检测或依赖高成本标注的问题，该方法首先利用商业闭源模型生成小规模的句子级幻觉标注数据集，并据此训练出一个能够识别物体、属性和关系等核心幻觉类型的细粒度检测模型。随后，研究团队提出了一种“检测再改写”的流水线，自动构建用于训练幻觉缓解模型的偏好数据集。此外，本文还提出了一种幻觉严重程度感知直接偏好优化（HSA-DPO）方法，通过将幻觉的严重程度纳入偏好学习过程，进一步增强模型对幻觉的抑制能力。广泛的实验结果证明了该方法在提升 LVLMs 跨模态对齐性方面的有效性。",
    "image": "",
    "code": "https://github.com/Mr-Loevan/HSA-DPO"
  },
  {
    "title": "Mitigating Hallucinations in Large Vision-Language Models with Instruction Contrastive Decoding",
    "venue": "arXiv",
    "date": "2024",
    "link": "https://arxiv.org/pdf/2403.18715.pdf",
    "source": "auto-script",
    "tags": [
      "Hallucination"
    ],
    "authors": "Xintong Wang, Jingheng Pan, Liang Ding, Chris Biemann",
    "description": "本文介绍了指令对比解码（ICD）方法，这是一种旨在减少大型视觉语言模型（LVLMs）推理过程中幻觉现象的新型方案。研究人员观察到，特定的“干扰指令”会显著加剧多模态融合模块中的幻觉问题，受此启发，ICD 通过对比标准指令与干扰指令下的输出分布，增加了对齐不确定性的权重，从而有效地从原始分布中减去幻觉概念。在判别性基准测试（POPE 和 MME）以及生成性基准测试（LLaVa-Bench）上的全面实验表明，ICD 不仅能显著缓解物体级和属性级的幻觉，还能大幅增强 LVLMs 的通用感知与识别能力。该方法为解决多模态决策和开放式生成中的文本与视觉内容不一致问题提供了一种有效的推理策略。",
    "image": "",
    "code": ""
  },
  {
    "title": "What if...?: Counterfactual Inception to Mitigate Hallucination Effects in Large Multimodal Models",
    "venue": "arXiv",
    "date": "2024",
    "link": "https://arxiv.org/pdf/2403.13513.pdf",
    "source": "auto-script",
    "tags": [
      "Hallucination"
    ],
    "authors": "Junho Kim, Yeon Ju Kim, Yong Man Ro",
    "description": "本文介绍了反事实启发（Counterfactual Inception），这是一种在无需额外训练的情况下，通过植入自我生成的反事实关键词来增强大型多模态模型（LMMs）可靠性并缓解跨模态不一致幻觉的新方法。该方法借鉴了人类认知中的反事实思维过程，即通过思考“替代现实”来促进更广泛的语境探索，从而使模型能够结合更丰富的背景场景理解来生成响应，减少幻觉输出。为了确保反事实思维的稳定触发，研究者还引入了合理性验证过程（PVP），这是一种简单且鲁棒的关键词约束机制，能够有效过滤掉次优关键词。针对开源及闭源等多种 LMMs 的全面分析证实，反事实思维显著降低了幻觉率，并有助于模型基于真实的视觉线索拓宽语境理解能力。",
    "image": "",
    "code": "https://ivy-lvlm.github.io/Counterfactual-Inception/"
  },
  {
    "title": "Strengthening Multimodal Large Language Model with Bootstrapped Preference Optimization",
    "venue": "arXiv",
    "date": "2024",
    "link": "https://arxiv.org/pdf/2403.08730.pdf",
    "source": "auto-script",
    "tags": [
      "Hallucination"
    ],
    "authors": "Renjie Pi, Tianyang Han, Wei Xiong, Jipeng Zhang, Runtao Liu, et al.",
    "description": "本文介绍了自举偏好优化（BPO），这是一种旨在缓解多模态大语言模型（MLLMs）中因过度偏向预训练语料而导致视觉信息被忽视问题的偏好学习方法。研究者将这种偏差视为模型对预训练统计数据的“偏好”，并利用模型自身自举生成的负面响应构建偏好数据集。具体而言，该方法包含两种策略：一是通过向 MLLM 输入失真图像，诱导其产生带有显著预训练偏差的响应；二是利用纯文本 LLM 有意在原始响应中注入错误但常见的元素。这些低质量响应与数据集中的原始标注响应配对后，用于进行偏好学习。实验结果表明，BPO 能有效抑制预训练 LLM 的固有偏差，显著增强模型对视觉输入的定位能力，在多个基准测试中提升了多模态对话系统的性能。",
    "image": "",
    "code": ""
  },
  {
    "title": "Debiasing Multimodal Large Language Models",
    "venue": "arXiv",
    "date": "2024",
    "link": "https://arxiv.org/pdf/2403.05262",
    "source": "auto-script",
    "tags": [
      "Hallucination"
    ],
    "authors": "YiFan Zhang, Yang Shi, Weichen Yu, Qingsong Wen, Xue Wang, et al.",
    "description": "本文针对多模态大语言模型（MLLMs）在生成内容时过度依赖底层语言模型（LLMs）的固有先验，而非输入图像这一显著偏向问题进行了研究。实验表明，MLLMs 即使在缺乏相关图像或输入不一致视觉信息的情况下仍能给出自信的回答。为纠正此类偏差并将模型注意力重新引向视觉信息，研究者提出了两种简单且无需训练的策略：首先，针对分类或多选题任务，引入“事后去偏”方法，通过仿射校准步骤调整输出分布，确保在图像缺失时答案分值均匀，从而有效抑制 LLM 先验的影响；其次，针对复杂的开放式生成任务，将该方法扩展为“视觉去偏解码”，通过对比基于正确图像和无意义图像的 Token 对数概率来减轻偏差。此外，研究还揭示了 MLLMs 在不同解码配置下的不稳定性，通过系统性探索优化设置，实现了超越以往研究的性能提升，并对当前评估实践的公平性提出了质疑。广泛的实验证实，这些策略在减少幻觉、提升回复精准度和参考价值方面具有显著成效。",
    "image": "",
    "code": ""
  },
  {
    "title": "HALC: Object Hallucination Reduction via Adaptive Focal-Contrast Decoding",
    "venue": "arXiv",
    "date": "2024",
    "link": "https://arxiv.org/pdf/2403.00425.pdf",
    "source": "auto-script",
    "tags": [
      "Hallucination"
    ],
    "authors": "Zhaorun Chen, Zhuokai Zhao, Hongyin Luo, Huaxiu Yao, Bo Li, Jiawei Zhou",
    "description": "本文介绍了 HALC，这是一种旨在缓解大型视觉语言模型（LVLMs）中物体幻觉（OH）问题的新型解码算法。HALC 利用视觉语言任务中独特的细粒度最优视觉信息，能够同时在局部和全局上下文中运行。具体而言，该算法在局部层面集成了一种鲁棒的自动聚焦定位机制，用于即时纠正幻觉标记（tokens）；在全局层面则采用专门的束搜索（beam search）算法，在显著减少物体幻觉的同时保持文本生成的质量。此外，HALC 作为一个即插即用模块，无需额外训练即可集成到任何 LVLMs 中。广泛的实验研究表明，HALC 在减少物体幻觉方面表现出色，在四个基准测试中的性能均优于现有最先进的技术。",
    "image": "",
    "code": "https://github.com/BillChan226/HALC"
  },
  {
    "title": "IBD: Alleviating Hallucinations in Large Vision-Language Models via Image-Biased Decoding",
    "venue": "arXiv",
    "date": "2024",
    "link": "https://arxiv.org/pdf/2402.18476.pdf",
    "source": "auto-script",
    "tags": [
      "Hallucination"
    ],
    "authors": "Lanyun Zhu, Deyi Ji, Tianrun Chen, Peng Xu, Jieping Ye, Jun Liu",
    "description": "本文介绍了一种名为图像偏置解码（IBD）的新技术，旨在缓解大型视觉语言模型（LVLMs）因过度依赖语言先验而导致的幻觉问题。该方法的核心是通过对比传统 LVLM 的预测结果与图像偏置 LVLM 的预测结果来推导下一个标记的概率分布，从而放大与图像内容高度相关的正确信息，并抑制由过度依赖文本引起的幻觉错误。研究团队还进行了全面的统计分析以验证该方法的可靠性，并设计了一种自适应调整策略，以实现在不同条件下的稳健和灵活处理。在多个评估指标上的实验结果证实，该方法在无需额外训练数据且仅极少量增加模型参数的情况下，能显著减少 LVLMs 的幻觉现象，并提升生成响应的真实性。",
    "image": "",
    "code": ""
  },
  {
    "title": "Less is More: Mitigating Multimodal Hallucination from an EOS Decision Perspective",
    "venue": "arXiv",
    "date": "2024",
    "link": "https://arxiv.org/pdf/2402.14545.pdf",
    "source": "auto-script",
    "tags": [
      "Hallucination"
    ],
    "authors": "Zihao Yue, Liang Zhang, Qin Jin",
    "description": "本文探讨了大型多模态模型（LMMs）中多模态幻觉的一个新视角：过度详细的训练数据会阻碍模型及时终止生成的能力，导致其输出超出视觉感知极限。通过研究模型如何利用特殊终止符（EOS）决定停止生成，作者发现模型会通过对比已生成文本与图像来评估整个序列的完整性，这表明模型具有基于视觉感知做出合理 EOS 决策以避免冗长输出的内在潜力。为了利用这一潜力，研究者提出了两种缓解幻觉的方法：一种是通过学习常规指令数据来降低幻觉的训练目标，另一种是防止有害训练数据加剧模型幻觉的数据过滤策略。实验证明，这两种方法在不需要任何额外数据或知识的前提下，均显著提升了 LMMs 在幻觉评估中的表现。",
    "image": "",
    "code": "https://github.com/yuezih/less-is-more"
  },
  {
    "title": "Logical Closed Loop: Uncovering Object Hallucinations in Large Vision-Language Models",
    "venue": "arXiv",
    "date": "2024",
    "link": "https://arxiv.org/pdf/2402.11622.pdf",
    "source": "auto-script",
    "tags": [
      "Hallucination"
    ],
    "authors": "Junfei Wu, Qiang Liu, Ding Wang, Jinghao Zhang, Shu Wu, et al.",
    "description": "本文介绍了一种名为 LogicCheckGPT 的逻辑闭环框架，旨在缓解大型视觉语言模型（LVLMs）中长期存在的物体幻觉问题。针对现有方法依赖大规模计算资源或外部检测模型的问题，该研究探索了如何利用模型自身的逻辑一致性来识别幻觉。其核心直觉在于：LVLM 对图像中真实存在的物体往往能做出逻辑一致的回答，而对幻觉物体则容易表现出逻辑矛盾。具体而言，LogicCheckGPT 通过设计逻辑一致性探针，提出具有逻辑关联的问题（如通过物体询问属性，或通过属性反推物体），并根据这些回答能否形成逻辑闭环来判定是否存在物体幻觉。作为一种即插即用的方法，它可以无缝应用于所有现有的 LVLM。在三个基准测试和四个主流 LVLM 上进行的全面实验表明，该方法在检测和缓解物体幻觉方面取得了显著改进，充分证明了其有效性与通用性。",
    "image": "",
    "code": "https://github.com/CRIPAC-DIG/LogicCheckGPT"
  },
  {
    "title": "The Instinctive Bias: Spurious Images lead to Hallucination in MLLMs",
    "venue": "arXiv",
    "date": "2024",
    "link": "https://arxiv.org/pdf/2402.03757.pdf",
    "source": "auto-script",
    "tags": [
      "Hallucination"
    ],
    "authors": "Tianyang Han, Qing Lian, Rui Pan, Renjie Pi, Jipeng Zhang, et al.",
    "description": "本文介绍了 CorrelationQA，这是首个专门用于评估多模态大语言模型（MLLMs）在面对误导性图像时的视觉幻觉（visual illusion）水平的基准测试。尽管以 GPT-4V 为代表的 MLLMs 在多模态任务中表现出色，但研究发现，当输入图像与正确答案高度相关却在事实上不一致时，这些模型极易受到干扰并产生错误判断。为了量化这一影响，CorrelationQA 收集了涵盖 13 个类别的 7,308 个文本-图像对。通过在该基准上对 9 种主流 MLLMs 进行深入分析，结果表明这些模型普遍存在这种本能偏差，且程度各异。该研究旨在通过精心构建的基准和评估结果，为衡量 MLLMs 在虚假或误导性图像存在时的稳健性提供更有效的手段。",
    "image": "",
    "code": "https://github.com/MasaiahHan/CorrelationQA"
  },
  {
    "title": "Unified Hallucination Detection for Multimodal Large Language Models",
    "venue": "arXiv",
    "date": "2024",
    "link": "https://arxiv.org/pdf/2402.03190.pdf",
    "source": "auto-script",
    "tags": [
      "Hallucination"
    ],
    "authors": "Xiang Chen, Chenxi Wang, Yida Xue, Ningyu Zhang, Xiaoyan Yang, et al.",
    "description": "本文针对多模态大语言模型（MLLMs）中普遍存在的幻觉问题，提出了一个旨在提升幻觉检测可靠性的研究方案。针对以往研究仅侧重于单一任务、幻觉类别覆盖不足以及缺乏细粒度分析等局限性，本研究扩展了幻觉检测的调查视野。首先，研究团队构建了一个全新的元评估基准 MHaluBench，专门用于评估和推动幻觉检测方法的进步。此外，本文还推出了一种名为 UNIHD 的统一多模态幻觉检测框架，该框架利用一系列辅助工具对幻觉的发生进行稳健验证。通过详尽的评估和全面分析，实验证明了 UNIHD 的有效性，并针对如何应用特定工具来应对不同类别的幻觉提供了战略性见解。",
    "image": "",
    "code": ""
  },
  {
    "title": "A Survey on Hallucination in Large Vision-Language Models",
    "venue": "arXiv",
    "date": "2024",
    "link": "https://arxiv.org/pdf/2402.00253.pdf",
    "source": "auto-script",
    "tags": [
      "Hallucination"
    ],
    "authors": "Hanchao Liu, Wenyuan Xue, Yifei Chen, Dapeng Chen, Xiutian Zhao, et al.",
    "description": "本文对大型视觉语言模型（LVLMs）中的“幻觉”问题（即事实视觉内容与生成文本之间不一致的现象）进行了全面的综述，旨在构建研究概览并推动未来的缓解方案。综述首先明确了 LVLM 幻觉的概念，展示了多种幻觉症状，并强调了该领域特有的挑战；随后，详细介绍了专门为评估 LVLM 幻觉设计的基准测试和方法论。此外，本文还从训练数据和模型组件等维度深入探讨了幻觉产生的根源，并对现有的缓解方法进行了评述。最后，文章总结并讨论了 LVLM 幻觉领域尚待解决的问题以及未来的研究方向，为利用和优化 LVLM 提供了重要参考。",
    "image": "",
    "code": "https://github.com/lhanchao777/LVLM-Hallucinations-Survey"
  },
  {
    "title": "Temporal Insight Enhancement: Mitigating Temporal Hallucination in Multimodal Large Language Models",
    "venue": "arXiv",
    "date": "2024",
    "link": "https://arxiv.org/pdf/2401.09861.pdf",
    "source": "auto-script",
    "tags": [
      "Hallucination"
    ],
    "authors": "Li Sun, Liuan Wang, Jun Sun, Takayuki Okatani",
    "description": "本文介绍了一种旨在解决多模态大语言模型（MLLMs）中事件级幻觉（特别是视频内容的时间理解偏差）的创新框架。针对模型在处理视频输入时容易产生错误感知或事件误判的挑战，该研究提出了一种利用事件查询（Event Query）和视频信息来精炼模型响应的新方法。其核心机制是将复杂的按需事件查询分解为一系列“图标动作”（Iconic Actions），并随后利用 CLIP 和 BLIP2 等模型来预测这些动作发生的具体时间戳。在 Charades-STA 数据集上的评估结果表明，该方法显著减少了时间维度的幻觉，并提升了事件相关响应的质量。这项研究不仅为克服 MLLMs 在视频感知上的关键缺陷提供了新视角，还为量化评估模型在处理时间相关问题时的表现贡献了可衡量的基准方法。",
    "image": "",
    "code": ""
  },
  {
    "title": "Hallucination Augmented Contrastive Learning for Multimodal Large Language Model",
    "venue": "arXiv",
    "date": "2023",
    "link": "https://arxiv.org/pdf/2312.06968.pdf",
    "source": "auto-script",
    "tags": [
      "Hallucination"
    ],
    "authors": "Chaoya Jiang, Haiyang Xu, Mengfan Dong, Jiaxing Chen, Wei Ye, et al.",
    "description": "本文从表示学习的新视角探讨了多模态大语言模型（MLLMs）中的幻觉问题。研究人员通过分析模型中文本和视觉标记的表示分布，发现了两个关键现象：一是文本与视觉表示之间存在显著鸿沟，表明跨模态对齐尚不理想；二是包含幻觉与不含幻觉的文本表示处于纠缠状态，导致模型难以区分。基于这些发现，本文提出了一种简单而有效的缓解方法，即将对比学习引入 MLLMs，并将带有幻觉的文本作为硬负样本。该方法能够使非幻觉文本与视觉样本的表示在空间上更加接近，同时将非幻觉文本与幻觉文本的表示相互推离。定量与定性评估结果显示，该方法在减少幻觉发生和提升模型性能方面极具成效；在 MMhal-Bench 基准测试中，该方法相比基准模型 MiniGPT-4 和 LLaVA 分别实现了 34.66% 和 29.5% 的性能提升。",
    "image": "",
    "code": "https://github.com/X-PLUG/mPLUG-HalOwl/tree/main/hacl"
  },
  {
    "title": "MOCHa: Multi-Objective Reinforcement Mitigating Caption Hallucinations",
    "venue": "arXiv",
    "date": "2023",
    "link": "https://arxiv.org/pdf/2312.03631.pdf",
    "source": "auto-script",
    "tags": [
      "Hallucination"
    ],
    "authors": "Assaf Ben-Kish, Moran Yanuka, Morris Alper, Raja Giryes, Hadar Averbuch-Elor",
    "description": "本文针对图像描述生成中存在的幻觉问题，即生成无法从图像中推断出的虚假细节，提出了一套在开放词汇设置下的解决方案。针对现有方法大多依赖闭源词汇列表而忽略实际应用中长尾幻觉现象的局限性，研究团队首先构建了全新的基准测试 OpenCHAIR，利用生成式基础模型评估开放词汇环境下的物体幻觉，其在多样性和准确性上均优于同规模的传统 CHAIR 基准。此外，为了在不使用固定物体列表的情况下缓解幻觉，本文提出了基于强化学习的 MOCHa 方法，通过设计一个多目标奖励函数，在无需强监督信号的前提下显式地权衡生成内容的忠实度与充分性。实验结果表明，MOCHa 能够显著提升各类图像描述模型的性能，并在 OpenCHAIR 及其他现有指标上展现出卓越的改进效果。",
    "image": "",
    "code": "https://assafbk.github.io/mocha/"
  },
  {
    "title": "Mitigating Fine-Grained Hallucination by Fine-Tuning Large Vision-Language Models with Caption Rewrites",
    "venue": "arXiv",
    "date": "2023",
    "link": "https://arxiv.org/pdf/2312.01701.pdf",
    "source": "auto-script",
    "tags": [
      "Hallucination"
    ],
    "authors": "Lei Wang, Jiabang He, Shenshen Li, Ning Liu, Ee-Peng Lim",
    "description": "本文针对大型视觉语言模型（LVLMs）中存在的细粒度物体幻觉问题进行了研究。研究指出，现有的评估方法通常仅关注粗粒度的物体幻觉（即生成图像中不存在的物体），而模型在生成过程中仍可能产生图像中并不存在的细粒度属性和行为描述，且这些内容尚未被现有方法衡量。为此，研究团队提出了一个名为 ReCaption 的框架，该框架由两个核心部分组成：一是利用 ChatGPT 对原始图像说明进行重写，二是在重写后的说明上对经过指令微调的 LVLMs 进行二次微调。此外，本文还提出了一种基于探测的细粒度评估方法，即“细粒度物体幻觉评估”（FGHE）。实验结果表明，ReCaption 框架能有效降低多种 LVLM 模型的细粒度物体幻觉，并显著提升其文本生成的整体质量。",
    "image": "",
    "code": "https://github.com/Anonymousanoy/FOHE"
  },
  {
    "title": "RLHF-V: Towards Trustworthy MLLMs via Behavior Alignment from Fine-grained Correctional Human Feedback",
    "venue": "arXiv",
    "date": "2023",
    "link": "https://arxiv.org/pdf/2312.00849.pdf",
    "source": "auto-script",
    "tags": [
      "RLHF",
      "Hallucination"
    ],
    "authors": "Tianyu Yu, Yuan Yao, Haoye Zhang, Taiwen He, Yifeng Han, et al.",
    "description": "本文介绍了 RLHF-V 框架，旨在通过细粒度的人类纠正反馈进行行为对齐，从而提升多模态大语言模型（MLLMs）的可信度。针对现有模型普遍存在的幻觉问题，RLHF-V 创新性地收集了人类对幻觉进行的片段级纠正偏好数据，并以此为基础执行密集的直接偏好优化（DPO）。在五个基准测试上的自动及人工评估结果显示，RLHF-V 能够以极高的数据和计算效率显著增强模型的可信度。值得关注的是，仅使用 1.4k 条标注数据，RLHF-V 就将基础模型的幻觉率降低了 34.8%，性能超越了使用 10k 条数据的 LLaVA-RLHF。最终模型在开源 MLLMs 中达到了最先进的可信度水平，并在防止由于过度泛化引起的幻觉方面展现出比 GPT-4V 更强的鲁棒性。",
    "image": "",
    "code": "https://rlhf-v.github.io/"
  },
  {
    "title": "OPERA: Alleviating Hallucination in Multi-Modal Large Language Models via Over-Trust Penalty and Retrospection-Allocation",
    "venue": "CVPR",
    "date": "2023",
    "link": "https://arxiv.org/pdf/2311.17911.pdf",
    "source": "auto-script",
    "tags": [
      "Hallucination"
    ],
    "authors": "Qidong Huang, Xiaoyi Dong, Pan Zhang, Bin Wang, Conghui He, et al.",
    "description": "本文介绍了 OPERA，这是一种旨在缓解多模态大语言模型（MLLMs）幻觉问题的创新解码方法。针对现有缓解方案往往需要额外训练数据或外部知识从而导致成本增加的局限性，OPERA 基于“过度信赖惩罚”和“回顾分配”策略，提供了一种无需额外数据、知识或训练的近乎“免费”的解决方案。研究团队首先发现了一个有趣的现象：大多数幻觉与自注意力矩阵中的知识聚合模式密切相关，即模型在生成新标记时倾向于过度关注少数几个“摘要标记”而非所有历史标记。这种局限的过度信赖倾向会导致模型忽视图像标记，进而产生与事实不符的幻觉内容。基于此观察，OPERA 在束搜索（beam-search）解码过程中对逻辑得分引入惩罚项以抑制过度信赖问题，并结合回退策略来回顾已生成序列中的摘要标记，在必要时重新分配标记选择。广泛的实验表明，OPERA 在不同模型和指标下均表现出显著的幻觉抑制效果，充分验证了其有效性与通用性。",
    "image": "",
    "code": "https://github.com/shikiw/OPERA"
  },
  {
    "title": "Mitigating Object Hallucinations in Large Vision-Language Models through Visual Contrastive Decoding",
    "venue": "CVPR",
    "date": "2023",
    "link": "https://arxiv.org/pdf/2311.16922.pdf",
    "source": "auto-script",
    "tags": [
      "Hallucination"
    ],
    "authors": "Sicong Leng, Hang Zhang, Guanzheng Chen, Xin Li, Shijian Lu, et al.",
    "description": "本文介绍了视觉对比解码（VCD），这是一种旨在缓解大型视觉语言模型（LVLMs）中物体幻觉问题的简单且无需训练的方法。尽管 LVLMs 在结合视觉识别与语言理解方面取得了显著进展，但仍普遍存在生成图像中并不存在的虚假物体的现象。针对导致幻觉的两个核心原因——模型对统计偏见和单模态先验的过度依赖，VCD 通过对比原始视觉输入与失真视觉输入下产生的输出分布来校准解码过程。这种调整确保了生成内容能够紧密锚定在实际的视觉输入上，从而产生符合语境的准确输出。实验结果表明，VCD 在不依赖额外训练或外部工具的前提下，能显著降低不同系列 LVLMs 的物体幻觉率。除了缓解幻觉，VCD 在通用的 LVLM 基准测试中也表现出色，展现了其广泛的适用价值。",
    "image": "",
    "code": "https://github.com/DAMO-NLP-SG/VCD"
  },
  {
    "title": "Beyond Hallucinations: Enhancing LVLMs through Hallucination-Aware Direct Preference Optimization",
    "venue": "arXiv",
    "date": "2023",
    "link": "https://arxiv.org/pdf/2311.16839.pdf",
    "source": "auto-script",
    "tags": [
      "Hallucination"
    ],
    "authors": "Zhiyuan Zhao, Bin Wang, Linke Ouyang, Xiaoyi Dong, Jiaqi Wang, Conghui He",
    "description": "本文介绍了一种名为幻觉感知直接偏好优化（HA-DPO）的创新解决方案，旨在解决多模态大语言模型中常见的“幻觉问题”，即模型生成的文本描述与图像内容不符或凭空捏造。该研究将幻觉问题重新定义为一个偏好选择任务，通过为同一张图像提供准确和幻觉两类响应，训练模型倾向于选择非幻觉的回复。此外，本文还提出了一套高效的流水线，用于构建高质量且风格一致的正负样本对，从而确保鲁棒的偏好学习。实验结果表明，将 HA-DPO 应用于三种主流多模态模型后，不仅显著缓解了幻觉问题，还增强了模型的泛化能力。特别是在 MiniGPT-4 模型上，HA-DPO 使其 POPE 准确率从 51.13% 提升至 86.13%（绝对提升 35%），MME 评分也从 932.00 飙升至 1326.46（相对提升 42.32%）。",
    "image": "",
    "code": "https://github.com/opendatalab/HA-DPO"
  },
  {
    "title": "Mitigating Hallucination in Visual Language Models with Visual Supervision",
    "venue": "arXiv",
    "date": "2023",
    "link": "https://arxiv.org/pdf/2311.16479.pdf",
    "source": "auto-script",
    "tags": [
      "Hallucination"
    ],
    "authors": "Zhiyang Chen, Yousong Zhu, Yufei Zhan, Zhaowen Li, Chaoyang Zhao, et al.",
    "description": "本文针对大型视觉语言模型（LVLMs）中频繁出现的幻觉问题进行了研究，指出模型偶尔生成与图像内容相矛盾响应的核心原因在于其对多模态语境下细节内容的理解力较弱，这主要归结为训练数据侧重全局描述以及自回归损失函数更偏向文本建模而非图像理解。为了解决这一问题，研究团队引入了更详尽的视觉标注和更具辨别力的视觉模型来辅助训练：一方面，利用全景场景图数据集（PSG）生成带有详细关系标注的图文对，通过关注图像中的细节事实，引导模型根据多模态语境回答问题；另一方面，集成 SAM（Segment Anything Model）和掩码预测损失作为辅助监督手段，强制模型具备识别语境相关物体的能力，从而生成更准确的响应并减轻幻觉。此外，为深入评估幻觉问题，本文提出了全新的基准测试 RAH-Bench，将视觉幻觉细分为类别错误、属性错误和关系错误三种类型，并引入假阳性率（FPR）作为细分指标。在该基准测试中，该方法相比原始 LLaVA 实现了 8.4% 的性能提升，并在其他模型上展现了广泛的改进效果。",
    "image": "",
    "code": ""
  },
  {
    "title": "HalluciDoctor: Mitigating Hallucinatory Toxicity in Visual Instruction Data",
    "venue": "arXiv",
    "date": "2023",
    "link": "https://arxiv.org/pdf/2311.13614.pdf",
    "source": "auto-script",
    "tags": [
      "Hallucination"
    ],
    "authors": "Qifan Yu, Juncheng Li, Longhui Wei, Liang Pang, Wentao Ye, et al.",
    "description": "本文针对利用机器生成的指令遵循数据微调多模态大语言模型（MLLMs）时，数据中潜藏的幻觉问题及其对模型输出的负面影响进行了深入研究。为了识别并消除大规模机器生成视觉指令数据集中的物体、关系及属性幻觉，研究团队借鉴人类辨别事实错误的能力，提出了一种基于交叉验证范式的新型幻觉检测与消除框架——HalluciDoctor。该框架能够自动识别并清除训练数据中的虚假信息。有趣的是，HalluciDoctor 的分析还揭示了长尾物体共现所产生的伪相关性是导致幻觉的重要诱因。基于这一发现，研究者执行了反事实视觉指令扩展，通过平衡数据分布来增强 MLLMs 对幻觉的抵抗力。在多个幻觉评估基准上的综合实验表明，该方法成功相对减少了 44.6% 的幻觉，并使模型在与 LLaVA 的对比中保持了极具竞争力的性能表现。",
    "image": "",
    "code": "https://github.com/Yuqifan1117/HalluciDoctor"
  },
  {
    "title": "An LLM-free Multi-dimensional Benchmark for MLLMs Hallucination Evaluation",
    "venue": "arXiv",
    "date": "2023",
    "link": "https://arxiv.org/pdf/2311.07397.pdf",
    "source": "auto-script",
    "tags": [
      "Hallucination"
    ],
    "authors": "Junyang Wang, Yuhang Wang, Guohai Xu, Jing Zhang, Yukai Gu, et al.",
    "description": "本文针对当前多模态大语言模型（MLLMs）在取得进展的同时面临严重的幻觉挑战这一现状，提出了一个名为 AMBER 的多维度评估基准，旨在解决以往研究中评估成本高（如依赖人工或高级 LLMs）以及评估维度不足（如任务类型和幻觉类别有限）的问题。AMBER 是一个无需大型语言模型（LLM-free）参与的评测工具，能够同时评估生成式和判别式任务，涵盖了物体存在、属性以及关系等多个幻觉维度。基于 AMBER，研究团队设计了一套低成本且高效的评估流程，并对包括 GPT-4V 在内的主流多模态模型进行了全面测试与深入分析，同时为缓解模型幻觉提供了具有指导意义的建议。",
    "image": "",
    "code": "https://github.com/junyangwang0410/AMBER"
  },
  {
    "title": "FAITHSCORE: Evaluating Hallucinations in Large Vision-Language Models",
    "venue": "arXiv",
    "date": "2023",
    "link": "https://arxiv.org/pdf/2311.01477.pdf",
    "source": "auto-script",
    "tags": [
      "Hallucination"
    ],
    "authors": "Liqiang Jing, Ruosen Li, Yunmo Chen, Xinya Du",
    "description": "本文介绍了 FaithScore（图像原子事实忠实度评分），这是一种无需参考答案且具有细粒度的评估指标，专门用于衡量大型视觉语言模型（LVLMs）生成的自由回答的忠实度。FaithScore 的评估流程首先识别出生成文本中包含描述性陈述并需要验证的子句，接着从这些子句中提取完整的原子事实列表，最后对这些细粒度的原子事实与输入图像进行一致性验证。元评估结果显示，该指标与人类对忠实度的判断具有高度相关性。研究团队收集了两个基准数据集（LLaVA-1k 和 MSCOCO-Cap）用于评估 LVLMs 在指令遵循中的幻觉，并利用 FaithScore 对当前最先进的系统进行了测试。评估结果揭示，现有系统极易生成与图像不符的幻觉内容，这为未来的改进预留了空间。研究者希望 FaithScore 指标能够助力未来 LVLMs 忠实度的评估，并为增强模型可信度提供参考建议。",
    "image": "",
    "code": "https://github.com/bcdnlp/FAITHSCORE"
  },
  {
    "title": "Woodpecker: Hallucination Correction for Multimodal Large Language Models",
    "venue": "arXiv",
    "date": "2023",
    "link": "https://arxiv.org/pdf/2310.16045.pdf",
    "source": "auto-script",
    "tags": [
      "Visual Reasoning",
      "Hallucination"
    ],
    "authors": "Shukang Yin, Chaoyou Fu, Sirui Zhao, Tong Xu, Hao Wang, et al.",
    "description": "本文介绍了 Woodpecker，这是一种旨在缓解多模态大语言模型（MLLMs）中幻觉问题的创新型无需训练的方法。针对现有研究主要依赖特定数据重训模型的局限性，Woodpecker 提出了一种不同的路径，其核心逻辑如同啄木鸟医治树木一般，从生成的文本中识别并修正幻觉内容。具体而言，该方法包含五个阶段：关键概念提取、问题构建、视觉知识验证、视觉声明生成以及幻觉修正。作为一种事后补救机制，Woodpecker 可以轻松应用于不同的 MLLMs，并通过访问五个阶段的中间输出来提供良好的可解释性。通过定量和定性评估，该方法展现出了巨大的应用潜力，在 POPE 基准测试中，Woodpecker 相比基准模型 MiniGPT-4 和 mPLUG-Owl 分别实现了 30.66% 和 24.33% 的准确率提升。",
    "image": "",
    "code": "https://github.com/VITA-MLLM/Woodpecker"
  },
  {
    "title": "Negative Object Presence Evaluation (NOPE) to Measure Object Hallucination in Vision-Language Models",
    "venue": "arXiv",
    "date": "2023",
    "link": "https://arxiv.org/pdf/2310.05338.pdf",
    "source": "auto-script",
    "tags": [
      "Hallucination"
    ],
    "authors": "Holy Lovenia, Wenliang Dai, Samuel Cahyawijaya, Ziwei Ji, Pascale Fung",
    "description": "本文针对视觉语言（VL）模型中存在的物体幻觉挑战——即模型生成包含不存在物体的虚假或不实响应——提出了一种名为 NOPE（负面物体存在评估）的新型基准。针对目前缺乏通用衡量标准来评估和缓解这一问题现状，研究团队开发了一种极具成本效益且可扩展的方法，利用大语言模型为 NOPE 生成了 29.5k 条高质量的合成负面代词（NegP）数据。通过该基准，研究者深入调查了 10 种最先进的 VL 模型在处理真实答案为“无”等负面表述的视觉问题时的辨别能力，并在其他 9 个视觉问答数据集上评估了它们的标准性能。实验结果表明，没有任何一个 VL 模型能完全免疫物体幻觉的威胁，所有模型在 NegP 数据上的准确率均低于 10%。此外，研究还揭示了词汇多样性高的视觉问题、大范围的问题类型以及与场景高度相关的物体更容易诱发 VL 模型的物体幻觉风险。",
    "image": "",
    "code": ""
  },
  {
    "title": "HallE-Switch: Rethinking and Controlling Object Existence Hallucinations in Large Vision Language Models for Detailed Caption",
    "venue": "arXiv",
    "date": "2023",
    "link": "https://arxiv.org/pdf/2310.01779.pdf",
    "source": "auto-script",
    "tags": [
      "Hallucination"
    ],
    "authors": "Bohan Zhai, Shijia Yang, Chenfeng Xu, Sheng Shen, Kurt Keutzer, et al.",
    "description": "本文针对大型多模态模型（LMMs）在进行详细图像描述时能否准确捕捉视觉细节的不确定性，提出了一种名为 CCEval 的 GPT-4 辅助评估方法。尽管 LMMs 在现有 VQA 基准中表现出的物体存在幻觉极少，但该评估方法揭示了模型在详细描述任务中仍极易受到此类幻觉的影响。研究通过从图像分辨率、语言解码器大小以及指令数据的数量、质量和粒度等多个维度进行调查发现，当语言描述的物体粒度细于视觉模块所能定位或验证的范围时，模型会产生不合理的推断，从而诱发幻觉。为控制此类幻觉，研究者将图像描述的可靠性归因于上下文知识（仅涉及语境关联的物体）和参数知识（包含模型推断的物体），并据此引入了 HallE-Control，这是一种在物体存在幻觉方面具备可控性的 LMM。该模型可以调节图像描述的倾向，使其在仅描绘语境关联的定位物体，与结合参数知识来想象推断物体之间进行切换。实验结果显示，该方法在保持物体覆盖率的同时，相比 LLaVA-7B 减少了 44% 的幻觉。",
    "image": "",
    "code": "https://github.com/bronyayang/HallE_Control"
  },
  {
    "title": "Analyzing and Mitigating Object Hallucination in Large Vision-Language Models",
    "venue": "ICLR",
    "date": "2023",
    "link": "https://arxiv.org/pdf/2310.00754.pdf",
    "source": "auto-script",
    "tags": [
      "Hallucination"
    ],
    "authors": "Yiyang Zhou, Chenhang Cui, Jaehong Yoon, Linjun Zhang, Zhun Deng, et al.",
    "description": "本文介绍了一种名为 LURE（大型视觉语言模型幻觉修正器）的算法，旨在事后纠正大型视觉语言模型（LVLMs）中普遍存在的物体幻觉问题，即模型生成的描述中包含图像中并不实际存在的物体。物体幻觉会严重影响视觉摘要和推理等任务的性能，而 LURE 通过重构幻觉程度较低的描述来解决这一挑战。该算法基于对引起物体幻觉的关键因素进行的严格统计分析，这些因素包括共现（某些物体在图像中频繁与其他物体共同出现）、不确定性（解码过程中具有较高不确定性的物体）以及物体位置（幻觉通常出现在生成文本的后期）。LURE 具有强大的通用性，可以与任何 LVLM 无缝集成。在对六种开源 LVLM 进行的评估中，LURE 在通用物体幻觉评价指标上比以往的最佳方法提升了 23%，并且在 GPT 评估和人工评估中均始终名列前茅。",
    "image": "",
    "code": "https://github.com/YiyangZhou/LURE"
  },
  {
    "title": "Aligning Large Multimodal Models with Factually Augmented RLHF",
    "venue": "arXiv",
    "date": "2023",
    "link": "https://arxiv.org/pdf/2309.14525.pdf",
    "source": "auto-script",
    "tags": [
      "RLHF",
      "Hallucination"
    ],
    "authors": "Zhiqing Sun, Sheng Shen, Shengcao Cao, Haotian Liu, Chunyuan Li, et al.",
    "description": "本文针对大型多模态模型（LMM）中因跨模态对齐不良而导致的“幻觉”问题（即生成脱离上下文多模态信息的文本输出），提出了一种将人类反馈强化学习（RLHF）从文本领域迁移至视觉-语言对齐任务的方法，通过让标注员对比并指出更具幻觉倾向的响应，训练模型最大化模拟的人类奖励。为了解决 RLHF 中的奖励破解（reward hacking）现象并进一步提升性能，研究者提出了一种名为“事实增强 RLHF”的新算法，利用图像描述和真实多选选项等额外事实信息来增强奖励模型。同时，该研究通过结合现有的手工编写图文对来增强由 GPT-4 生成的视觉指令微调训练数据，从而提升模型的通用能力。为了在真实场景下评估该方法，研究团队开发了专门针对惩罚幻觉的新基准测试 MMHAL-BENCH。作为首个通过 RLHF 训练的大型多模态模型，该方法在 LLaVA-Bench 数据集上达到了纯文本 GPT-4 性能水平的 94%（以往最佳方法仅为 87%），并在 MMHAL-BENCH 上相比其他基准模型实现了 60% 的性能提升。",
    "image": "",
    "code": "https://llava-rlhf.github.io/"
  },
  {
    "title": "Evaluation and Mitigation of Agnosia in Multimodal Large Language Models",
    "venue": "arXiv",
    "date": "2023",
    "link": "https://arxiv.org/pdf/2309.04041.pdf",
    "source": "auto-script",
    "tags": [
      "Hallucination"
    ],
    "authors": "Jiaying Lu, Jinmeng Rao, Kezhen Chen, Xiaoyuan Guo, Yawen Zhang, et al.",
    "description": "本文针对大型视觉语言模型（LVLMs）在安全性、鲁棒性和可靠性方面面临的语义接地（semantic grounding）能力受限问题进行了研究，即模型将语言与图像中物理世界实体或概念进行关联的能力。尽管这一能力对真实场景应用至关重要，但目前仍缺乏足够的调查。为填补这一空白，研究团队设计了一套流水线，用于生成涵盖颜色、数量、材质等细粒度语义信息的大规模评估数据集，并对七种主流 LVLMs 的语义接地能力进行了全面评估。结果显示，这些模型在不同维度和程度上普遍存在接地错误（misgrounding）现象。针对该问题，本文提出了一种以数据为中心的增强方法，通过对细粒度对话进行多模态指令微调，旨在提升 LVLMs 的语义接地能力。在增强后的 LVLMs 上进行的实验表明，该方法在解决接地错误问题上取得了显著改进。",
    "image": "",
    "code": ""
  },
  {
    "title": "CIEM: Contrastive Instruction Evaluation Method for Better Instruction Tuning",
    "venue": "arXiv",
    "date": "2023",
    "link": "https://arxiv.org/pdf/2309.02301.pdf",
    "source": "auto-script",
    "tags": [
      "Hallucination"
    ],
    "authors": "Hongyu Hu, Jiyuan Zhang, Minyi Zhao, Zhenbang Sun",
    "description": "本文针对当前大型视觉语言模型（LVLMs）因对视觉和语言模态理解不足，在下游应用中（如描述不存在的实体）产生幻觉这一缺陷进行了研究。为了应对幻觉现象，研究团队提出了一套名为对比指令评估方法（CIEM）的自动化流程，该方法利用标注的图文数据集结合大语言模型，生成事实性与对比性的问答对，用于评估视觉语言模型的幻觉水平。在此基础上，本文进一步提出了一种名为对比指令微调（CIT）的新型微调方法，通过自动生成高质量的事实/对比问答对及相应的论证理由来训练模型，从而缓解其幻觉问题。通过在 CIEM 和 CIT 上进行的广泛实验，本研究揭示了现有视觉语言模型中普遍存在的幻觉问题，指出当前指令微调数据集在处理幻觉现象方面的不足，并证实了经 CIT 微调后的模型在 CIEM 评估和公共数据集上的表现均优于现有方法。",
    "image": "",
    "code": ""
  },
  {
    "title": "Evaluation and Analysis of Hallucination in Large Vision-Language Models",
    "venue": "arXiv",
    "date": "2023",
    "link": "https://arxiv.org/pdf/2308.15126.pdf",
    "source": "auto-script",
    "tags": [
      "Hallucination"
    ],
    "authors": "Junyang Wang, Yiyang Zhou, Guohai Xu, Pengcheng Shi, Chenlin Zhao, et al.",
    "description": "本文介绍了一种名为 HaELM（基于大语言模型的幻觉评估）的评估框架，旨在解决大型视觉语言模型（LVLMs）中普遍存在的幻觉问题，即模型生成的响应包含视觉输入中并不存在的信息。针对目前 LVLMs 幻觉评估研究相对匮乏的现状，HaELM 提供了一种基于大语言模型的自动化评估方案，其性能可达到 ChatGPT 的 95% 左右，并具备低成本、可复现性、隐私保护以及支持本地部署等额外优势。通过利用 HaELM，研究团队对当前主流 LVLMs 的幻觉现状进行了评估，深入分析了导致幻觉产生的各类因素，并为缓解该问题提出了具有建设性的建议",
    "image": "",
    "code": "https://github.com/junyangwang0410/HaELM"
  },
  {
    "title": "VIGC: Visual Instruction Generation and Correction",
    "venue": "arXiv",
    "date": "2023",
    "link": "https://arxiv.org/pdf/2308.12714.pdf",
    "source": "auto-script",
    "tags": [
      "Hallucination"
    ],
    "authors": "Bin Wang, Fan Wu, Xiao Han, Jiahui Peng, Huaping Zhong, et al.",
    "description": "本文介绍了视觉指令生成与纠正（VIGC）框架，旨在解决多模态大语言模型（MLLMs）中高质量指令微调数据匮乏的挑战。针对目前如 LLaVA 等主流方法过度依赖仅含文本能力的 GPT-4 且难以理解图像细节的问题，VIGC 提出利用现有的多模态大语言模型来生成指令数据。为了克服当前多模态模型容易生成错误信息或不充分响应的局限性，该框架包含两个核心组件：首先通过视觉指令生成（VIG）引导模型产生多样化的微调数据，随后通过视觉指令纠正（VIC）采用迭代更新机制来修正数据中的不准确之处，从而有效降低幻觉风险。通过利用 VIGC 生成的高质量多样化数据对主流模型进行微调，实验结果证明该框架不仅弥补了仅靠文本生成数据的不足，还显著提升了模型在各项基准测试中的性能表现。",
    "image": "",
    "code": "https://opendatalab.github.io/VIGC/"
  },
  {
    "title": "Detecting and Preventing Hallucinations in Large Vision Language Models",
    "venue": "arXiv",
    "date": "2023",
    "link": "https://arxiv.org/pdf/2308.06394.pdf",
    "source": "auto-script",
    "tags": [
      "Hallucination"
    ],
    "authors": "Anisha Gunjal, Jihan Yin, Erhan Bas",
    "description": "本文针对指令微调的大型视觉语言模型（LVLMs）在生成细粒度且视觉对齐的响应时面临的挑战进行了研究，指出即使是当前最先进的模型（如 InstructBLIP），其生成的文本中仍有高达 30% 涉及不存在的物体、不忠实的描述及不准确的关系等幻觉内容。为了解决这一问题，研究团队推出了首个针对详细图像描述的全方位多模态幻觉检测数据集 M-HalDetect，该数据集包含 1.6 万条关于视觉问答（VQA）样例的细粒度标注，不仅涵盖了以往研究关注的物体幻觉，还额外标注了不忠实的实体描述和关系。为了展示该数据集在预防幻觉方面的潜力，研究者提出了一种新型的细粒度直接偏好优化（FDPO）方法对 InstructBLIP 进行优化，并训练了细粒度的多模态奖励模型以执行 N 选 1 拒绝采样（rejection sampling）。人工评估结果显示，FDPO 和拒绝采样分别使 InstructBLIP 的幻觉率降低了 41% 和 55%；此外，该奖励模型还展现出强大的泛化能力，分别使 LLaVA 和 mPLUG-OWL 的幻觉减少了 15% 和 57%，且与人工评估的准确性得分高度相关。",
    "image": "",
    "code": "https://github.com/hendryx-scale/mhal-detect"
  },
  {
    "title": "Mitigating Hallucination in Large Multi-Modal Models via Robust Instruction Tuning",
    "venue": "ICLR",
    "date": "2023",
    "link": "https://arxiv.org/pdf/2306.14565.pdf",
    "source": "auto-script",
    "tags": [
      "Hallucination"
    ],
    "authors": "Fuxiao Liu, Kevin Lin, Linjie Li, Jianfeng Wang, Yaser Yacoob, Lijuan Wang",
    "description": "本文推出了首个大规模且多样化的视觉指令微调数据集，命名为 LRV-Instruction。该数据集包含由 GPT-4 生成的 400k 条视觉指令，涵盖了 16 个视觉语言任务，并提供开放式的指令与回答。不同于以往主要关注正向样本的研究，LRV-Instruction 专门设计了正向和负向指令，以实现更稳健的视觉指令微调，其中负向指令分为不存在物体操纵、已存在物体操纵和知识操纵三个语义层面。为了高效衡量模型产生的幻觉，本文还提出了一种名为 GAVIE 的 GPT-4 辅助视觉指令评估方法，该方法无需人工标注的标准答案，能像人类专家一样稳定地评估各种指令格式。综合实验结果表明，现有模型在面对负向指令（尤其是物体和知识操纵）时表现出严重的幻觉，而通过在 LRV-Instruction 上微调 MiniGPT-4 和 mPLUG-Owl，不仅能有效缓解幻觉，还能在多个公开数据集上提升性能。此外研究发现，在训练数据中保持正负样本的比例平衡有助于构建更稳健的模型。",
    "image": "",
    "code": "https://github.com/FuxiaoLiu/LRV-Instruction"
  },
  {
    "title": "Evaluating Object Hallucination in Large Vision-Language Models",
    "venue": "EMNLP",
    "date": "2023",
    "link": "https://arxiv.org/pdf/2305.10355.pdf",
    "source": "auto-script",
    "tags": [
      "Hallucination"
    ],
    "authors": "Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne Xin Zhao, Ji-Rong Wen",
    "description": "本文针对大型视觉语言模型（LVLMs）在集成大语言模型（LLM）以处理复杂多模态任务时所面临的“物体幻觉”问题进行了系统性研究。研究指出，这些模型在生成描述时往往会产生与目标图像不一致的物体。通过对几种具有代表性的 LVLMs 进行评估实验，作者发现物体幻觉是一个普遍且严峻的问题。研究进一步探讨了视觉指令对幻觉的影响，发现视觉指令中高频出现的物体，或经常与图像中物体共同出现的物体，更容易诱发模型的幻觉现象。此外，作者还发现现有的评估方法容易受到输入指令和生成风格的干扰，为此设计了一种名为 POPE 的基于轮询查询的改进评估方法。实验结果表明，POPE 能够以更稳定、更灵活的方式对 LVLMs 的物体幻觉进行评估。",
    "image": "",
    "code": "https://github.com/RUCAIBox/POPE"
  },
  {
    "title": "Visual In-Context Learning for Large Vision-Language Models",
    "venue": "arXiv",
    "date": "2024",
    "link": "https://arxiv.org/pdf/2402.11574.pdf",
    "source": "auto-script",
    "tags": [
      "In-Context Learning"
    ],
    "authors": "Yucheng Zhou, Xiang Li, Qianning Wang, Jianbing Shen",
    "description": "本文针对大型视觉语言模型（LVLMs）中语境学习（ICL）因跨模态交互挑战和表示差异而受限的问题，提出了一种新型的视觉语境学习（VICL）方法。该方法由视觉示例检索、面向意图的图像摘要以及面向意图的示例组合三个部分组成。具体而言，该方法采用“检索与重排”范式来获取相关图像，结合任务意图和特定任务的视觉解析对图像进行摘要处理，并构建基于语言的示例，从而减少标记数量并缓解跨模态交互难题。在五个视觉推理数据集上的实验评估验证了该方法的有效性。此外，研究通过信息流分析阐明了该机制的作用原理，并探讨了示例长度和位置对模型表现的影响。研究还表明，使用语境去学习（In-context unlearning）有望在无需重训的情况下重置模型的特定知识。",
    "image": "",
    "code": ""
  },
  {
    "title": "RAG-Driver: Generalisable Driving Explanations with Retrieval-Augmented In-Context Learning in Multi-Modal Large Language Model",
    "venue": "RSS",
    "date": "2024",
    "link": "https://arxiv.org/abs/2402.10828",
    "source": "auto-script",
    "tags": [
      "In-Context Learning"
    ],
    "authors": "Jianhao Yuan, Shuyang Sun, Daniel Omeiza, Bo Zhao, Paul Newman, et al.",
    "description": "在复杂的自动驾驶领域，解释性对于建立终端用户的透明度与接受度至关重要，而多模态大语言模型（MLLMs）在生成驾驶预测及自然语言解释方面展现了巨大潜力。针对标注成本高昂导致的严重数据稀缺、不同数据集间的领域鸿沟，以及模型训练成本高和灾难性遗忘导致的部署后泛化性受限等挑战，本文提出了 RAG-Driver。这是一种新型的检索增强多模态大语言模型，利用语境学习（In-context learning）来实现高性能、可解释且具泛化性的自动驾驶。通过以检索到的专家示例为基础进行预测，实验证明 RAG-Driver 在生成驾驶动作解释、理由说明及控制信号预测方面达到了最先进的水平；更重要的是，该模型在无需进一步训练的情况下，对未见过的环境表现出了卓越的零样本泛化能力。如果你需要，我可以为你详细对比这种检索增强方法与传统微调方法在处理自动驾驶长尾场景时的优劣。",
    "image": "",
    "code": ""
  },
  {
    "title": "Can MLLMs Perform Text-to-Image In-Context Learning?",
    "venue": "arXiv",
    "date": "2024",
    "link": "https://arxiv.org/pdf/2402.01293.pdf",
    "source": "auto-script",
    "tags": [
      "In-Context Learning"
    ],
    "authors": "Yuchen Zeng, Wonjun Kang, Yicong Chen, Hyung Il Koo, Kangwook Lee",
    "description": "从大语言模型（LLMs）向多模态大语言模型（MLLMs）的演进，激发了将语境学习（ICL）扩展至多模态领域的研究，但现有研究主要集中在“图像到文本”的语境学习，而具有独特特性和应用潜力的“文本到图像”语境学习（T2I-ICL）仍处于探索不足的状态。为了弥补这一空白，本文正式定义了 T2I-ICL 任务，并推出了首个包含十个任务的 T2I-ICL 基准数据集 CoBSAT。通过利用该数据集对六种先进的多模态大语言模型进行测试，研究揭示了这些模型在解决 T2I-ICL 任务时面临的巨大困难，并确定其核心挑战在于多模态和图像生成的固有复杂性。此外，研究还表明，通过采用微调和思维链（Chain-of-Thought）提示等策略可以有效缓解这些困难，从而显著提升模型的性能表现。",
    "image": "",
    "code": "https://github.com/UW-Madison-Lee-Lab/CoBSAT"
  },
  {
    "title": "Generative Multimodal Models are In-Context Learners",
    "venue": "CVPR",
    "date": "2023",
    "link": "https://arxiv.org/pdf/2312.13286",
    "source": "auto-script",
    "tags": [
      "In-Context Learning"
    ],
    "authors": "Quan Sun, Yufeng Cui, Xiaosong Zhang, Fan Zhang, Qiying Yu, et al.",
    "description": "本文介绍了 Emu2，这是一个拥有 370 亿参数的生成式多模态大模型，旨在增强多模态系统在仅需少量示例或简单指令下的语境学习能力。该模型采用统一的自回归目标在大规模多模态序列上进行训练，展现出强大的多模态语境学习能力，甚至能够处理需要即时推理的任务，如视觉提示（visual prompting）和基于物体的定位生成。在少样本设置下，Emu2 在多个多模态理解任务上刷新了记录；而经过针对性指令微调后，它在大型多模态模型问答基准测试及开放式主题驱动生成等挑战性任务中均达到了最先进的水平。这些成果表明，Emu2 可以作为广泛多模态任务的基础模型和通用接口，为后续研究提供了强有力的支持。",
    "image": "",
    "code": "https://github.com/baaivision/Emu"
  },
  {
    "title": "Hijacking Context in Large Multi-modal Models",
    "venue": "arXiv",
    "date": "2023",
    "link": "https://arxiv.org/pdf/2312.07553.pdf",
    "source": "auto-script",
    "tags": [
      "In-Context Learning"
    ],
    "authors": "Joonhyun Jeong",
    "description": "最近，大型多模态模型（LMMs）展现出了根据指令理解图像视觉内容的能力。基于大语言模型（LLMs）构建的 LMMs 也继承了其原有的能力与特性，例如在输入提示中给定连贯的图文序列进行语境学习（In-context learning）。然而，本研究发现了现有现成 LMMs 的一个新局限：一小部分不连贯的图像或文本描述会误导模型仅生成关于被“劫持”语境的偏见输出，而非最初预期的内容。为了解决这一问题，研究团队提出了一种预过滤方法，利用 GPT-4V 对语境分布偏移的鲁棒性来剔除无关语境。此外，本研究还进一步探讨了是否可以通过 GPT-4V 和文本生成图像模型，将这些被劫持的视觉及文本语境替换为相关的语境，从而帮助模型生成连贯的响应。",
    "image": "",
    "code": ""
  },
  {
    "title": "Towards More Unified In-context Visual Understanding",
    "venue": "arXiv",
    "date": "2023",
    "link": "https://arxiv.org/pdf/2312.02520.pdf",
    "source": "auto-script",
    "tags": [
      "In-Context Learning"
    ],
    "authors": "Dianmo Sheng, Dongdong Chen, Zhentao Tan, Qiankun Liu, Qi Chu, et al.",
    "description": "随着大语言模型（LLMs）的飞速发展，语境学习（ICL）已成为自然语言处理领域的前沿方法，并近期在语义分割和图像描述等视觉理解任务中取得了显著成果。针对现有视觉语境学习框架无法跨多模态生成内容从而限制了应用场景这一问题，本文提出了一种支持多模态输出的新型视觉理解语境学习框架。该框架首先将文本和视觉提示进行量化并嵌入到统一的表示空间中，构建为交错的语境序列，随后采用仅解码器的稀疏 Transformer 架构对其进行生成式建模，以实现语境学习。得益于这种设计，模型能够以统一的方式处理具有多模态输出的语境视觉理解任务。实验结果表明，该模型与专用模型及以往的语境学习基准相比具有竞争力，为实现统一的多模态语境学习迈出了重要一步。",
    "image": "",
    "code": ""
  },
  {
    "title": "MMICL: Empowering Vision-language Model with Multi-Modal In-Context Learning",
    "venue": "arXiv",
    "date": "2023",
    "link": "https://arxiv.org/pdf/2309.07915.pdf",
    "source": "auto-script",
    "tags": [
      "In-Context Learning"
    ],
    "authors": "Haozhe Zhao, Zefan Cai, Shuzheng Si, Xiaojian Ma, Kaikai An, et al.",
    "description": "本文针对大语言模型（LLMs）能够利用语境学习（ICL）处理丰富的背景知识和任务信息，而大多数视觉语言模型（VLMs）在理解包含多张图像的复杂多模态提示词方面仍然表现挣扎、导致下游任务效果不佳的局限性，提出了具有多模态语境学习能力的视觉语言模型 MMICL。该研究通过引入一种使视觉语言模型能够高效处理多模态输入的新方法，并提出一种全新的上下文方案来增强模型的语境学习能力，同时构建了专门设计的多模态语境学习（MIC）数据集，以提升模型对复杂多模态提示词的理解力。实验结果证实，MMICL 在广泛的通用视觉语言任务中刷新了零样本性能记录，尤其在 MME 和 MMBench 等复杂基准测试上表现卓越。分析表明，MMICL 不仅能有效应对复杂多模态提示理解的挑战，展现出令人印象深刻的语境学习能力，还成功缓解了视觉语言模型中常见的语言偏差问题，从而减少了在面对大量文本上下文时容易产生的幻觉。",
    "image": "",
    "code": "https://github.com/PKUnlp-icler/MIC"
  },
  {
    "title": "Link-Context Learning for Multimodal LLMs",
    "venue": "arXiv",
    "date": "2023",
    "link": "https://arxiv.org/pdf/2308.07891.pdf",
    "source": "auto-script",
    "tags": [
      "In-Context Learning"
    ],
    "authors": "Yan Tai, Weichen Fan, Zhao Zhang, Feng Zhu, Rui Zhao, Ziwei Liu",
    "description": "本文针对多模态大语言模型（MLLMs）在无需训练的情况下识别未见图像或理解新概念所面临的挑战，提出了一种名为链接上下文学习（LCL）的新型学习范式。不同于传统的语境学习（ICL），LCL 强调通过“因果推理”来增强模型的学习能力，通过明确强化支持集与查询集之间的因果关系，引导模型不仅识别类比关系，更能洞察数据点之间潜在的因果关联。这种方法赋予了 MLLMs 更有效地识别未见图像和理解新概念的能力。为了评估这一新方法，研究团队推出了 ISEKAI 数据集，该数据集完全由为链接上下文学习设计的、从未见过的生成图像-标签对组成。广泛的实验结果表明，相比于传统的 MLLMs，采用 LCL 架构的模型在处理新概念时展现出了强大的链接上下文学习能力。",
    "image": "",
    "code": "https://github.com/isekai-portal/Link-Context-Learning"
  },
  {
    "title": "OpenFlamingo: An Open-Source Framework for Training Large Autoregressive Vision-Language Models",
    "venue": "arXiv",
    "date": "2023",
    "link": "https://arxiv.org/pdf/2308.01390.pdf",
    "source": "auto-script",
    "tags": [
      "In-Context Learning"
    ],
    "authors": "Anas Awadalla, Irena Gao, Josh Gardner, et al.",
    "description": "本文介绍了 OpenFlamingo，这是一个参数量从 30 亿到 90 亿不等的自回归视觉语言模型系列。作为一项旨在开源复现 DeepMind Flamingo 模型的研究工作，OpenFlamingo 在七个视觉语言数据集上的平均性能达到了对应 Flamingo 模型的 80% 至 89%。本技术报告详细描述了该系列模型的架构设计、训练数据、超参数配置以及配套的评估套件。",
    "image": "",
    "code": "https://github.com/mlfoundations/open_flamingo"
  },
  {
    "title": "Med-Flamingo: a Multimodal Medical Few-shot Learner",
    "venue": "arXiv",
    "date": "2023",
    "link": "https://arxiv.org/pdf/2307.15189.pdf",
    "source": "auto-script",
    "tags": [
      "In-Context Learning"
    ],
    "authors": "Michael Moor, Qian Huang, Shirley Wu, et al.",
    "description": "本文介绍了 Med-Flamingo，这是一个专为医学领域设计的多模态少样本学习器，旨在解决现有医学视觉语言模型（VLMs）因依赖大规模下游数据集微调而难以应对医疗数据稀缺及实时学习需求的问题。该模型基于 OpenFlamingo-9B，通过在来自出版物和教科书的配对及交错的医学图文数据上进行持续预训练，成功解锁了少样本生成式医学视觉问答（VQA）能力。研究团队在包括一个新型且极具挑战性的、模拟美国执业医师资格考试（USMLE）风格的开放式 VQA 数据集在内的多个基准上进行了评估。此外，该研究还开展了首次生成式医学 VQA 的人类评估，由医生在交互式应用中以盲测方式对生成的回答进行审核。实验结果表明，Med-Flamingo 在临床医生评分中的表现提升了高达 20%，并首次实现了多模态医学少样本适配能力，例如生成诊断逻辑（rationale generation）。",
    "image": "",
    "code": "https://github.com/snap-stanford/med-flamingo"
  },
  {
    "title": "Generative Pretraining in Multimodality",
    "venue": "ICLR",
    "date": "2023",
    "link": "https://arxiv.org/pdf/2307.05222.pdf",
    "source": "auto-script",
    "tags": [
      "In-Context Learning",
      "Foundation Models"
    ],
    "authors": "Quan Sun, Qiying Yu, Yufeng Cui, Fan Zhang, Xiaosong Zhang, et al.",
    "description": "本文介绍了 Emu，这是一个基于 Transformer 的多模态基础模型，能够在中交错的多模态上下文中无缝生成图像和文本。该模型采用“单一模型处理所有”的自回归训练过程，可以不加区别地接收任何单模态或多模态数据输入，例如交错的图像、文本和视频。首先，视觉信号被编码为嵌入向量，并与文本标记共同构成交错的输入序列，随后 Emu 以统一的目标进行端到端训练，即在多模态序列中分类下一个文本标记或回归下一个视觉嵌入。这种通用的多模态处理能力使其能够大规模探索多样化的预训练数据源，包括带有交错帧和文本的视频、交错图文的网页，以及海量的图文对和视频文本对。Emu 可作为图像转文本和文本转图像任务的通用多模态接口，并支持语境内的图像和文本生成。在包括图像描述、视觉问答、视频问答和文本生成图像在内的广泛零样本及少样本任务中，Emu 与现有的先进大型多模态模型相比表现出卓越的性能，同时在经过指令微调后，其作为多模态助手所展示的扩展能力也令人印象深刻。",
    "image": "",
    "code": "https://github.com/baaivision/Emu"
  },
  {
    "title": "AVIS: Autonomous Visual Information Seeking with Large Language Models",
    "venue": "arXiv",
    "date": "2023",
    "link": "https://arxiv.org/pdf/2306.08129.pdf",
    "source": "auto-script",
    "tags": [
      "In-Context Learning",
      "Visual Reasoning"
    ],
    "authors": "Ziniu Hu, Ahmet Iscen, Chen Sun, et al.",
    "description": "本文提出了一种名为 AVIS 的自动信息寻求视觉问答框架，该框架利用大语言模型（LLM）动态制定外部工具的使用策略并分析其输出，从而获取回答复杂视觉问题所需的核心知识。针对诸如“图中建筑是为了纪念什么事件？”这类需要外部知识的视觉问答任务，AVIS 旨在解决涉及调用 API、分析响应及做出决策等一系列动作的组合搜索空间难题。研究团队通过用户研究收集了人类在面对此类任务时的决策实例，并以此设计了一个由三个核心组件构成的系统：一个动态决定下一步工具调用的 LLM 驱动规划器、一个负责分析并提取工具输出关键信息的 LLM 驱动推理器，以及一个在整个过程中保留已获取信息的当前内存组件。收集到的用户行为数据在两个关键维度上指导了系统的运行：首先，通过分析用户的决策序列构建转移图，界定不同的状态并限制每个状态下的可用操作集合；其次，利用用户决策示例为规划器和推理器提供相关的上下文实例，增强其决策能力。实验结果表明，AVIS 在 Infoseek 和 OK-VQA 等知识密集型视觉问答基准测试中达到了最先进的水平。",
    "image": "",
    "code": ""
  },
  {
    "title": "Exploring Diverse In-Context Configurations for Image Captioning",
    "venue": "NeurIPS",
    "date": "2023",
    "link": "https://arxiv.org/pdf/2305.14800.pdf",
    "source": "auto-script",
    "tags": [
      "In-Context Learning"
    ],
    "authors": "Xu Yang, Yongliang Wu, Mingzhuo Yang, Haokun Chen, Xin Geng",
    "description": "本文针对大语言模型（LMs）作为优秀的语境下少样本学习器这一特性，探讨了在视觉语言（VL）领域中优化语境序列配置的策略。尽管自然语言处理领域已有多种优化策略，但现有的视觉语言少样本学习器通常仅采用随机采样这种最简单的方式来配置语境中的图文对。为了深入探究不同配置对视觉语言语境学习的影响，研究团队以图像描述（Image Captioning）这一视觉条件下的语言模型任务作为案例研究，设计了四种图像选择策略和四种描述分配策略来配置语境图文对。通过全面的实验，该研究得出了两个与直觉相悖但极具价值的见解，揭示了由于多模态协同作用，视觉语言语境学习展现出与纯自然语言处理案例截然不同的特征。此外，在对最优组合策略的探索中，研究发现该方法相比基线水平在 CIDEr 分值上平均提升了 20.9 分。",
    "image": "",
    "code": "https://github.com/yongliang-wu/ExploreCfg"
  },
  {
    "title": "Chameleon: Plug-and-Play Compositional Reasoning with Large Language Models",
    "venue": "arXiv",
    "date": "2023",
    "link": "https://arxiv.org/pdf/2304.09842.pdf",
    "source": "auto-script",
    "tags": [
      "In-Context Learning",
      "Visual Reasoning",
      "Chain-of-Thought"
    ],
    "authors": "Pan Lu, Baolin Peng, Hao Cheng, Michel Galley, Kai-Wei Chang, et al.",
    "description": "本文介绍了 Chameleon，这是一个旨在缓解大语言模型（LLMs）固有局限性（如无法获取即时信息、难以使用外部工具以及进行精确数理逻辑推理等）的 AI 系统。该系统通过为 LLMs 增加即插即用的模块来实现组合推理，能够合成程序并组合包括现成视觉模型、网络搜索引擎、Python 函数及启发式模块在内的多种工具来完成复杂的推理任务。Chameleon 的核心是一个基于 LLM 的规划器，它负责组装一系列待执行的工具序列以生成最终响应。研究团队在 ScienceQA 和 TabMWP 这两个多模态知识密集型推理任务上展示了 Chameleon 的有效性：基于 GPT-4 驱动的 Chameleon 在 ScienceQA 上达到了 86.54% 的整体准确率，将此前已发表的最佳少样本结果提升了 11.37%；在 TabMWP 上则将准确率提升了 17.0%，使该任务的最先进水平达到了 98.78%。此外，分析表明，与基于 ChatGPT 的规划器相比，由 GPT-4 驱动的规划器通过从指令中推断潜在约束，展现出了更连贯且更合理的工具选择能力。",
    "image": "",
    "code": "https://chameleon-llm.github.io/"
  },
  {
    "title": "HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in HuggingFace",
    "venue": "arXiv",
    "date": "2023",
    "link": "https://arxiv.org/pdf/2303.17580.pdf",
    "source": "auto-script",
    "tags": [
      "In-Context Learning",
      "Visual Reasoning"
    ],
    "authors": "Yongliang Shen, Kaitao Song, Xu Tan, et al.",
    "description": "本文介绍了 HuggingGPT，这是一个以大语言模型（LLM）为核心的智能体，旨在通过将大语言模型作为控制器来连接和管理不同领域的 AI 模型，以解决复杂的多模态任务。该系统利用 ChatGPT 在语言理解、生成、交互和推理方面的卓越能力，将语言作为通用接口，连接如 Hugging Face 等社区中的丰富模型资源。具体而言，当接收到用户请求时，HuggingGPT 使用 ChatGPT 进行任务规划，并根据 Hugging Face 中模型的功能描述选择合适的模型，随后执行各子任务并汇总执行结果生成最终响应。通过整合 ChatGPT 的强大语言处理能力与 Hugging Face 上的多样化模型，HuggingGPT 能够处理跨越语言、视觉、语音等多个领域的复杂 AI 任务，为实现通用人工智能开辟了新路径。",
    "image": "",
    "code": "https://github.com/microsoft/JARVIS"
  },
  {
    "title": "MM-REACT: Prompting ChatGPT for Multimodal Reasoning and Action",
    "venue": "arXiv",
    "date": "2023",
    "link": "https://arxiv.org/pdf/2303.11381.pdf",
    "source": "auto-script",
    "tags": [
      "In-Context Learning",
      "Visual Reasoning",
      "Chain-of-Thought"
    ],
    "authors": "Zhengyuan Yang, Linjie Li, Jianfeng Wang, et al.",
    "description": "本文提出了 MM-REACT，这是一种将 ChatGPT 与一系列视觉专家模型相结合的系统范式，旨在实现多模态推理与行动。研究团队定义并探索了一系列具有挑战性的高级视觉任务，这些任务往往超出了现有视觉或视觉语言模型的能力范围。为了实现这种高级视觉智能，MM-REACT 引入了一种独特的文本提示设计，能够通过文本描述、文本化的空间坐标以及对齐的文件名来表征图像和视频等密集的视觉信号。这种提示设计使语言模型能够接收、关联并处理多模态信息，从而促进了 ChatGPT 与各类视觉专家模型之间的协同配合。零样本实验证明，MM-REACT 在处理特定的复杂视觉理解任务方面表现出色，并能广泛应用于多种应用场景。此外，本文还深入讨论并将 MM-REACT 的系统范式与另一种通过联合微调来扩展语言模型多模态能力的方法进行了对比分析。",
    "image": "",
    "code": "https://multimodal-react.github.io/"
  },
  {
    "title": "ICL-D3IE: In-Context Learning with Diverse Demonstrations Updating for Document Information Extraction",
    "venue": "ICCV",
    "date": "2023",
    "link": "https://arxiv.org/pdf/2303.05063.pdf",
    "source": "auto-script",
    "tags": [
      "In-Context Learning",
      "Visual Reasoning"
    ],
    "authors": "Jiabang He, Lei Wang, Yi Hu, Ning Liu, Hui Liu, et al.",
    "description": "本文提出了 ICL-D3IE 框架，这是首个探索大语言模型（LLMs）如何通过语境学习（In-context learning）执行文档信息抽取（DIE）任务的研究，旨在解决应用 LLMs 处理该任务时面临的模态及任务差异挑战。该框架通过设计多种类型的示例来增强模型能力：首先从具有挑战性的训练文档中提取最困难且最具辨识度的片段作为硬示例，以提升模型在处理复杂测试实例时的表现；其次设计了描述实体关系的示例，帮助模型理解文档中的位置关系；同时引入格式化示例以简化答案提取过程。此外，该框架还通过迭代更新机制来不断优化示例的多样性。在三个主流基准数据集上的实验结果表明，ICL-D3IE 能够使 Davinci-003 或 ChatGPT 在分布内（ID）和分布外（OOD）设置下，均取得优于以往经过全量数据微调的预训练方法的性能表现。",
    "image": "",
    "code": "https://github.com/MAEHCM/ICL-D3IE"
  },
  {
    "title": "Prompting Large Language Models with Answer Heuristics for Knowledge-based Visual Question Answering",
    "venue": "CVPR",
    "date": "2023",
    "link": "https://arxiv.org/pdf/2303.01903.pdf",
    "source": "auto-script",
    "tags": [
      "In-Context Learning"
    ],
    "authors": "Zhou Yu, Xuecheng Ouyang, Zhenwei Shao, Meng Wang, Jun Yu",
    "description": "本文提出了一种名为 Prophet 的概念简单、灵活且通用的框架，旨在通过启发式答案提示来增强大语言模型（LLM）在知识密集型视觉问答（VQA）任务中的表现。针对以往研究中仅凭文本输入难以让“盲目”的 LLM 充分理解视觉信息的问题，Prophet 首先在不借助外部知识的情况下，在特定数据集上训练一个基础 VQA 模型，并从中提取两种互补的启发式信息：候选答案和答案感知示例。这两类信息被共同编码进格式化提示词中，以帮助 LLM 更好地理解图像与问题，从而生成更准确的回答。通过结合 GPT-3，Prophet 在四个挑战性的知识密集型 VQA 基准数据集上显著超越了现有技术，且该框架具有极强的通用性，支持不同类型的 VQA 模型（判别式或生成式）与不同 LLM（商业或开源）的组合。此外，该框架还可以升级为 Prophet++，通过在不同阶段集成现代大型多模态模型，进一步提升模型在知识密集型 VQA 任务上的处理能力。",
    "image": "",
    "code": "https://github.com/MILVLG/prophet"
  },
  {
    "title": "Visual Programming: Compositional visual reasoning without training",
    "venue": "CVPR",
    "date": "2022",
    "link": "https://openaccess.thecvf.com/content/CVPR2023/papers/Gupta_Visual_Programming_Compositional_Visual_Reasoning_Without_Training_CVPR_2023_paper.pdf",
    "source": "auto-script",
    "tags": [
      "In-Context Learning",
      "Visual Reasoning",
      "Chain-of-Thought"
    ],
    "authors": "Tanmay Gupta, Aniruddha Kembhavi",
    "description": "本文介绍了 VISPROG，这是一种在给定自然语言指令下解决复杂和组合视觉任务的神经符号方法，该方法无需任何特定任务的训练。VISPROG 利用大语言模型的语境学习能力来生成类 Python 的模块化程序，通过执行这些程序来获得解决方案以及全面且具有可解释性的推理逻辑。生成程序中的每一行代码都可以调用现成的计算机视觉模型、图像处理子程序或 Python 函数，从而产生可供后续程序步骤使用的中间输出。研究者在组合视觉问答、图像对的零样本推理、事实知识物体标记以及语言引导的图像编辑这四个多样化任务上证明了 VISPROG 的灵活性。作者认为，像 VISPROG 这样的神经符号方法是轻松有效扩展 AI 系统范畴的一个令人兴奋的途径，能够服务于人类可能希望执行的各种复杂的长尾任务。",
    "image": "",
    "code": "https://github.com/allenai/visprog"
  },
  {
    "title": "An Empirical Study of GPT-3 for Few-Shot Knowledge-Based VQA",
    "venue": "AAAI",
    "date": "2022",
    "link": "https://ojs.aaai.org/index.php/AAAI/article/download/20215/19974",
    "source": "auto-script",
    "tags": [
      "In-Context Learning"
    ],
    "authors": "Zhengyuan Yang, Zhe Gan, Jianfeng Wang, et al.",
    "description": "本文介绍了一种名为 PICa 的简单且有效的方法，该方法通过使用图像描述引导 GPT-3 解决知识密集型视觉问答（VQA）任务，克服了以往方法因依赖外部结构化知识库而引入噪声或信息失真的局限性。PICa 将 GPT-3 视为一个隐含且非结构化的知识库，通过将图像转换为模型可理解的文本描述（或标签），并辅以少量语境内的 VQA 示例，使模型能够以少样本学习的方式处理多模态任务。实验表明，仅使用 16 个示例，PICa 在 OK-VQA 数据集上就比之前的监督学习最先进方法高出 8.6 分，并在 VQAv2 基准测试中也展现了出色的少样本性能。此外，本研究还引入了 VISPROG，这是一种神经符号方法，利用大语言模型的语境学习能力生成类 Python 的模块化程序，通过调用现成的视觉模型或处理程序来解决复杂的组合视觉任务，且无需任何特定任务的训练。VISPROG 在组合视觉问答、图像对推理、知识物体标记和图像编辑等多样化任务中表现出极高的灵活性，并能提供具有可解释性的逻辑说明，为扩展 AI 系统以服务复杂的长尾任务开辟了新路径。",
    "image": "",
    "code": ""
  },
  {
    "title": "Flamingo: a Visual Language Model for Few-Shot Learning",
    "venue": "NeurIPS",
    "date": "2022",
    "link": "https://arxiv.org/pdf/2204.14198.pdf",
    "source": "auto-script",
    "tags": [
      "In-Context Learning"
    ],
    "authors": "Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, et al.",
    "description": "本文介绍了 Flamingo 系列视觉语言模型（VLM），该系列模型旨在解决多模态机器学习中仅利用少量标注示例即可快速适配新任务的挑战。通过关键的架构创新，Flamingo 成功桥接了预训练的纯视觉和纯语言模型，能够处理任意交错的视觉与文本数据序列，并支持图像或视频作为输入。得益于这种灵活性，Flamingo 可以在包含交错图文的大规模多模态网络语料库上进行训练，从而获得强大的语境下少样本学习能力。研究人员对该模型在视觉问答、图像描述以及多选视觉问答等多种开放式和封闭式图像及视频任务上进行了全面评估。实验结果表明，仅通过提供特定任务的示例，单个 Flamingo 模型就能在少样本学习设置下刷新多项任务的最先进记录，且在众多基准测试中，其性能甚至超越了那些在数千倍于其的任务特定数据上进行微调的模型。",
    "image": "",
    "code": ""
  },
  {
    "title": "Multimodal Few-Shot Learning with Frozen Language Models",
    "venue": "NeurIPS",
    "date": "2021",
    "link": "https://arxiv.org/pdf/2106.13884.pdf",
    "source": "auto-script",
    "tags": [
      "In-Context Learning"
    ],
    "authors": "Maria Tsimpoukelli, Jacob Menick, Serkan Cabi, et al.",
    "description": "本文介绍了一种简单且有效的方法，旨在将自动回归语言模型在足够训练规模下展现出的少样本学习能力迁移到视觉与语言的多模态场景中。研究者利用对齐的图像和标题数据训练了一个视觉编码器，将每张图像表示为一系列连续嵌入向量，作为前缀引导预训练且冻结参数的语言模型生成相应的标题。由此构建的系统成为了一个多模态少样本学习器，它具备一种惊人的能力，即在给定由多个交错图文嵌入组成的示例序列作为条件时，能够学习各种新任务。通过在多个成熟及新增的基准测试中进行评估，研究证明了该单一模型能够快速学习新物体的名称和全新的视觉类别，仅通过少量示例即可完成视觉问答，并能有效利用外部知识。",
    "image": "",
    "code": ""
  },
  {
    "title": "Insight-V: Exploring Long-Chain Visual Reasoning with Multimodal Large Language Models",
    "venue": "arXiv",
    "date": "2024",
    "link": "https://arxiv.org/pdf/2411.14432",
    "source": "auto-script",
    "tags": [
      "Chain-of-Thought"
    ],
    "authors": "Yuhao Dong, Zuyan Liu, Hai-Long Sun, et al.",
    "description": "本文介绍了 Insight-V，这是一项旨在提升多模态大语言模型（MLLMs）在复杂视觉任务中推理能力的研究，填补了视觉语言领域高质量长链推理数据和优化训练流程的空白。研究团队首先设计了一个包含渐进式生成策略和多粒度评估方法的两阶段流程，旨在无需人工干预的情况下，规模化地生产长且稳健的结构化推理数据。针对直接使用复杂长链数据监督模型效果不佳的问题，Insight-V 提出了一个多智能体系统，由专门负责长链推理的推理智能体和负责评判并汇总结果的总结智能体组成，并进一步结合迭代直接偏好优化（DPO）算法以提升生成的稳定性。实验表明，基于 LLaVA-NeXT 和更强基础模型构建的 Insight-V，在多项具有挑战性的视觉推理基准测试中取得了显著的性能增益，并能通过其多智能体架构在感知类多模态任务中保持或提升表现。",
    "image": "",
    "code": "https://github.com/dongyh20/Insight-V"
  },
  {
    "title": "Cantor: Inspiring Multimodal Chain-of-Thought of MLLM",
    "venue": "arXiv",
    "date": "2024",
    "link": "https://arxiv.org/pdf/2404.16033.pdf",
    "source": "auto-script",
    "tags": [
      "Chain-of-Thought"
    ],
    "authors": "Timin Gao, Peixian Chen, Mengdan Zhang, et al.",
    "description": "本文提出了一种名为 Cantor 的创新型多模态思维链（CoT）框架，旨在通过感知-决策架构解决复杂视觉推理任务中存在的决策幻觉及底层感知工具摘要能力不足等挑战。针对现有方法在决策时因视觉信息不足或缺乏高度抽象总结而产生的难题，Cantor 首先作为决策生成器整合视觉输入来深入分析图像与问题，确保推理过程与实际语境紧密对齐。随后，Cantor 利用多模态大语言模型（MLLMs）的高级认知功能，使其充当多领域专家来提取高层级信息，从而显著增强思维链的生成质量。广泛的实验结果表明，该框架在无需微调或参考标准推理过程（ground-truth rationales）的情况下，在两个复杂的视觉推理数据集上显著提升了多模态思维链的表现。",
    "image": "",
    "code": "https://ggg0919.github.io/cantor/"
  },
  {
    "title": "Visual CoT: Unleashing Chain-of-Thought Reasoning in Multi-Modal Language Models",
    "venue": "arXiv",
    "date": "2024",
    "link": "https://arxiv.org/pdf/2403.16999.pdf",
    "source": "auto-script",
    "tags": [
      "Chain-of-Thought"
    ],
    "authors": "Hao Shao, Shengju Qian, Han Xiao, et al.",
    "description": "本文针对多模态大语言模型（MLLMs）在处理复杂视觉输入时缺乏可解释性以及难以捕捉高分辨率图像中微小关键区域信息的问题，提出了 Visual CoT 数据集和一套多轮处理流程。该研究收集并发布了包含 438,000 个问答对的大规模 Visual CoT 数据集，其中所有问答对均配有标注关键区域的中间边界框，且约 98,000 个问答对包含详细的推理步骤。为了更好地解决上述挑战，研究团队设计了一种多轮处理流水线，能够动态聚焦视觉输入并生成可解释的思考逻辑，并引入了相应的基准测试以评估模型在特定局部区域识别场景下的表现。广泛的实验结果证明了该框架的有效性，并为优化多模态模型的推理策略提供了新的视角。",
    "image": "",
    "code": "https://github.com/deepcs233/Visual-CoT"
  },
  {
    "title": "Compositional Chain-of-Thought Prompting for Large Multimodal Models",
    "venue": "CVPR",
    "date": "2023",
    "link": "https://arxiv.org/pdf/2311.17076",
    "source": "auto-script",
    "tags": [
      "Chain-of-Thought"
    ],
    "authors": "Chancharik Mitra, Brandon Huang, Trevor Darrell, Roei Herzig",
    "description": "尽管强大的视觉主干网络与大语言模型（LLM）的推理能力相结合，已使大型多模态模型（LMMs）成为处理广泛视觉语言任务的标准，但研究表明，即便最先进的模型在捕获对象属性及相互关系等组合视觉推理方面仍面临挑战。为了解决这一问题，本文受思维链（CoT）方法的启发，提出了一种名为组合思维链（CCoT）的新型零样本提示方法。该方法利用场景图（SG）这一正式化表示作为视觉与文本领域之间的桥梁，通过首先让 LMM 生成场景图，随后将其整合进提示词中来引导模型生成最终响应，从而有效提取多模态模型中的组合知识。CCoT 的优势在于它无需昂贵的场景图标注数据进行微调，从而避免了微调过程中可能出现的预训练目标灾难性遗忘。广泛的实验结果证明，CCoT 不仅在多个视觉语言组合推理基准测试中提升了模型表现，还在多个通用多模态基准测试中增强了主流 LMMs 的性能，展现了其在无需额外标注或训练情况下的普适性与有效性。",
    "image": "",
    "code": "https://github.com/chancharikmitra/CCoT"
  },
  {
    "title": "DDCoT: Duty-Distinct Chain-of-Thought Prompting for Multimodal Reasoning in Language Models",
    "venue": "NeurIPS",
    "date": "2023",
    "link": "https://arxiv.org/pdf/2310.16436.pdf",
    "source": "auto-script",
    "tags": [
      "Chain-of-Thought"
    ],
    "authors": "Ge Zheng, Bin Yang, Jiajin Tang, Hong-Yu Zhou, Sibei Yang",
    "description": "本文介绍了一种名为 DDCoT 的新型提示方法，旨在解决多模态大语言模型在实现类似人类的复杂多步推理时面临的人工标注成本高、灵活性差以及泛化性与可解释性不足等挑战。通过对多模态思维链（CoT）推理的深入分析，研究者提出了“保持批判性思维”和“各司其职”两个核心洞察，并据此设计了通过负空间提示（negative-space prompting）维持批判态度的机制。DDCoT 将大语言模型的推理职责拆分为“推理”与“识别”，并将视觉模型的识别能力整合进联合推理过程中，从而激发了多模态思维链推理。实验结果表明，DDCoT 生成的推理逻辑不仅在零样本提示和微调学习中显著提升了大模型和小模型的推理能力，性能超越了现有最先进的方法，同时在泛化性和可解释性方面也表现出色。",
    "image": "",
    "code": "https://github.com/SooLab/DDCOT"
  },
  {
    "title": "Explainable Multimodal Emotion Reasoning",
    "venue": "arXiv",
    "date": "2023",
    "link": "https://arxiv.org/pdf/2306.15401.pdf",
    "source": "auto-script",
    "tags": [
      "Chain-of-Thought"
    ],
    "authors": "Zheng Lian, Haiyang Sun, Licai Sun, Hao Gu, Zhuofan Wen, et al.",
    "description": "本文提出了一个名为“可解释多模态情绪识别（EMER）”的新任务，旨在解决传统情绪识别中因过度依赖主观标注和多数投票机制而导致细微或非候选情绪被忽略的问题。不同于仅输出预测标签的传统方法，EMER 任务要求模型在识别情绪状态的同时提供相应的解释，从而通过逻辑依据确保情绪标签的可靠性。该研究利用大语言模型（LLMs）来消除单模态线索中的歧义，并生成更完整的多模态解释，进而以开放词汇的方式提取出更丰富的情绪表达。本文介绍了在该任务上的初步尝试，包括构建新数据集、建立基准模型以及定义评估指标，同时 EMER 也可作为评估多模态大语言模型在音视频文本理解性能上的基准任务。",
    "image": "",
    "code": ""
  },
  {
    "title": "EmbodiedGPT: Vision-Language Pre-Training via Embodied Chain of Thought",
    "venue": "arXiv",
    "date": "2023",
    "link": "https://arxiv.org/pdf/2305.15021.pdf",
    "source": "auto-script",
    "tags": [
      "Chain-of-Thought"
    ],
    "authors": "Yao Mu, Qinglong Zhang, Mengkang Hu, et al.",
    "description": "本文介绍了 EmbodiedGPT，这是一个为具身智能（Embodied AI）设计的端到端多模态基础模型，旨在赋予智能体在物理环境中进行多模态理解与执行复杂长程任务的能力。为了实现这一目标，研究团队首先构建了一个名为 EgoCOT 的大规模具身规划数据集，该数据集从 Ego4D 中精选视频并配以高质量指令，通过“思维链（Chain of Thoughts）”模式生成子目标序列以实现有效的具身规划；其次，提出了一种高效的训练方法，通过前缀微调（prefix tuning）使一个 7B 参数量的大语言模型（LLM）适配 EgoCOT 数据集，从而生成高质量的规划方案；最后，引入了一种从 LLM 生成的规划查询中提取任务相关特征的范式，从而在高级规划与底层控制之间形成闭环。广泛的实验证明了 EmbodiedGPT 在具身规划、具身控制、视觉描述和视觉问答等任务中的有效性，特别是在具身控制任务中，该模型通过提取更有效的特征显著提升了成功率，在 Franka Kitchen 和 Meta-World 基准测试中分别达到了 BLIP-2 基准模型的 1.6 倍和 1.3 倍。",
    "image": "",
    "code": ""
  },
  {
    "title": "Let’s Think Frame by Frame: Evaluating Video Chain of Thought with Video Infilling and Prediction",
    "venue": "arXiv",
    "date": "2023",
    "link": "https://arxiv.org/pdf/2305.13903.pdf",
    "source": "auto-script",
    "tags": [
      "Chain-of-Thought"
    ],
    "authors": "Vaishnavi Himakunthala, Andy Ouyang, Daniel Rose, et al.",
    "description": "本文针对视觉语言系统在图像推理方面已取得显著成果但在视频推理领域仍缺乏深入探索的现状，提出将视频推理定义为对少数关键帧的序列化理解，旨在利用视觉语言模型的强大鲁棒性，同时减轻处理视频带来的计算复杂性。为了评估这一新应用，研究团队引入了 VIP 数据集，这是一个专门用于通过视频思维链（Video Chain-of-Thought）探索模型推理能力的推理挑战数据集。受视觉描述场景剧本的启发，研究提出了两种关键帧描述格式：非结构化的密集描述，以及识别焦点、动作、情绪、物体和环境（FAMOuS）的结构化场景描述。为进一步评估视频推理能力，研究设计了“视频填充（Video Infilling）”和“视频预测（Video Prediction）”两项任务，分别测试模型生成多个中间关键帧和预测未来关键帧的能力。通过在 VIP 数据集上对 GPT-4、GPT-3 和 VICUNA 进行基准测试，研究展示了这些模型在复杂视频推理任务中的性能差距，并鼓励未来研究优先考虑利用语言模型来实现高效且通用的视频推理。",
    "image": "",
    "code": "https://github.com/vaishnaviHimakunthala/VIP"
  },
  {
    "title": "T-SciQ: Teaching Multimodal Chain-of-Thought Reasoning via Large Language Model Signals for Science Question Answering",
    "venue": "arXiv",
    "date": "2023",
    "link": "https://arxiv.org/pdf/2305.03453.pdf",
    "source": "auto-script",
    "tags": [
      "Chain-of-Thought"
    ],
    "authors": "Lei Wang, Yi Hu, Jiabang He, et al.",
    "description": "本文提出了一种名为 T-SciQ 的新方法，旨在通过大语言模型（LLM）生成的信号来提升科学问题回答（ScienceQA）任务的性能。针对获取高质量人工标注思维链（CoT）推理逻辑耗时、昂贵且由于缺失外部关键信息而导致准确性不足的挑战，T-SciQ 利用大语言模型生成高质量的思维链推理逻辑作为教学信号，并以此训练规模更小的模型在复杂模态下进行思维链推理。此外，该研究还引入了一种全新的数据混合策略，为简单和复杂的科学问答问题生成更有效的教学数据样本。广泛的实验结果表明，T-SciQ 方法在 ScienceQA 基准测试中取得了 96.18% 的准确率，刷新了该任务的最先进性能记录，并且相比于最强大的微调基线模型，其准确率提升了 4.5%。",
    "image": "",
    "code": "https://github.com/T-SciQ/T-SciQ"
  },
  {
    "title": "Caption Anything: Interactive Image Description with Diverse Multimodal Controls",
    "venue": "arXiv",
    "date": "2023",
    "link": "https://arxiv.org/pdf/2305.02677.pdf",
    "source": "auto-script",
    "tags": [
      "Visual Reasoning",
      "Chain-of-Thought"
    ],
    "authors": "Teng Wang, Jinrui Zhang, Junjie Fei, et al.",
    "description": "本文介绍了一种名为 Caption AnyThing (CAT) 的基础模型增强型图像描述框架，旨在解决可控图像描述任务中因高质量多模态标注数据稀缺而导致模型可用性和扩展性受限的问题。CAT 充分利用了单模态指令遵循基础模型的优势，通过集成 Segment Anything Model (SAM) 和 ChatGPT，将视觉和语言提示统一到一个模块化框架中，支持包括点、框、轨迹在内的多种视觉控制，以及涉及情感、长度、语言和事实性等语言控制。这种设计实现了不同控制方式之间的灵活组合，使模型能够遵循人类意图对指定区域进行描述或以特定文本风格进行表达。广泛的案例研究证明了该框架在对齐用户意图方面的强大能力，为视觉语言应用中有效的用户交互建模提供了新的见解。",
    "image": "",
    "code": "https://github.com/ttengwang/Caption-Anything"
  },
  {
    "title": "Visual Chain of Thought: Bridging Logical Gaps with Multimodal Infillings",
    "venue": "arXiv",
    "date": "2023",
    "link": "https://arxiv.org/pdf/2305.02317.pdf",
    "source": "auto-script",
    "tags": [
      "Chain-of-Thought"
    ],
    "authors": "Daniel Rose, Vaishnavi Himakunthala, Andy Ouyang, et al.",
    "description": "本文提出了一种名为 VCoT 的创新方法，旨在解决大语言模型中思维链（CoT）推理仅局限于单模态且多应用于问答任务的局限性。针对复杂且具有想象力的任务，VCoT 通过将视觉增强引入推理过程，利用结合了视觉语言基元（vision-language grounding）的思维链提示，递归地弥合序列数据中的逻辑鸿沟。该方法通过视觉引导生成合成的多模态填充信息，为下游任务提供连贯且新颖的增强数据，不仅有助于提升需要时间推理任务的性能，还为模型的多步推理提供了可解释性。在 Visual Storytelling 和 WikiHow 摘要数据集上的应用结果及人工评估表明，VCoT 能够生成优于传统思维链基准的新颖且一致的合成数据，从而有效增强模型在下游任务中的表现。",
    "image": "",
    "code": ""
  },
  {
    "title": "Chain of Thought Prompt Tuning in Vision Language Models",
    "venue": "arXiv",
    "date": "2023",
    "link": "https://arxiv.org/pdf/2304.07919.pdf",
    "source": "auto-script",
    "tags": [
      "Chain-of-Thought"
    ],
    "authors": "Jiaxin Ge, Hongyin Luo, Siyuan Qian, et al.",
    "description": "本文介绍了一种面向视觉语言建模的新型思维链（Chain of Thought）提示微调方法，旨在解决现有语言-图像预训练研究中仅使用单一提示词而忽略了人类在处理复杂任务时逐步认知推理过程的问题。受自然语言处理领域思维链方法的启发，该研究认为有效的推理对于视觉任务同样至关重要，并首次成功实现了结合视觉与文本嵌入的思维链提示适配。广泛的实验结果表明，该方法不仅在图像分类任务中展现出更好的泛化能力，具备跨数据集的高迁移性和更强的领域泛化性能，而且在图文检索和视觉问答等对推理能力要求较高的任务中表现尤为出色。",
    "image": "",
    "code": ""
  },
  {
    "title": "Visual ChatGPT: Talking, Drawing and Editing with Visual Foundation Models",
    "venue": "arXiv",
    "date": "2023",
    "link": "https://arxiv.org/pdf/2303.04671.pdf",
    "source": "auto-script",
    "tags": [
      "Visual Reasoning",
      "Chain-of-Thought"
    ],
    "authors": "Chenfei Wu, Shengming Yin, Weizhen Qi, et al.",
    "description": "本文介绍了 Visual ChatGPT，这是一个将多种视觉基础模型（如 Visual Transformers 和 Stable Diffusion）整合进 ChatGPT 的系统，旨在克服 ChatGPT 无法处理视觉信号以及视觉基础模型仅能处理单轮固定任务的局限性。该系统通过设计一系列提示词将视觉模型信息注入 ChatGPT，使其能够处理具有多个输入输出或需要视觉反馈的模型，从而允许用户通过发送和接收图文信息与模型进行交互。Visual ChatGPT 支持用户提出需要多步操作和多个 AI 模型协作的复杂视觉问题或编辑指令，并允许用户提供反馈以获取修正后的结果。实验结果表明，借助于视觉基础模型，Visual ChatGPT 为探索 ChatGPT 在视觉领域的功能应用开辟了新途径。",
    "image": "",
    "code": "https://github.com/chenfei-wu/TaskMatrix"
  },
  {
    "title": "Multimodal Chain-of-Thought Reasoning in Language Models",
    "venue": "arXiv",
    "date": "2023",
    "link": "https://arxiv.org/pdf/2302.00923.pdf",
    "source": "auto-script",
    "tags": [
      "Chain-of-Thought"
    ],
    "authors": "Zhuosheng Zhang, Aston Zhang, Mu Li, et al.",
    "description": "本文提出了 Multimodal-CoT，这是一种将语言（文本）和视觉（图像）模态整合进统一框架的多模态思维链推理方法。针对现有思维链研究主要局限于语言模态的问题，该方法设计了一个两阶段框架，将逻辑依据（rationale）的生成与最终答案的推导进行解耦，使答案推导能够利用基于多模态信息生成的更优质的逻辑依据。在 ScienceQA 和 A-OKVQA 基准数据集上的实验结果证明了该方法的有效性，凭借 Multimodal-CoT，参数量小于 10 亿的模型在 ScienceQA 基准测试中取得了最先进的性能表现。分析表明，Multimodal-CoT 具有缓解模型幻觉并显著提升模型收敛速度的优势。",
    "image": "",
    "code": "https://github.com/amazon-science/mm-cot"
  },
  {
    "title": "Learn to Explain: Multimodal Reasoning via Thought Chains for Science Question Answering",
    "venue": "NeurIPS",
    "date": "2022",
    "link": "https://proceedings.neurips.cc/paper_files/paper/2022/file/11332b6b6cf4485b84afadb1352d3a9a-Paper-Conference.pdf",
    "source": "auto-script",
    "tags": [
      "Chain-of-Thought"
    ],
    "authors": "Pan Lu, Swaroop Mishra, Tong Xia, et al.",
    "description": "本文首先介绍了 VISPROG，这是一种无需特定任务训练的神经符号方法，它利用大语言模型的语境学习能力生成类 Python 的模块化程序，通过调用各种现成的计算机视觉模型或函数来解决复杂的组合视觉任务，并提供具有可解释性的推理逻辑。随后介绍了 PICa，该方法通过将图像转化为文本描述并利用少样本提示，将 GPT-3 视为一个隐含且非结构化的知识库，从而有效解决了知识密集型视觉问答（VQA）中外部知识库引入噪声的问题，在 OK-VQA 数据集上显著超越了之前的有监督最先进水平。最后介绍了 SCIENCEQA 基准测试，这是一个包含约 2.1 万个多模态多选科学问题的大规模数据集，通过为答案配备相应的讲解和说明，该研究成功引导语言模型模仿人类的思维链（CoT）推理过程，实验证明引入这种解释性能显著提升模型在少样本和微调设置下的问答表现。",
    "image": "",
    "code": "https://scienceqa.github.io/"
  },
  {
    "title": "VideoDeepResearch: Long Video Understanding With Agentic Tool Using",
    "venue": "arXiv",
    "date": "2025",
    "link": "https://arxiv.org/pdf/2506.10821",
    "source": "auto-script",
    "tags": [
      "Visual Reasoning"
    ],
    "authors": "Huaying Yuan, Zheng Liu, Junjie Zhou, et al.",
    "description": "本文提出了 VideoExplorer，这是一个基于“以视频进行思考”原则的长视频理解（LVU）框架，旨在解决现有方法因下采样牺牲细节或依赖任务无关表示而导致感知受阻的问题。VideoExplorer 将规划、时序定位和可扩展感知自然交织在一起，通过迭代式地提出子问题、定位相关片段并执行面向任务的理解，直到得出最终答案，从而实现了忠实、高效且具有可解释性的推理过程。针对长视频理解训练资源匮乏的挑战，研究团队利用难度自适应采样构建了一个长视频推理数据集，以确保复杂任务下轨迹的高质量。在此基础上，该框架采用两阶段训练流程，即先进行监督轨迹初始化，再进行轨迹级偏好优化，以鼓励模型在下游奖励的引导下实现自适应时序定位和迭代信息整合。在多个主流长视频理解与推理基准测试中的广泛评估证明，VideoExplorer 在鲁棒性、适配性和效率方面均较现有基准模型具有显著优势。",
    "image": "",
    "code": "https://github.com/yhy-2000/VideoDeepResearch"
  },
  {
    "title": "Beyond Embeddings: The Promise of Visual Table in Multi-Modal Models",
    "venue": "arXiv",
    "date": "2024",
    "link": "https://arxiv.org/pdf/2403.18252.pdf",
    "source": "auto-script",
    "tags": [
      "Visual Reasoning"
    ],
    "authors": "Yiwu Zhong, Zi-Yuan Hu, Michael R. Lyu, Liwei Wang",
    "description": "本文提出了一种名为 Visual Table（视觉表格）的新型视觉表示形式，专门为增强视觉推理能力而设计，旨在弥补 CLIP 等传统视觉嵌入模型在处理关键世界知识方面的不足。Visual Table 采用层次化的视觉场景描述结构，包含场景整体描述以及涵盖类别、属性和知识的多个以物体为中心的描述。得益于其结构化和文本化的格式，Visual Table 相比单纯的视觉嵌入展现出独特的优势，如具备更强的可解释性、支持可控编辑，并能提供对视觉推理至关重要的实例级世界知识与详尽属性。为了生成这种表示形式，研究团队开发了一个基于小规模标注数据训练的生成器。在 11 个视觉推理基准测试上的广泛实验结果表明，Visual Table 的表现显著优于以往的结构化或基于文本的表示方法；此外，它还能持续提升现有先进多模态大语言模型在各类基准测试中的性能，充分展示了其在推动视觉推理任务进步方面的潜力。",
    "image": "",
    "code": "https://github.com/LaVi-Lab/Visual-Table"
  },
  {
    "title": "V∗: Guided Visual Search as a Core Mechanism in Multimodal LLMs",
    "venue": "arXiv",
    "date": "2023",
    "link": "https://arxiv.org/pdf/2312.14135.pdf",
    "source": "auto-script",
    "tags": [
      "Visual Reasoning"
    ],
    "authors": "Penghao Wu, Saining Xie",
    "description": "本文介绍了 V*，这是一种由大语言模型（LLM）引导的视觉搜索机制，旨在解决当前多模态大语言模型（MLLMs）在处理高分辨率和视觉信息密集的图像时，因缺乏视觉搜索能力而难以聚焦关键视觉细节的问题。该机制利用 LLM 蕴含的世界知识进行高效的视觉查询，并与 MLLM 相结合，从而增强了协作推理、语境理解以及对特定视觉元素的精准定位能力。这种集成催生了一种名为“展示、搜索与讲述”（SEAL）的新型 MLLM 元架构。此外，研究团队还创建了 V*Bench 基准测试，专门用于评估 MLLMs 处理高分辨率图像和聚焦视觉细节的能力。本研究强调了在多模态系统中引入视觉搜索功能的必要性。",
    "image": "",
    "code": "https://github.com/penghao-wu/vstar"
  },
  {
    "title": "LLaVA-Interactive: An All-in-One Demo for Image Chat, Segmentation, Generation and Editing",
    "venue": "arXiv",
    "date": "2023",
    "link": "https://arxiv.org/pdf/2311.00571.pdf",
    "source": "auto-script",
    "tags": [
      "Visual Reasoning"
    ],
    "authors": "Wei-Ge Chen, Irina Spiridonova, Jianwei Yang, Jianfeng Gao, Chunyuan Li",
    "description": "本文介绍了 LLaVA-Interactive，这是一个用于多模态人机交互的研究原型系统，能够通过接收多模态输入并生成多模态响应与用户进行多轮对话。该系统超越了传统的文本提示方式，引入了视觉提示（visual prompt）以在交互过程中更精准地对齐人类意图。LLaVA-Interactive 的开发具有极高的成本效益，因为它无需额外的模型训练，而是直接将预构建 AI 模型的三个多模态能力相结合：LLaVA 的视觉聊天能力、SEEM 的图像分割能力以及 GLIGEN 的图像生成与编辑能力。通过展示一系列多样化的应用场景，该系统证明了其在多模态交互领域的应用前景，并为未来交互式系统的研究提供了启发。",
    "image": "",
    "code": "https://llava-vl.github.io/llava-interactive/"
  },
  {
    "title": "MM-VID: Advancing Video Understanding with GPT-4V(vision)",
    "venue": "arXiv",
    "date": "2023",
    "link": "https://arxiv.org/pdf/2310.19773.pdf",
    "source": "auto-script",
    "tags": [
      "Visual Reasoning"
    ],
    "authors": "Kevin Lin, Faisal Ahmed, Linjie Li, Chung-Ching Lin, et al.",
    "description": "本文介绍了 MM-VID，这是一个集成系统，它利用 GPT-4V 的能力并结合视觉、音频及语音领域的专业工具，旨在促进高级视频理解。该系统专门针对长篇视频带来的挑战而设计，能够处理长达一小时内容的推理以及跨多集剧情的理解等复杂任务。MM-VID 通过 GPT-4V 将视频转化为脚本，将多模态元素转录为详尽的文本描述，内容涵盖角色动作、行为、表情及对话，从而为大语言模型（LLMs）实现深度视频理解铺平了道路。这赋予了系统包括音频描述、角色识别和多模态高层级理解在内的高级功能。实验结果表明，MM-VID 在处理不同类型及不同长度的视频方面均具有显著效果；此外，研究还展示了该系统在视频游戏和图形用户界面等交互式环境中的应用潜力。",
    "image": "",
    "code": ""
  },
  {
    "title": "ControlLLM: Augment Language Models with Tools by Searching on Graphs",
    "venue": "arXiv",
    "date": "2023",
    "link": "https://arxiv.org/pdf/2310.17796.pdf",
    "source": "auto-script",
    "tags": [
      "Visual Reasoning"
    ],
    "authors": "Zhaoyang Liu, Zeqiang Lai, Zhangwei Gao, et al.",
    "description": "本文介绍了 ControlLLM，这是一个旨在使大语言模型（LLMs）能够利用多模态工具解决复杂现实任务的创新框架。针对现有 LLMs 在工具调用中面临的用户提示歧义、工具选择与参数化不准确以及工具调度低效等挑战，该框架由三个关键组件构成：首先是任务分解器，将复杂任务拆分为具有明确输入输出的子任务；其次是图上思考（Thoughts-on-Graph, ToG）范式，在预构建的工具图上搜索最优解决方案路径，该图明确了不同工具间的参数和依赖关系；最后是一个配备丰富工具箱的执行引擎，负责解析解决方案路径并高效地在不同计算设备上运行工具。通过在涉及图像、音频和视频处理的多样化任务上进行评估，实验证明 ControlLLM 与现有方法相比，在准确性、效率和通用性方面均表现出显著优势。",
    "image": "",
    "code": "https://github.com/OpenGVLab/ControlLLM"
  },
  {
    "title": "MindAgent: Emergent Gaming Interaction",
    "venue": "arXiv",
    "date": "2023",
    "link": "https://arxiv.org/pdf/2309.09971.pdf",
    "source": "auto-script",
    "tags": [
      "Visual Reasoning"
    ],
    "authors": "Ran Gong, Qiuyuan Huang, Xiaojian Ma, Hoi Vo, et al.",
    "description": "本文介绍了 MindAgent，这是一种旨在评估游戏交互中新兴规划与协作能力的全新基础设施，填补了现有游戏框架在构建涵盖大语言模型（LLM）与人类及 NPC 协作的通用多智能体协作基准方面的空白。该基础设施利用现有游戏框架，要求协调者理解多智能体系统，通过未经微调的指令与人类玩家协作，并建立基于带有反馈的少样本提示的语境学习。此外，研究团队引入了名为 CUISINEWORLD 的全新游戏场景及相关基准测试，用于调度多智能体协作效率并监督多个智能体同时进行游戏，同时提出了一种新的自动度量指标 CoS 来衡量协作效率。最后，该基础设施能够部署到定制的 VR 版 CUISINEWORLD 以及更广泛的我的世界（Minecraft）游戏领域中，研究者希望这些关于 LLM 调度与协调能力的研究结果能为如何通过大型语言语料库学习获取此类技能提供启示。",
    "image": "",
    "code": "https://mindagent.github.io/"
  },
  {
    "title": "Towards Language Models That Can See: Computer Vision Through the LENS of Natural Language",
    "venue": "arXiv",
    "date": "2023",
    "link": "https://arxiv.org/pdf/2306.16410.pdf",
    "source": "auto-script",
    "tags": [
      "Visual Reasoning"
    ],
    "authors": "William Berrios, Gautam Mittal, Tristan Thrush, Douwe Kiela, Amanpreet Singh",
    "description": "本文提出了一种名为 LENS 的模块化方法，旨在利用大语言模型（LLM）的强大能力来解决计算机视觉问题。该系统通过语言模型对一系列独立且具有高度描述性的视觉模块输出进行推理，这些模块能够提供关于图像的详尽信息。研究团队在纯计算机视觉设置（如零样本和少样本目标识别）以及视觉语言问题上对该方法进行了评估。实验发现，LENS 可以应用于任何现成的大语言模型，并且在无需任何多模态训练的情况下，配备 LENS 的大语言模型表现出极强的竞争力，足以媲美规模更大、结构更复杂的系统。",
    "image": "",
    "code": "https://github.com/ContextualAI/lens"
  },
  {
    "title": "Retrieving-to-Answer: Zero-Shot Video Question Answering with Frozen Large Language Models",
    "venue": "arXiv",
    "date": "2023",
    "link": "https://arxiv.org/pdf/2306.11732.pdf",
    "source": "auto-script",
    "tags": [
      "Visual Reasoning"
    ],
    "authors": "Junting Pan, Ziyi Lin, Yuying Ge, et al.",
    "description": "本文提出了一种简单且有效的检索式问答（R2A）框架，旨在解决现有视频问答（VideoQA）方法中跨模态对齐训练成本高昂，或描述模型领域泛化性有限的问题。对于输入的视频，R2A 首先利用预训练的多模态模型（如 CLIP）从通用文本语料库中检索出一组语义相似的文本，随后将检索到的文本与问题一并输入大语言模型（如 DeBERTa）直接生成答案。由于无需进行跨模态微调，R2A 实现了大语言模型、检索模型及文本语料库等关键组件的即插即用。在多个 VideoQA 基准测试上的广泛实验表明，即便在不进行任何微调且参数量仅为 13 亿的情况下，R2A 的表现依然超越了参数规模大其 61 倍且在近 21 亿多模态数据上进行过额外训练的 Flamingo-80B 模型。",
    "image": "",
    "code": ""
  },
  {
    "title": "AssistGPT: A General Multi-modal Assistant that can Plan, Execute, Inspect, and Learn",
    "venue": "arXiv",
    "date": "2023",
    "link": "https://arxiv.org/pdf/2306.08640.pdf",
    "source": "auto-script",
    "tags": [
      "Visual Reasoning"
    ],
    "authors": "Difei Gao, Lei Ji, Luowei Zhou, et al.",
    "description": "本文提出了一种名为 AssistGPT 的多模态 AI 助手，通过名为“计划、执行、检查与学习”（PEIL）的代码与语言交替推理方法，旨在解决大语言模型（LLMs）在处理复杂且多样化的视觉任务时面临的推理路径难以预判以及输入与中间结果形式灵活多变等挑战。在 PEIL 框架中，计划器（Planner）利用自然语言根据当前推理进度决定执行器（Executor）中下一步应调用的工具；检查器（Inspector）作为高效的记忆管理器，协助计划器将恰当的视觉信息输入特定工具；学习器（Learner）则设计用于在复杂且灵活的推理过程中使模型能够自主探索并发现最优解决方案。实验结果显示，AssistGPT 在 A-OKVQA 和 NExT-QA 基准测试上均取得了最先进的性能，且案例展示证明了该系统处理远超基准测试复杂程度问题的能力。",
    "image": "",
    "code": "https://showlab.github.io/assistgpt/"
  },
  {
    "title": "Mindstorms in Natural Language-Based Societies of Mind",
    "venue": "arXiv",
    "date": "2023",
    "link": "https://arxiv.org/pdf/2305.17066.pdf",
    "source": "auto-script",
    "tags": [
      "Visual Reasoning"
    ],
    "authors": "Mingchen Zhuge, Haozhe Liu, Francesco Faccio, Dylan R. Ashley, et al.",
    "description": "受明斯基的“心智社会”和施密德胡伯的“学习思考”启发，本文提出了一种通过“头脑风暴”式相互访谈来解决问题的多元大规模多模态神经网络（NNs）社会。现有的基于神经网络的心智社会实现方式是由大语言模型（LLMs）和其他神经网络专家通过自然语言接口进行通信，这种基于自然语言的心智社会（NLSOMs）克服了单一 LLM 的局限性，提升了多模态零样本推理能力，且能以模块化方式轻松添加使用同一种通用符号语言通信的新代理。为了验证 NLSOMs 的效能，研究团队组建并实验了成员数量多达 129 个的多个系统，利用其中的“头脑风暴”机制解决了视觉问答、图像描述、文本生成图像、3D 生成、自我中心检索、具身智能及通用语言任务解决等多种实际 AI 任务。该工作将此视为构建由数十亿代理（包括人类）组成的更大规模 NLSOMs 的起点，并探讨了随之而来的关键研究问题，包括 NLSOM 的社会结构设计（如君主制与民主制的优劣对比），以及如何利用神经网络经济学原则最大化强化学习型 NLSOM 的总奖励，并对这些问题进行了深入的讨论与初步解答。",
    "image": "",
    "code": ""
  },
  {
    "title": "LayoutGPT: Compositional Visual Planning and Generation with Large Language Models",
    "venue": "arXiv",
    "date": "2023",
    "link": "https://arxiv.org/pdf/2305.15393.pdf",
    "source": "auto-script",
    "tags": [
      "Visual Reasoning"
    ],
    "authors": "Weixi Feng, Wanrong Zhu, Tsu-jui Fu, et al.",
    "description": "本文提出了 LayoutGPT，这是一种研究如何将大语言模型（LLMs）作为视觉规划器，通过从文本条件生成布局来与视觉生成模型协作的方法。针对获取高用户可控性通常需要复杂的细粒度布局输入，从而给用户带来沉重负担的问题，LayoutGPT 采用样式表语言（style sheet language）构建语境内的视觉示例，以增强 LLMs 的视觉规划技能。LayoutGPT 能够生成涵盖 2D 图像到 3D 室内场景等多个领域的合理布局，尤其在将数值和空间关系等挑战性语言概念转化为布局排列方面表现卓越，确保了文本生成图像的忠实度。实验结果显示，当与下游图像生成模型结合时，LayoutGPT 在数值和空间准确性上的表现超过了现有文本生成图像模型 20-40%，且在布局设计上达到了与人类用户相当的水平。此外，LayoutGPT 在 3D 室内场景合成任务中也取得了与有监督方法相媲美的结果，充分证明了其在多种视觉领域中的有效性与潜力。",
    "image": "",
    "code": "https://github.com/weixi-feng/LayoutGPT"
  },
  {
    "title": "IdealGPT: Iteratively Decomposing Vision and Language Reasoning via Large Language Models",
    "venue": "arXiv",
    "date": "2023",
    "link": "https://arxiv.org/pdf/2305.14985.pdf",
    "source": "auto-script",
    "tags": [
      "Visual Reasoning"
    ],
    "authors": "Haoxuan You, Zhecan Wang, Rui Sun, et al.",
    "description": "本文提出了 IdealGPT，这是一个利用大语言模型（LLMs）迭代分解视觉语言（VL）推理任务的框架，旨在解决现有端到端预训练模型在需要多步推导的零样本推理任务中表现不足的问题。针对以往分治策略依赖特定领域分解模型以及在信息不足时强行预测答案的局限性，IdealGPT 采用了一种协同工作机制：由一个 LLM 负责生成子问题，一个视觉语言模型（VLM）提供相应的子答案，再由另一个 LLM 进行逻辑推理。这三个模块持续执行迭代式分治流程，直到模型对主问题的最终答案产生足够的置信度。在多个具有挑战性的零样本视觉语言推理任务评估中，IdealGPT 表现优异，特别是在 VCR 和 SNLI-VE 基准测试上，其准确率分别比现有的类 GPT-4 模型高出 10% 和 15%。",
    "image": "",
    "code": "https://github.com/Hxyou/IdealGPT"
  },
  {
    "title": "Accountable Textual-Visual Chat Learns to Reject Human Instructions in Image Re-creation",
    "venue": "arXiv",
    "date": "2023",
    "link": "https://arxiv.org/pdf/2303.05983.pdf",
    "source": "auto-script",
    "tags": [
      "Visual Reasoning"
    ],
    "authors": "Zhiwei Zhang, Yuliang Liu",
    "description": "本文针对目前学术界缺乏能够有效评估视觉语言模型（VLMs）在图文聊天任务中多模态生成能力的挑战，引入了两个新型多模态数据集：合成的 CLEVR-ATVC 数据集（620K）和人工拍摄的 Fruit-ATVC 数据集（50K），这两个数据集均包含视觉与文本形式的输入与输出。为了增强多模态系统在拒绝人类请求时的可解释性，研究者在数据集中引入了特定的规则作为监督信号，使训练后的模型能在进行视觉和文本推理后给出明确的“是”或“否”回答，并提供语言解释以说明无法执行指令的原因。该研究提出了一种两阶段训练方案：第一阶段采用离散变分自编码器（dVAE）将图像压缩为简明的令牌（tokens），并将其与文本令牌合并为单一数据流；第二阶段则由基于解码器的 Transformer 处理该数据流，以生成视觉再创作和文本反馈。通过对图像再创作质量、答案准确性以及模型处理不确定性和不完美查询时的行为进行全面分析，本研究旨在为构建具备可问责性的图文生成模型提供有价值的见解。",
    "image": "",
    "code": "https://matrix-alpha.github.io/"
  },
  {
    "title": "ViperGPT: Visual Inference via Python Execution for Reasoning",
    "venue": "arXiv",
    "date": "2023",
    "link": "https://arxiv.org/pdf/2303.08128.pdf",
    "source": "auto-script",
    "tags": [
      "Visual Reasoning"
    ],
    "authors": "Dídac Surís, Sachit Menon, Carl Vondrick",
    "description": "本文介绍了 ViperGPT，这是一个利用代码生成模型将视觉和语言模型组合成子程序，以解决各类视觉查询任务的框架。针对端到端模型在处理复杂视觉任务时因缺乏视觉处理与推理的明确区分而导致的可解释性和泛化性受限问题，ViperGPT 提供了一种极具前景的模块化方案。该框架通过调用提供的 API 来访问可用模块，并自动生成可执行的 Python 代码进行任务组合。这种方法无需额外的模型训练，即可在多种复杂的视觉任务中取得最先进的性能表现。",
    "image": "",
    "code": "https://viper.cs.columbia.edu/"
  },
  {
    "title": "ChatGPT Asks, BLIP-2 Answers: Automatic Questioning Towards Enriched Visual Descriptions",
    "venue": "arXiv",
    "date": "2023",
    "link": "https://arxiv.org/pdf/2303.06594.pdf",
    "source": "auto-script",
    "tags": [
      "Visual Reasoning"
    ],
    "authors": "Deyao Zhu, Jun Chen, Kilichbek Haydarov, et al.",
    "description": "本文介绍了 ChatCaptioner，这是一种应用于图像描述任务的新型自动提问方法，旨在通过挖掘大语言模型（LLMs）的主动提问能力来提升对世界知识的获取与理解。针对以往 AI 研究过度关注模型回答能力而忽略提问重要性的现状，该框架通过提示 ChatGPT 针对图像向强大的视觉问答模型 BLIP-2 提出一系列具有信息量的相关问题，并根据 BLIP-2 的回答不断获取新的视觉信息，从而生成更丰富的图像描述。在 COCO、Conceptual Caption 和 WikiArt 等数据集上的实验及人工评估结果显示，ChatCaptioner 生成的描述在信息丰富度上显著优于 BLIP-2 和人工标注的基准，其提供的信息量获得了三倍于其他模型的人工评价投票。此外，通过 WordNet 词集匹配测量，ChatCaptioner 在图像中识别出的物体数量比单独使用 BLIP-2 提升了 53%。",
    "image": "",
    "code": "https://github.com/Vision-CAIR/ChatCaptioner"
  },
  {
    "title": "Prompt, Generate, then Cache: Cascade of Foundation Models makes Strong Few-shot Learners",
    "venue": "CVPR",
    "date": "2023",
    "link": "https://arxiv.org/pdf/2303.02151.pdf",
    "source": "auto-script",
    "tags": [
      "Visual Reasoning"
    ],
    "authors": "Renrui Zhang, Xiangfei Hu, Bohao Li, et al.",
    "description": "本文提出了 CaFo，一个通过级联多种基础模型（Cascade of Foundation models）来整合多样化先验知识，以增强小样本表征学习的框架。针对在低数据量场景下如何进一步利用预训练知识辅助学习的问题，CaFo 协同集成了 CLIP 的语言对比知识、DINO 的视觉对比知识、DALL-E 的视觉生成知识以及 GPT-3 的语言生成知识。具体而言，该框架遵循“提示、生成、再缓存”的工作流程：首先利用 GPT-3 生成丰富的下游语义文本以提示 CLIP；其次通过 DALL-E 生成合成图像以在无需人力的情况下扩展小样本训练数据；最后引入一个可学习的缓存模型，自适应地融合来自 CLIP 和 DINO 的预测结果。通过这种多模型协作机制，CaFo 能够充分释放不同预训练方法的潜力，在小样本分类任务中达到了最先进的性能水平。",
    "image": "",
    "code": "https://github.com/ZrrSkywalker/CaFo"
  },
  {
    "title": "From Images to Textual Prompts: Zero-shot VQA with Frozen Large Language Models",
    "venue": "CVPR",
    "date": "2022",
    "link": "https://arxiv.org/pdf/2212.10846.pdf",
    "source": "auto-script",
    "tags": [
      "Visual Reasoning"
    ],
    "authors": "Jiaxian Guo, Junnan Li, Dongxu Li, et al.",
    "description": "本文提出了一种名为 Img2Prompt 的即插即用模块，旨在解决大语言模型（LLMs）在执行零样本视觉问答（VQA）任务时面临的模态脱节和任务脱节挑战。为了在无需昂贵的端到端训练的情况下桥接这些脱节，该模块利用与大语言模型无关的特定模型生成能够描述图像内容的提示词，并结合自构建的问答对来引导大语言模型完成任务。Img2Prompt 具有极高的灵活性，能够适配多种不同的大语言模型，并显著降低了在零样本 VQA 任务中部署大语言模型的成本。实验结果显示，该方法在性能上可媲美甚至超越依赖端到端训练的方法，例如在 VQAv2 基准测试上比 Flamingo 高出 5.6%，在极具挑战性的 A-OKVQA 数据集上，其表现甚至比部分少样本学习方法高出多达 20%。",
    "image": "",
    "code": "https://github.com/salesforce/LAVIS/tree/main/projects/img2llm-vqa"
  },
  {
    "title": "SuS-X: Training-Free Name-Only Transfer of Vision-Language Models",
    "venue": "arXiv",
    "date": "2022",
    "link": "https://arxiv.org/pdf/2211.16198.pdf",
    "source": "auto-script",
    "tags": [
      "Visual Reasoning"
    ],
    "authors": "Vishaal Udandarao, Ankush Gupta, Samuel Albanie",
    "description": "本文提出了一种名为 SuS-X 的创新方法，旨在探索在仅已知目标类别名称且无需访问目标分布图像的情况下的“仅名称迁移”（name-only transfer）模式，以解决 CLIP 模型在全量微调时资源消耗大、稳定性差的问题。SuS-X 由 SuS 和 TIP-X 两个核心组件构成，既不需要密集的微调过程，也不依赖昂贵的标注数据。实验结果显示，SuS-X 在 19 个基准数据集上均取得了最先进的零样本分类结果。此外，研究还展示了 TIP-X 在无需训练的少样本设置下的实用性，其性能同样显著超越了现有的强力无训练基准模型，实现了最先进的水平。",
    "image": "",
    "code": "https://github.com/vishaal27/SuS-X"
  },
  {
    "title": "PointCLIP V2: Adapting CLIP for Powerful 3D Open-world Learning",
    "venue": "CVPR",
    "date": "2022",
    "link": "https://arxiv.org/pdf/2211.11682.pdf",
    "source": "auto-script",
    "tags": [
      "Visual Reasoning"
    ],
    "authors": "Xiangyang Zhu, Renrui Zhang, Bowei He, et al.",
    "description": "本文提出了一种名为 PointCLIP V2 的统一 3D 开放世界学习框架，通过协同 CLIP 和 GPT 模型，成功将其在大规模预训练中获得的视觉与语言能力扩展至 3D 点云领域，涵盖了零样本 3D 分类、分割及检测任务。针对 3D 数据与预训练语言知识对齐受限的挑战，该框架设计了两个关键组件：在视觉端，通过形状投影模块提示 CLIP 生成更真实的深度图，从而缩小投影点云与自然图像之间的领域差距；在文本端，则利用 GPT 模型生成具有 3D 特征的描述性文本，作为 CLIP 文本编码器的输入。在无需任何 3D 领域训练的情况下，PointCLIP V2 在三个数据集的零样本 3D 分类任务中，准确率分别超越前作 PointCLIP 达 42.90%、40.44% 和 28.75%。此外，该框架还能以简便方式扩展至小样本 3D 分类、零样本 3D 部件分割及 3D 目标检测，充分展现了其在统一 3D 开放世界学习中的泛化能力。",
    "image": "",
    "code": "https://github.com/yangyangyang127/PointCLIP_V2"
  },
  {
    "title": "Socratic Models: Composing Zero-Shot Multimodal Reasoning with Language",
    "venue": "arXiv",
    "date": "2022",
    "link": "https://arxiv.org/pdf/2204.00598.pdf",
    "source": "auto-script",
    "tags": [
      "Visual Reasoning"
    ],
    "authors": "Andy Zeng, Maria Attarian, Brian Ichter, et al.",
    "description": "本文提出了苏格拉底模型（Socratic Models, SMs），这是一个旨在通过组合不同领域的预训练大模型（如基础模型）来协同其多样化知识的模块化框架。针对视觉语言模型（VLMs）与纯语言模型（LMs）在训练数据和常识储备上的领域差异，SMs 采用多模态启发式提示（multimodal-informed prompting）的方式，使多个模型能够在无需微调的情况下进行零样本信息交换，从而捕捉新的多模态能力。实验证明，该方法在零样本图像描述和视频文本检索任务中具有极强竞争力，并能实现一系列新兴应用，包括对自我中心视频进行自由格式问答、通过对接外部 API 和数据库与人类进行多模态辅助对话（如烹饪指导），以及在机器人感知与规划任务中的应用。",
    "image": "",
    "code": "https://socraticmodels.github.io/"
  },
  {
    "title": "VideoLLaMA 3: Frontier Multimodal Foundation Models for Image and Video Understanding",
    "venue": "arXiv",
    "date": "2025",
    "link": "https://arxiv.org/pdf/2501.13106",
    "source": "auto-script",
    "tags": [
      "Foundation Models"
    ],
    "authors": "Boqiang Zhang, Kehan Li, Zesen Cheng, et al.",
    "description": "本文提出了 VideoLLaMA3，这是一个更先进的图像和视频理解多模态基础模型，其核心设计理念在于“以视觉为中心”，这体现在训练范式和框架设计两个维度。在训练范式方面，VideoLLaMA3 强调高质量图文数据对图像和视频理解的重要性，而非单纯依赖大规模视频文本数据集，并将其训练划分为四个阶段：首先是视觉编码器适配，使其能接受变分辨率图像输入；其次是视觉语言对齐，利用涵盖场景、文档、图表的大规模图文数据及纯文本数据协同微调编码器、投影层及大语言模型；随后是多任务微调，通过引入图像和视频的指令微调（SFT）数据为视频理解奠定基础；最后是专门的以视频为中心的微调。在框架设计方面，为了捕捉图像的细粒度细节，预训练视觉编码器被适配为根据图像尺寸生成变长的视觉令牌，而对于视频输入，则根据令牌间的相似度进行压缩，从而实现更精准、更紧凑的视频表示。凭借这些以视觉为中心的设计，VideoLLaMA3 在图像和视频理解的基准测试中均取得了极具竞争力的性能表现。",
    "image": "",
    "code": "https://github.com/DAMO-NLP-SG/VideoLLaMA3"
  },
  {
    "title": "Emu3: Next-Token Prediction is All You Need",
    "venue": "arXiv",
    "date": "2024",
    "link": "https://arxiv.org/pdf/2409.18869",
    "source": "auto-script",
    "tags": [
      "Foundation Models"
    ],
    "authors": "Xinlong Wang, Xiaosong Zhang, Zhengxiong Luo, et al.",
    "description": "本文介绍了 Emu3，这是一套仅通过下一标记预测（next-token prediction）训练而成的先进多模态模型集群。针对下一标记预测在多模态任务中表现欠佳且长期被扩散模型或组合式架构主导的现状，Emu3 通过将图像、文本和视频标记化到离散空间，从零开始在混合多模态序列上训练单一的 Transformer 模型。实验结果显示，Emu3 在生成和感知任务中均优于 SDXL 和 LLaVA-1.6 等旗舰级模型，且完全无需扩散架构或复杂的组合设计。此外，该模型还能通过预测视频序列中的下一个标记生成高保真视频。通过将复杂的多模态设计简化为对标记（tokens）的单一聚焦，Emu3 释放了在训练和推理阶段进行大规模扩展的巨大潜力，证明了下一标记预测是构建超越语言的通用多模态智能的一条极具前景的路径。",
    "image": "",
    "code": "https://emu.baai.ac.cn/about"
  },
  {
    "title": "Llama 3.2: Revolutionizing edge AI and vision with open, customizable models",
    "venue": "Meta",
    "date": "2024",
    "link": "https://ai.meta.com/blog/llama-3-2-connect-2024-vision-edge-mobile-devices/",
    "source": "auto-script",
    "tags": [
      "Foundation Models"
    ],
    "authors": "Meta AI",
    "description": "Meta 推出了首个支持多模态任务的开源模型系列，重点包括具有图像理解和推理能力的 11B 及 90B 视觉模型，能够执行分析图表、文档和图像内容等复杂任务，同时还发布了专门针对移动和边缘设备优化、采用剪枝与蒸馏技术构建的 1B 和 3B 轻量化文本模型。此外，Meta 还引入了 Llama Stack 分布版以简化跨平台的开发部署流程，并推出了支持图像安全过滤的 Llama Guard 3 视觉版，旨在通过提升模型在边缘端的效率与安全性，构建更开放且灵活的 AI 生态系统。",
    "image": "",
    "code": "https://github.com/meta-llama/llama-models"
  },
  {
    "title": "Pixtral-12B",
    "venue": "Mistral",
    "date": "2024",
    "link": "https://mistral.ai/news/pixtral-12b/",
    "source": "auto-script",
    "tags": [
      "Foundation Models"
    ],
    "authors": "Mistral AI",
    "description": "Pixtral 12B 是 Mistral AI 推出的首款开源多模态大模型，它采用了 120 亿参数的多模态解码器（基于 Mistral NeMo 构建）和全新从零训练的 4 亿参数视觉编码器，能够原生处理交错的图像和文本数据，其核心优势在于支持 12.8 万标记（128k tokens）的长上下文窗口，并能以任意分辨率和长宽比输入多张图像，在 MMMU 推理基准测试中达到了 52.5% 的准确率，在理解图表、科学图示、文档问答及指令遵循等复杂任务上表现卓越，展现出超越部分更大规模模型的强大性能。",
    "image": "",
    "code": "https://github.com/mistralai/mistral-inference"
  },
  {
    "title": "xGen-MM (BLIP-3): A Family of Open Large Multimodal Models",
    "venue": "arXiv",
    "date": "2024",
    "link": "https://arxiv.org/pdf/2408.08872",
    "source": "auto-script",
    "tags": [
      "Foundation Models"
    ],
    "authors": "Le Xue, Manli Shu, Anas Awadalla, Jun Wang, An Yan, Senthil Purushwalkam, et al.",
    "description": "本文介绍了 BLIP-3，这是一个用于开发大多模态模型（LMMs）的开源框架，其内容涵盖了精心策划的数据集、训练方案、模型架构以及一系列生成的多模态模型。该框架发布了 4B 和 14B 两种参数规模的模型版本，包括预训练基座模型和经过指令微调后的模型。这些模型在包括单图和多图基准测试在内的多种任务中经过了严格评估，在同等参数规模的开源多模态模型中表现出了极强的竞争力，并具备理解交错图文输入的能力。为了进一步支持研究社区，本项目将开源全部训练代码、模型权重以及所使用的所有数据集，其中包括新创建的三个大规模数据集及预处理后的数据集。",
    "image": "",
    "code": "https://www.salesforceairesearch.com/opensource/xGen-MM/index.html"
  },
  {
    "title": "The Llama 3 Herd of Models",
    "venue": "arXiv",
    "date": "2024",
    "link": "https://arxiv.org/pdf/2407.21783",
    "source": "auto-script",
    "tags": [
      "Foundation Models"
    ],
    "authors": "Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, et al.",
    "description": "本文介绍了新一代基础模型系列 Llama 3，这是一组原生支持多语言、编程、推理和工具调用的语言模型。其中规模最大的模型是一个拥有 405B 参数、上下文窗口高达 128K 令牌的稠密 Transformer 模型。通过对 Llama 3 进行广泛的经验性评估，研究发现该模型在众多任务中表现出与 GPT-4 等领先语言模型相当的质量。Meta 官方公开发布了 Llama 3，包括 405B 参数模型的预训练和后训练版本，以及用于输入输出安全的 Llama Guard 3 模型。此外，本文还展示了通过组合方式将图像、视频和语音能力集成到 Llama 3 中的实验结果，观察到这种方法在图像、视频和语音识别任务上具有极强的竞争力，目前这些多模态模型仍处于开发阶段，尚未广泛发布。",
    "image": "",
    "code": ""
  },
  {
    "title": "Chameleon: Mixed-Modal Early-Fusion Foundation Models",
    "venue": "arXiv",
    "date": "2024",
    "link": "https://arxiv.org/pdf/2405.09818",
    "source": "auto-script",
    "tags": [
      "Foundation Models"
    ],
    "authors": "Chameleon Team",
    "description": "本文介绍了 Chameleon，这是一组采用早期融合及基于标记（token-based）设计的混合模态模型系列，能够以任意顺序理解和生成图像与文本。研究团队展示了从零开始的稳定训练方法、对齐方案以及专为早期融合混合模态设置定制的架构参数化方案。该模型在视觉问答、图像描述、文本生成、图像生成以及长篇混合模态生成等广泛任务中进行了评估。实验表明，Chameleon 展现了极其通用且广泛的能力：在单一模型中，它不仅在图像描述任务中达到了最先进水平，在纯文本任务上超越了 Llama-2 并能与 Mixtral 8x7B 和 Gemini-Pro 竞争，同时还具备出色的图像生成能力。根据针对包含图文混合序列的长篇生成任务的人类评估，Chameleon 的表现达到或超过了规模更大的模型（如 Gemini Pro 和 GPT-4V），标志着多模态文档统一建模领域迈出了重要一步。",
    "image": "",
    "code": ""
  },
  {
    "title": "Hello GPT-4o",
    "venue": "OpenAI",
    "date": "2024",
    "link": "https://openai.com/index/hello-gpt-4o/",
    "source": "auto-script",
    "tags": [
      "Foundation Models"
    ],
    "authors": "OpenAI",
    "description": "GPT-4o（其中的“o”代表“omni”，即全能）是 OpenAI 推出的全新旗舰多模态大模型，旨在实现更自然的人机交互，它能够实时处理文本、音频、图像和视频的任意组合输入，并生成相应的输出。该模型在音频输入上的平均响应时间仅为 320 毫秒，接近人类在对话中的反应速度，且在保持 GPT-4 Turbo 级别文本与代码性能的同时，显著提升了非英语语言的处理能力和视觉、音频理解效率，通过在单一神经网络中进行端到端训练，GPT-4o 克服了以往语音模式因多模型拼接导致的延迟和信息丢失问题，目前其文本和图像功能已在 ChatGPT 免费版及订阅版中上线，并同步提供给开发者 API 使用。",
    "image": "",
    "code": ""
  },
  {
    "title": "The Claude 3 Model Family: Opus, Sonnet, Haiku",
    "venue": "Anthropic",
    "date": "2024",
    "link": "https://www-cdn.anthropic.com/de8ba9b01c9ab7cbabf5c33b80b7bbc618857627/Model_Card_Claude_3.pdf",
    "source": "auto-script",
    "tags": [
      "Foundation Models"
    ],
    "authors": "Anthropic",
    "description": "该技术文档详细介绍了 Claude 3 系列模型（包括 Haiku、Sonnet 和 Opus 三款型号）的设计与评估结果，重点展示了这些模型在多模态视觉理解、长文本处理及跨多语言（如中文、法语、德语等）任务中的卓越性能，实验数据显示 Claude 3 系列不仅在 MMLU 等主流基准测试上取得了领先的成绩，还通过改进安全性对齐技术显著降低了在处理非毒性指令时的误拒率，其中旗舰模型 Opus 表现出极强的图表推理与多步逻辑计算能力，在各领域的应用评测中展现了极高的人类偏好胜率，为实现更安全、更可靠的通用人工智能提供了技术支撑 。",
    "image": "",
    "code": ""
  },
  {
    "title": "Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context",
    "venue": "Google",
    "date": "2024",
    "link": "https://storage.googleapis.com/deepmind-media/gemini/gemini_v1_5_report.pdf",
    "source": "auto-script",
    "tags": [
      "Foundation Models"
    ],
    "authors": "Google Gemini Team",
    "description": "本报告介绍了 Gemini 1.5 系列模型，这是 Google 推出的下一代具有极高计算效率的多模态模型，其核心突破在于能够处理数百万标记（tokens）的超长上下文，并能在此基础上进行细粒度的信息回忆与推理，涵盖了从长文档合集、数小时视频到长达数天的音频处理能力 。该系列包含更新后的 Gemini 1.5 Pro 和轻量化且高效的 Gemini 1.5 Flash，二者在长文本检索任务中展现出近乎完美的召回率，并在长文档问答、长视频理解及语音识别等基准测试中显著提升了当前技术水平 。研究表明，Gemini 1.5 Pro 在处理高达 1000 万标记的上下文时仍能保持卓越的召回性能，实现了超越同时期如 Claude 3 或 GPT-4 Turbo 等模型的跨代飞跃，同时在无需额外多模态训练的情况下，仅通过上下文学习（In-context Learning）即可掌握极其稀少的语言翻译技能，充分证明了其在处理复杂、海量非结构化数据方面的巨大潜力 。",
    "image": "",
    "code": ""
  },
  {
    "title": "Fuyu-8B: A Multimodal Architecture for AI Agents",
    "venue": "Blog",
    "date": "2023",
    "link": "https://www.adept.ai/blog/fuyu-8b",
    "source": "auto-script",
    "tags": [
      "Foundation Models"
    ],
    "authors": "Rohan Bavishi, Erich Elsen, Curtis Hawthorne, et al.",
    "description": "Fuyu-8B 是由 Adept AI 推出的一款轻量级多模态模型，其核心突破在于采用了一种极简的“纯解码器”（decoder-only）Transformer 架构，完全去掉了专门的视觉编码器，而是通过将图像块直接线性投影到模型的第一层，从而实现了对任意分辨率图像的原生支持。该模型专为数字代理（Digital Agents）设计，能够高效处理图表理解、UI 界面问答及屏幕元素的精细定位等任务，在保持推理速度极快（大图响应时间小于 100 毫秒）的同时，在 VQAv2 和 AI2D 等标准视觉基准测试中展现出媲美大规模模型的性能，为构建能够理解复杂视觉上下文并执行自动化操作的通用人工智能助手提供了高效的架构方案。",
    "image": "",
    "code": "https://huggingface.co/adept/fuyu-8b"
  },
  {
    "title": "Unified Model for Image, Video, Audio and Language Tasks",
    "venue": "arXiv",
    "date": "2023",
    "link": "https://arxiv.org/pdf/2307.16184.pdf",
    "source": "auto-script",
    "tags": [
      "Foundation Models"
    ],
    "authors": "Mustafa Shukor, Corentin Dancette, Alexandre Rame, Matthieu Cord",
    "description": "本文提出了一种名为 UnIVAL 的统一多模态模型，旨在通过单一框架整合文本、图像、视频和音频四种模态，从而解决大语言模型（LLMs）在处理多样化任务时面临的模态脱节挑战。不同于以往依赖数以十亿计参数量或海量数据集的大型模型（如 Flamingo），UnIVAL 仅包含约 2.5 亿（0.25B）参数，却展现出了超越仅支持双模态（如仅图文或视频文本）的传统中小规模统一模型的泛化能力。该模型采用了一种高效的预训练策略，通过任务平衡和多模态课程学习（Multimodal Curriculum Learning）在多种任务上进行训练，在图像和视频文本任务中均取得了极具竞争力的性能表现。令人关注的是，尽管 UnIVAL 在预训练阶段并未直接接触音频数据，但其从图像和视频文本模态中学习到的特征表征，使其在经过音频文本任务微调后，依然能够达到与当前最先进技术（SoTA）相媲美的水平。此外，基于其统一架构的优势，研究者还提出了一种新型的多模态模型合并方法，通过对在不同多模态任务上训练的模型进行权重插值，有效提升了模型的分布外（OOD）泛化能力，并证明了不同任务之间存在显著的协同效应。",
    "image": "",
    "code": "https://github.com/mshukor/UnIVAL"
  },
  {
    "title": "PaLI-3 Vision Language Models: Smaller, Faster, Stronger",
    "venue": "arXiv",
    "date": "2023",
    "link": "https://arxiv.org/pdf/2310.09199.pdf",
    "source": "auto-script",
    "tags": [
      "Foundation Models"
    ],
    "authors": "Xi Chen, Xiao Wang, Lucas Beyer, et al.",
    "description": "本文介绍了 PaLI-3，这是一个更小、更快且更强劲的视觉语言模型（VLM），其性能足以媲美参数规模大其 10 倍的同类模型。为了实现这一卓越表现，研究者对比了使用分类目标预训练的视觉 Transformer（ViT）模型与使用对比学习目标（SigLIP）预训练的模型，发现基于 SigLIP 的 PaLI 虽然在标准图像分类基准上表现略逊，但在各种多模态基准测试中展现出更优越的性能，尤其是在定位和视觉场景下的文本理解任务中表现突出。通过将 SigLIP 图像编码器扩展至 20 亿参数，该模型在多语言跨模态检索任务中取得了新的最先进水平。尽管 PaLI-3 的总参数量仅为 50 亿，但研究者希望它能重新激发对复杂视觉语言模型基础组件的研究，并为新一代更大规模模型的开发提供动力。",
    "image": "",
    "code": ""
  },
  {
    "title": "GPT-4V(ision) System Card",
    "venue": "OpenAI",
    "date": "2023",
    "link": "https://cdn.openai.com/papers/GPTV_System_Card.pdf",
    "source": "auto-script",
    "tags": [
      "Foundation Models"
    ],
    "authors": "OpenAI",
    "description": "该系统说明书详细记录了 OpenAI 在发布具备视觉能力的 GPT-4（即 GPT-4V）之前所进行的安全性评估、准备工作及风险缓解措施，重点分析了多模态大模型在引入图像输入后所面临的新型挑战，如人脸识别隐私、敏感特征归因、不当推理以及通过图像进行的“越狱”攻击等风险。文档通过与早期测试用户（如 Be My Eyes）的合作反馈，以及广泛的外部专家“红队”测试，揭示了模型在医疗诊断、科学预测、仇恨言论过滤和地理定位等高风险领域的局限性与不确定性，并阐述了 OpenAI 如何通过强化学习（RLHF）、系统级 Moderation 过滤及特定拒绝机制（针对人名识别和无根据推断的拒绝率接近 100%）来确保模型在正式发布前的鲁棒性与安全性，旨在为多模态智能系统的负责任部署提供透明的技术参考。",
    "image": "",
    "code": ""
  },
  {
    "title": "Unified Language-Vision Pretraining in LLM with Dynamic Discrete Visual Tokenization",
    "venue": "arXiv",
    "date": "2023",
    "link": "https://arxiv.org/pdf/2309.04669.pdf",
    "source": "auto-script",
    "tags": [
      "Foundation Models"
    ],
    "authors": "Yang Jin, Kun Xu, Kun Xu, Liwei Chen, Chao Liao, Jianchao Tan, et al.",
    "description": "本文介绍了一种名为 LaVIT（Language-VIsion Transformer）的多模态基础模型，旨在解决现有大语言模型（LLM）在处理视觉任务时过度依赖视觉提示词而忽略视觉模态自身潜力的局限性。研究团队通过将视觉和语言表示为统一形式实现了突破，具体而言，LaVIT 引入了一种精心设计的视觉分词器（Visual Tokenizer），能够将非语言类的图像转化为类似于“外语”的一系列离散令牌（discrete tokens），使 LLM 能够像阅读文本一样直接读取视觉信息。这些生成的视觉令牌不仅包含了相当于单词级别的语义，还支持根据图像内容动态调整序列长度，从而减少信息冗余。基于这种统一的生成式学习范式，LaVIT 可以无差别地处理图像和文本，使其成为一个能够同时理解和生成多模态内容的通用智能接口。大量实验表明，LaVIT 在广泛的视觉语言任务中表现卓越，以显著优势超越了现有的多模态模型。",
    "image": "",
    "code": "https://github.com/jy0205/LaVIT"
  },
  {
    "title": "Multimodal Foundation Models: From Specialists to General-Purpose Assistants",
    "venue": "arXiv",
    "date": "2023",
    "link": "https://browse.arxiv.org/pdf/2309.10020.pdf",
    "source": "auto-script",
    "tags": [
      "Foundation Models"
    ],
    "authors": "Chunyuan Li, Zhe Gan, Zhengyuan Yang, et al.",
    "description": "本文对具有视觉及视觉语言能力的多模态基础模型的分类与演进进行了全面的综述，重点探讨了模型从特定任务专家向通用助手转型的过程。该研究领域涵盖了两大类共五个核心主题：首先综述了发展成熟的特定用途预训练多模态基础模型，包括用于视觉理解的视觉主干网络学习方法以及文本生成图像技术；随后重点介绍了旨在发挥通用助手作用的前沿探索性研究，包括受大语言模型启发的统一视觉模型、端到端训练的多模态大语言模型，以及将多模态工具与大语言模型进行链式组合的方法。本文旨在为计算机视觉和视觉语言多模态社区中渴望掌握多模态基础模型基础知识与最新进展的研究人员、研究生及专业人士提供参考。",
    "image": "",
    "code": ""
  },
  {
    "title": "Bootstrapping Vision-Language Learning with Decoupled Language Pre-training",
    "venue": "NeurIPS",
    "date": "2023",
    "link": "https://arxiv.org/pdf/2307.07063.pdf",
    "source": "auto-script",
    "tags": [
      "Foundation Models"
    ],
    "authors": "Yiren Jian, Chongyang Gao, Soroush Vosoughi",
    "description": "本文提出了一种旨在优化冻结的大语言模型（LLMs）在资源密集型视觉语言（VL）预训练中应用的新方法。现有的主流范式通常将视觉特征作为引导语言模型的提示词，重点关注如何为对应文本确定最相关的视觉特征；而本研究另辟蹊径，将核心转向语言部分，致力于识别能与视觉特征精准对齐的最佳文本提示词。为此，研究者引入了提示词转换器（P-Former），该模型专门在语言数据上进行训练以预测这些理想提示词，从而无需依赖图文配对数据，并巧妙地将端到端的视觉语言训练过程分化为一个额外的独立阶段。实验结果表明，该框架显著增强了强大的图像转文本基准模型（BLIP-2）的性能，并有效缩小了使用 400 万与 1.29 亿图文对训练的模型之间的性能差距。更重要的是，该框架具有模态无关性且架构设计灵活，已通过在不同基础模块的视频学习任务中的成功应用得到了验证。",
    "image": "",
    "code": "https://github.com/yiren-jian/BLIText"
  },
  {
    "title": "Kosmos-2: Grounding Multimodal Large Language Models to the World",
    "venue": "arXiv",
    "date": "2023",
    "link": "https://arxiv.org/pdf/2306.14824.pdf",
    "source": "auto-script",
    "tags": [
      "Foundation Models"
    ],
    "authors": "Zhiliang Peng, Wenhui Wang, Li Dong, et al.",
    "description": "本文介绍了多模态大语言模型 Kosmos-2，该模型赋予了模型感知物体描述（如边界框）以及将文本与视觉世界进行基准对齐（grounding）的新能力。具体而言，研究者将指代表达式表示为 Markdown 格式的链接，其中物体描述被转化为一系列位置令牌，并利用构建的大规模对齐图像-文本对数据集（GrIT）与多模态语料库共同对模型进行训练。除了具备感知通用模态、遵循指令和上下文学习等现有能力外，Kosmos-2 进一步将基准对齐能力集成到下游应用中，在多模态对齐（如指代理解、短语对齐）、多模态指代生成、感知语言任务以及纯语言理解与生成等广泛任务中均表现出色。这项工作为具身人工智能的发展奠定了基础，并揭示了语言、多模态感知、行动与世界建模大融合的趋势，是通往通用人工智能的关键一步。",
    "image": "",
    "code": "https://github.com/microsoft/unilm/tree/master/kosmos-2"
  },
  {
    "title": "Transfer Visual Prompt Generator across LLMs",
    "venue": "arXiv",
    "date": "2023",
    "link": "https://arxiv.org/pdf/2305.01278.pdf",
    "source": "auto-script",
    "tags": [
      "Foundation Models"
    ],
    "authors": "Ao Zhang, Hao Fei, Yuan Yao, et al.",
    "description": "本文提出了一个名为 VPGTrans 的两阶段传输框架，旨在解决开发新型多模态大语言模型（MLLM）时，视觉提示生成器（VPG）训练极度耗费资源的问题。针对现有范式中连接大语言模型与 VPG 仍需大量计算成本和数据的现状，该研究首次探讨了 VPG 在不同参数规模（如从小到大）及不同类型的大语言模型之间的可迁移性，并诊断出了提升传输效率的关键因素。实验结果证明，VPGTrans 能够显著加速迁移学习过程且不损失模型性能，例如在将 BLIP-2 的 VPG 从 OPT 2.7B 迁移至 OPT 6.7B 时，与从零开始训练相比，该方法实现了超过 10 倍的加速，且仅需 10.7% 的训练数据。此外，作者还提供了关于 VPG 传输的一系列发现及其背后的原理解释，并通过结合最新的 Llama 和 Vicuna 模型定制出 VL-LLaMA 和 VL-Vicuna 两个新型多模态大模型，充分展示了 VPGTrans 的实际应用价值。",
    "image": "",
    "code": "https://vpgtrans.github.io/"
  },
  {
    "title": "GPT-4 Technical Report",
    "venue": "arXiv",
    "date": "2023",
    "link": "https://arxiv.org/pdf/2303.08774.pdf",
    "source": "auto-script",
    "tags": [
      "Foundation Models"
    ],
    "authors": "OpenAI",
    "description": "本文介绍了 GPT-4 的开发情况，这是一个能够接收图像和文本输入并生成文本输出的大规模多模态模型。尽管在许多现实世界场景中其能力仍逊于人类，但 GPT-4 在多种专业和学术基准测试中展现了人类水平的表现，例如在模拟律师资格考试中其成绩位列应试者的前 10% 左右。作为一个基于 Transformer 的模型，GPT-4 最初被预训练用于预测文档中的下一个令牌，而随后的后训练对齐过程则提升了其在事实性及遵循预期行为方面的表现。该项目的核心组成部分是开发了在各种规模下均表现可预测的基础设施和优化方法，这使得研究团队能够基于计算量不足其千分之一的模型，准确预测 GPT-4 在某些方面的性能表现。",
    "image": "",
    "code": ""
  },
  {
    "title": "PaLM-E: An Embodied Multimodal Language Model",
    "venue": "arXiv",
    "date": "2023",
    "link": "https://arxiv.org/pdf/2303.03378.pdf",
    "source": "auto-script",
    "tags": [
      "Foundation Models"
    ],
    "authors": "Danny Driess, Fei Xia, Mehdi S. M. Sajjadi, Corey Lynch, et al.",
    "description": "本文提出了具身智能语言模型 PaLM-E，旨在通过将真实的连续传感器模态直接整合进大语言模型，解决通用推理在机器人等现实世界应用中的基准对齐（grounding）挑战。该模型将视觉、连续状态估计和文本输入编码交织成多模态句子作为输入，并与预训练的大语言模型进行端到端联合训练，使其能够处理机器人操作规划、视觉问答及图像描述等多种具身任务。评估结果表明，PaLM-E 作为一个单一的大规模具身多模态模型，不仅能处理跨多种观测模态和不同机器人的具身推理任务，还展现出显著的正向迁移效应，即模型能够从互联网规模的语言、视觉及视觉语言领域的多元联合训练中获益。其中参数量达 5620 亿（562B）的最大规模版本 PaLM-E-562B，不仅在机器人任务上表现卓越，还作为视觉语言通用模型在 OK-VQA 基准上取得了最先进的性能，并随着规模扩大持续保持了强大的通用语言能力。",
    "image": "",
    "code": ""
  },
  {
    "title": "Prismer: A Vision-Language Model with An Ensemble of Experts",
    "venue": "arXiv",
    "date": "2023",
    "link": "https://arxiv.org/pdf/2303.02506.pdf",
    "source": "auto-script",
    "tags": [
      "Foundation Models"
    ],
    "authors": "Shikun Liu, Linxi Fan, Edward Johns, et al.",
    "description": "本文介绍了一种名为 Prismer 的视觉语言模型，旨在解决传统多模态模型对海量数据和超大规模参数训练的过度依赖，提供一种更具扩展性的替代方案 。Prismer 的核心理念是利用由多个特定任务专家组成的集成架构，其大部分网络权重直接继承自现有的、预训练好的领域专家模型，并在训练期间保持冻结，仅需训练极少数的组件即可实现数据与参数的高效利用 。通过整合来自广泛领域的专家知识，Prismer 能够有效地汇聚这些信息并将其适配到各种视觉语言推理任务中 。实验结果表明，Prismer 在微调和小样本学习方面的表现可与当前的先进模型相媲美，同时所需的训练数据量比同类模型减少了多达两个数量级 。",
    "image": "",
    "code": "https://shikun.io/projects/prismer"
  },
  {
    "title": "Language Is Not All You Need: Aligning Perception with Language Models",
    "venue": "arXiv",
    "date": "2023",
    "link": "https://arxiv.org/pdf/2302.14045.pdf",
    "source": "auto-script",
    "tags": [
      "Foundation Models"
    ],
    "authors": "Shaohan Huang, Li Dong, Wenhui Wang, et al.",
    "description": "本文介绍了多模态大语言模型 Kosmos-1，该模型旨在通过融合语言、多模态感知、行动与世界建模，迈向通用人工智能的关键一步。研究团队从零开始，利用包括随机交错的图文数据、图像-说明对以及纯文本数据在内的网络规模多模态语料库对 Kosmos-1 进行训练，使其具备感知通用模态、进行上下文学习（即少样本学习）以及遵循指令（即零样本学习）的能力。在无需任何梯度更新或微调的情况下，Kosmos-1 在多种评估设置（如零样本、少样本及多模态思维链提示）下展现了卓越的性能，涵盖了语言理解与生成、无需光学字符识别的自然语言处理（直接处理文档图像）、多模态对话、图像描述、视觉问答，以及通过文本指令指定分类的图像识别等视觉任务。此外，该研究证明了多模态大语言模型能够从跨模态迁移中获益，实现语言与多模态知识的双向迁移，并引入了瑞文推理测验（Raven IQ test）数据集，用以诊断模型在非语言推理方面的能力。",
    "image": "",
    "code": "https://github.com/microsoft/unilm"
  },
  {
    "title": "BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models",
    "venue": "arXiv",
    "date": "2023",
    "link": "https://arxiv.org/pdf/2301.12597.pdf",
    "source": "auto-script",
    "tags": [
      "Foundation Models"
    ],
    "authors": "Junnan Li, Dongxu Li, Silvio Savarese, Steven Hoi",
    "description": "本文提出了 BLIP-2，一种通用且高效的预训练策略，旨在解决由于大规模模型端到端训练导致视觉语言预训练成本日益高昂的问题。该方法通过一个轻量级的查询转换器（Querying Transformer）作为桥梁，利用现成的冻结图像编码器和冻结大语言模型来引导视觉语言预训练。其预训练过程分为两个阶段：第一阶段从冻结的图像编码器中引导视觉语言表示学习，第二阶段则从冻结的语言模型中引导视觉到语言的生成式学习。尽管 BLIP-2 的可训练参数显著少于现有方法，但它在多项视觉语言任务上均取得了最先进的性能，例如在零样本 VQAv2 任务上，其性能超越了参数量大其 54 倍的 Flamingo80B 达 8.7%。此外，该研究还展示了模型在零样本图像到文本生成方面的能力，并能够遵循自然语言指令执行任务。",
    "image": "",
    "code": "https://github.com/salesforce/LAVIS/tree/main/projects/blip2"
  },
  {
    "title": "VIMA: General Robot Manipulation with Multimodal Prompts",
    "venue": "ICML",
    "date": "2022",
    "link": "https://arxiv.org/pdf/2210.03094.pdf",
    "source": "auto-script",
    "tags": [
      "Foundation Models"
    ],
    "authors": "Yunfan Jiang, Agrim Gupta, Zichen Zhang, et al.",
    "description": "本文介绍了一种名为 VIMA 的新型机器人智能体，旨在通过统一的多模态提示词框架来解决机器人任务描述多样化（如模仿单次演示、遵循语言指令或达成视觉目标）所带来的挑战。研究表明，通过交替使用文本和视觉令牌，可以利用多模态提示词来表达极其广泛的机器人操控任务。为此，研究团队开发了一个全新的模拟基准测试，其中包含数千个程序生成的桌面任务、超过 60 万条用于模仿学习的专家轨迹，以及一套用于系统泛化评估的四级协议。VIMA 采用基于 Transformer 的架构，能够自动回归地处理这些提示词并输出动作指令，其独特的设计配方使其具备极强的模型可扩展性与数据效率。实验结果显示，在最具挑战性的零样本泛化设置下，VIMA 在相同训练数据量下的任务成功率是其他设计方案的 2.9 倍；即使训练数据量减少 10 倍，其表现依然比最强的竞争变体高出 2.7 倍。",
    "image": "",
    "code": "https://vimalabs.github.io/"
  },
  {
    "title": "MineDojo: Building Open-Ended Embodied Agents with Internet-Scale Knowledge",
    "venue": "NeurIPS",
    "date": "2022",
    "link": "https://arxiv.org/pdf/2206.08853.pdf",
    "source": "auto-script",
    "tags": [
      "Foundation Models"
    ],
    "authors": "Linxi Fan, Guanzhi Wang, Yunfan Jiang, et al.",
    "description": "本文介绍了 MineDojo，这是一个基于《我的世界》（Minecraft）游戏构建的新型框架，旨在突破传统自主智能体在受限、单一且需手动设计目标的孤立环境中学习的局限性，进而推动通往通用具身智能体的研究。研究团队提出了构建通用智能体的三大核心要素：一个支持海量任务与目标的环境、大规模多模态知识数据库以及灵活且可扩展的智能体架构。MineDojo 包含一个拥有数千个多样化开放式任务的模拟套件，以及一个涵盖了视频、教程、百科页面及论坛讨论的互联网规模知识库。基于这些丰富的数据，本文提出了一种创新的智能体学习算法，利用大规模预训练的视觉语言模型作为习得的奖励函数，使智能体能够根据自由格式的语言指令解决各种开放式任务，而无需任何人工设计的稠密奖励塑形。目前，MineDojo 的模拟套件、知识库、算法实现及预训练模型均已开源，以促进具身智能领域的相关研究。",
    "image": "",
    "code": "https://minedojo.org/"
  },
  {
    "title": "Write and Paint: Generative Vision-Language Models are Unified Modal Learners",
    "venue": "ICLR",
    "date": "2022",
    "link": "https://arxiv.org/pdf/2206.07699.pdf",
    "source": "auto-script",
    "tags": [
      "Foundation Models"
    ],
    "authors": "Shizhe Diao, Wangchunshu Zhou, Xinsong Zhang, Jiawei Wang",
    "description": "本文介绍了 DaVinci，这是一个新型的统一模态基础模型，旨在探索对称生成式视觉语言预训练在同时学习“写作”（图像到文本生成）与“绘画”（文本到图像生成）任务中的巨大潜力。研究团队提出了一种简洁的生成式自监督目标，即在前缀多模态建模框架下进行前缀语言建模和前缀图像建模，使得 DaVinci 不仅易于训练且能够扩展至海量数据，同时还能灵活适配写作、绘画以及其他视觉、文本和多模态理解任务。实验结果表明，DaVinci 在涵盖生成与理解的 27 项广泛任务中均取得了极具竞争力的表现，充分证明了将视觉与语言生成式预训练相结合的优越性。此外，该研究还针对不同规模及广域分布的预训练数据集，对多种视觉语言预训练目标进行了深入的基准测试，揭示了利用语言和视觉输入进行双向自监督学习的潜力，并为不同数据规模下的未来研究建立了更强大的新基准。",
    "image": "",
    "code": "https://github.com/shizhediao/DaVinci"
  },
  {
    "title": "Language Models are General-Purpose Interfaces",
    "venue": "arXiv",
    "date": "2022",
    "link": "https://arxiv.org/pdf/2206.06336.pdf",
    "source": "auto-script",
    "tags": [
      "Foundation Models"
    ],
    "authors": "Yaru Hao, Haoyu Song, Li Dong, et al.",
    "description": "本文提出将语言模型作为连接各类基础模型的通用接口，通过一组预训练编码器感知视觉、语言等多种模态，并将其接入作为通用任务层的语言模型中。研究团队提出了一种半因果语言建模（semi-causal language modeling）目标，用于联合预训练接口与模块化编码器，从而融合了因果与非因果建模的优势。该方法不仅继承了因果语言建模在上下文学习和开放式生成方面的能力，还通过双向编码器显著提升了微调效果。更重要的是，该架构实现了多种能力的无缝组合，例如能够在使用微调编码器的同时进行上下文学习或指令遵循。在多项纯语言和视觉语言基准测试中的实验结果表明，该模型在微调、零样本泛化以及少样本学习任务上，表现均优于或可媲美现有的专门化模型。",
    "image": "",
    "code": "https://github.com/microsoft/unilm"
  },
  {
    "title": "Thinking in Space: How Multimodal Large Language Models See, Remember, and Recall Spaces",
    "venue": "arXiv",
    "date": "2024",
    "link": "https://arxiv.org/pdf/2412.14171",
    "source": "auto-script",
    "tags": [
      "Evaluation"
    ],
    "authors": "Jihan Yang, Shusheng Yang, Anjali W. Gupta, et al.",
    "description": "本文介绍了一个名为 VSI-Bench 的新型视频视觉空间智能基准测试，该基准包含超过 5,000 个问答对，旨在探究在百万级视频数据集上训练的多模态大语言模型（MLLMs）是否具备像人类一样从连续视觉观察中记忆空间并进行“空间思维”的能力。研究发现，尽管 MLLMs 展现出了极具竞争力的视觉空间智能，但其表现仍低于人类水平。通过要求模型以语言和视觉两种方式表达其空间思维过程，研究者发现，虽然空间推理能力仍是限制模型提升基准测试成绩的主要瓶颈，但模型内部确实涌现出了局部世界模型和空间意识。值得注意的是，现有的主流语言推理技术（如思维链、自一致性和思维树）未能有效提升模型在该基准上的表现，而如果在问答过程中显式地生成认知地图（cognitive maps），则能显著增强 MLLMs 的空间距离感知能力。",
    "image": "",
    "code": "https://vision-x-nyu.github.io/thinking-in-space.github.io/"
  },
  {
    "title": "MMGenBench: Evaluating the Limits of LMMs from the Text-to-Image Generation Perspective",
    "venue": "arXiv",
    "date": "2024",
    "link": "https://arxiv.org/pdf/2411.14062",
    "source": "auto-script",
    "tags": [
      "Evaluation"
    ],
    "authors": "Hailang Huang, Yong Wang, Zixuan Huang, et al.",
    "description": "本文提出了一种名为 MMGenBench-Pipeline 的简单且全自动化的评估管线，旨在解决当前大型多模态模型（LMMs）基准测试中存在的构建成本高、领域局限性大以及难以评估模型生成详细图像描述能力等问题。该管线通过将输入图像转化为文本描述，再利用文生图模型根据该描述生成辅助图像，最后对比原始图像与生成图像来实现自动化评估。为了验证该管线的有效性，研究者设计了涵盖 13 种不同图像模式的 MMGenBench-Test 和侧重于生成图像表现的 MMGenBench-Domain。针对超过 50 种主流 LMMs 的深入评估显示，许多在现有基准测试中表现优异的模型，在完成基础的图像理解和详细描述任务时却表现欠佳，这揭示了当前模型在性能提升上的巨大潜力及未来的优化方向。同时，MMGenBench-Pipeline 能够仅凭借图像输入，高效地评估 LMMs 在不同领域下的综合性能。",
    "image": "",
    "code": "https://github.com/lerogo/MMGenBench"
  },
  {
    "title": "OmniBench: Towards The Future of Universal Omni-Language Models",
    "venue": "arXiv",
    "date": "2024",
    "link": "https://arxiv.org/pdf/2409.15272",
    "source": "auto-script",
    "tags": [
      "Evaluation"
    ],
    "authors": "Yizhi Li, Yinghao Ma, Ge Zhang, Ruibin Yuan, Kang Zhu, Hangyu Guo, et al.",
    "description": "本文介绍了 OmniBench，这是一个旨在严格评估模型同时识别、解析和推理视觉、听觉及文本输入能力的创新基准测试 。研究者将具备这种三模态处理能力的模型定义为全模态语言模型（Omni-Language Models, OLMs） 。OmniBench 以高质量的人工标注为特色，确保模型只有在整合理解并推理所有三种模态信息时才能给出准确答案 。研究的主要发现表明，现有的开源 OLMs 在三模态语境下的指令遵循和推理能力存在严重局限，且大多数基准模型即使在获得图像或音频的替代文本表示时，表现依然不佳（准确率低于 50%），这反映出当前多模态大语言模型的训练范式往往忽视了从文本、图像和音频中构建一致上下文的能力 。为了填补这一空白，研究团队策划了一个包含 8.45 万个训练样本的指令微调数据集 OmniInstruct，用于训练 OLMs 适配三模态语境，并呼吁未来的研究应侧重于开发更稳健的三模态整合技术和训练策略，以增强 OLMs 的性能 。",
    "image": "",
    "code": "https://m-a-p.ai/OmniBench/"
  },
  {
    "title": "MME-RealWorld: Could Your Multimodal LLM Challenge High-Resolution Real-World Scenarios that are Difficult for Humans?",
    "venue": "arXiv",
    "date": "2024",
    "link": "https://arxiv.org/pdf/2408.13257",
    "source": "auto-script",
    "tags": [
      "Evaluation"
    ],
    "authors": "Yi-Fan Zhang, Huanyu Zhang, Haochen Tian, Chaoyou Fu, et al.",
    "description": "本文介绍了 MME-RealWorld，这是一个旨在解决当前多模态大语言模型（MLLMs）基准测试中存在的样本规模小、标注质量受限以及任务难度不足（尤其是受限于图像分辨率）等问题的全新评估基准。研究团队从公开数据集和互联网收集了超过 30 万张图像，并从中筛选出 13,366 张高质量图像，由 25 位专业标注员和 7 位多模态专家共同参与，贡献了 29,429 个问答对，涵盖了 5 个真实世界场景下的 43 个子任务，其难度甚至对人类也极具挑战性。据已知信息，MME-RealWorld 是目前规模最大的纯人工标注基准测试，具有极高的图像分辨率并专注于实际应用场景。通过对包括 GPT-4o、Gemini 1.5 Pro 和 Claude 3.5 Sonnet 在内的 28 个主流多模态大模型进行深入评估，结果显示即使是最先进的模型在该基准上的准确率也均未达到 60%，这表明感知高分辨率图像和理解复杂的现实世界场景仍是当前多模态领域亟待解决的挑战。",
    "image": "",
    "code": "https://mme-realworld.github.io/"
  },
  {
    "title": "UNK-VQA: A Dataset and a Probe into the Abstention Ability of Multi-modal Large Models",
    "venue": "TPAMI",
    "date": "2023",
    "link": "https://arxiv.org/pdf/2310.10942",
    "source": "auto-script",
    "tags": [
      "Evaluation"
    ],
    "authors": "Yangyang Guo, Fangkai Jiao, Zhiqi Shen, Liqiang Nie, Mohan Kankanhalli",
    "description": "本文旨在通过构建一个名为 UNK-VQA 的全面数据集，弥补现有视觉问答（VQA）研究在处理不可回答问题方面的不足，从而推动构建更具可信度的人工智能系统。该数据集专为应对模型“不知道”的问题而设计，研究者通过对原始图像或问题进行刻意的扰动来扩充数据，并确保扰动后的图文语义仍与原始分布保持接近，这种设计使得识别不可回答的问题变得极具挑战性，从而区别于仅进行简单图像替换的现有数据集。通过对多种新兴多模态大模型在零样本和少样本设置下的表现进行广泛评估，研究发现这些模型在处理该数据集时存在显著局限。此外，本文还提出了一种处理不可回答问题的直观方法，该数据集有望成为增强 VQA 模型弃权（abstention）能力的重要基准，进而提升 AI 系统的可信赖程度。",
    "image": "",
    "code": "https://github.com/guoyang9/UNK-VQA"
  },
  {
    "title": "MMEvalPro: Calibrating Multimodal Benchmarks Towards Trustworthy and Efficient Evaluation",
    "venue": "arXiv",
    "date": "2024",
    "link": "https://arxiv.org/pdf/2407.00468",
    "source": "auto-script",
    "tags": [
      "Evaluation"
    ],
    "authors": "Jinsheng Huang, Liang Chen, Taian Guo, et al.",
    "description": "本文介绍了 MMEvalPro，这是一个旨在解决当前大型多模态模型（LMMs）基准测试中存在的系统性偏差问题的新型评估基准，特别是针对那些即使不具备视觉感知能力的纯语言模型（LLMs）也能在多选题测试中取得不俗表现、从而削弱评估公信力的现象。为了在保持多选题评估效率的同时避免第一类错误（Type-I errors），研究者提出了一个由“三部曲”评估管线和更严格指标构成的框架，通过人工标注为每个原始问题增设一个感知问题和一个知识锚点问题，最终形成了包含 2,138 个问题三元组、共计 6,414 个不同问题的基准。实验结果表明，与现有基准（如 MMMU、ScienceQA 和 MathVista）相比，MMEvalPro 具有更高的挑战性与可信度：最先进的 LMM 与人类表现的差距从传统基准的平均 8.03% 扩大到了 31.73%，而最优 LLM 与最优 LMM 之间的性能差距也从 14.64% 拉大到了 23.09%。通过深入分析模型性能差异的原因，该研究验证了 MMEvalPro 评估方案的公信力，并强调了其在推动未来多模态研究方面的巨大潜力。",
    "image": "",
    "code": "https://github.com/chenllliang/MMEvalPro"
  },
  {
    "title": "Web2Code: A Large-scale Webpage-to-Code Dataset and Evaluation Framework for Multimodal LLMs",
    "venue": "arXiv",
    "date": "2024",
    "link": "https://arxiv.org/pdf/2406.20098",
    "source": "auto-script",
    "tags": [
      "Evaluation"
    ],
    "authors": "Sukmin Yun, Haokun Lin, Rusiru Thushara, et al.",
    "description": "本文介绍了 Web2Code，这是一个旨在解决当前多模态大语言模型（MLLMs）在理解网页截图及生成相应 HTML 代码方面表现不佳问题的全新基准测试。该基准包含一个用于指令微调的大规模网页转代码数据集，以及一个用于评估模型网页理解与 HTML 代码转换能力的框架。在数据集构建过程中，研究者利用预训练的大语言模型增强了现有的网页转代码数据，并生成了大量多样化的新网页图像，其输入为网页图像与指令，输出为对应的 HTML 代码。此外，为了实现对网页内容更全面的理解，回复内容中还加入了关于网页信息的多元自然语言问答对。大量的实验结果表明，Web2Code 数据集不仅对网页相关任务有益，还能提升模型在通用视觉领域的表现。研究团队希望这项工作能推动适用于网页内容生成和任务自动化的通用多模态大语言模型的发展。",
    "image": "",
    "code": "https://mbzuai-llm.github.io/webpage2code/"
  },
  {
    "title": "CharXiv: Charting Gaps in Realistic Chart Understanding in Multimodal LLMs",
    "venue": "arXiv",
    "date": "2024",
    "link": "https://arxiv.org/pdf/2406.18521",
    "source": "auto-script",
    "tags": [
      "Evaluation"
    ],
    "authors": "Zirui Wang, Mengzhou Xia, Luxi He, et al.",
    "description": "本文介绍了 CharXiv，这是一个包含 2,323 个源自 arXiv 论文的高质量、具挑战性且多样化图表的综合评估套件，旨在解决现有图表理解基准测试中因图表过于简化和问题模版化而导致的进度虚高问题。研究通过压力测试发现，尽管开源模型在传统基准上似乎能超越强大的闭源模型，但在面对稍微改变的图表或问题时，其性能下降幅度可达 34.5%。CharXiv 由人类专家精心挑选并验证，包含两种问题类型：一是针对基础图表元素提取的描述性问题，二是需要综合复杂视觉信息进行逻辑推演的推理向问题。实验结果揭示了在图表推理能力上长期被低估的性能差距，其中最强的闭源模型 GPT-4o 的准确率为 47.1%，而最强的开源模型 InternVL Chat V1.5 仅为 29.2%，且所有模型均远低于人类 80.5% 的表现水平，这充分暴露出当前多模态大模型在图表理解方面的缺陷。研究团队希望 CharXiv 能为未来的研究提供更真实、可靠的衡量标准，从而推动该领域的发展。",
    "image": "",
    "code": "https://charxiv.github.io/"
  },
  {
    "title": "ChartMimic: Evaluating LMM's Cross-Modal Reasoning Capability via Chart-to-Code Generation",
    "venue": "arXiv",
    "date": "2024",
    "link": "https://arxiv.org/pdf/2406.09961",
    "source": "auto-script",
    "tags": [
      "Evaluation"
    ],
    "authors": "Cheng Yang, Chufan Shi, Yaxin Liu, Bo Shui, Junjie Wang, et al.",
    "description": "本文介绍了 ChartMimic，这是一个旨在评估大型多模态模型（LMMs）视觉驱动代码生成能力的新型基准测试。该基准以信息密集的视觉图表和文本指令作为输入，要求模型生成能够渲染出相应图表的代码，共包含 4,800 组人工精选的“图像-指令-代码”三元组，这些图表均源自物理、计算机科学、经济学等多个领域科学论文中的真实使用案例。ChartMimic 涵盖了 18 种常规类型和 4 种高级类型的图表，并细分为 201 个子类，同时配备了多层级评估指标，以对生成的代码和渲染出的图表进行自动化且彻底的评估。与现有的代码生成基准不同，ChartMimic 强调评估模型协调视觉理解、代码生成和跨模态推理等多种认知能力的能力。通过对 4 个闭源模型和 14 个开源模型的评估显示，ChartMimic 带来了巨大的挑战，即使是先进的 GPT-4o 和 InternVL2-Llama3-76B，在“直接模仿”和“定制化模仿”任务中的平均得分也分别仅为 82.2 和 61.6，表明模型仍有巨大的提升空间，研究者期待该基准能启发多模态模型的进一步发展，助力迈向通用人工智能。",
    "image": "",
    "code": "https://github.com/ChartMimic/ChartMimic"
  },
  {
    "title": "Video-MME: The First-Ever Comprehensive Evaluation Benchmark of Multi-modal LLMs in Video Analysis",
    "venue": "arXiv",
    "date": "2024",
    "link": "https://arxiv.org/pdf/2405.21075",
    "source": "auto-script",
    "tags": [
      "Evaluation"
    ],
    "authors": "Chaoyou Fu, Yuhan Dai, Yongdong Luo, Lei Li, Shuhuai Ren, Renrui Zhang, et al.",
    "description": "本文介绍了 Video-MME，这是首个全频谱的多模态视频分析评估基准，旨在弥补当前多模态大语言模型（MLLMs）在处理序列视觉数据能力评估方面的不足。该基准具备四大核心特色：一是视频类型多样化，涵盖 6 大视觉领域及其 30 个子领域，确保了场景的泛化性；二是时间维度跨度大，包含从 11 秒到 1 小时不等的短、中、长期视频，以测试模型对上下文动态的鲁棒性；三是数据模态广，除了视频帧外还整合了字幕和音频，以揭示模型的全方位能力；四是标注质量高，由专家标注员在反复观看总计 254 小时的 900 个精选视频后进行严格的人工标注，最终形成了 2,700 个问答对。通过利用 Video-MME 对包括 GPT-4 系列、Gemini 1.5 Pro 以及 InternVL-Chat-V1.5、LLaVA-NeXT-Video 等前沿开源和商业模型进行广泛评估，结果表明 Gemini 1.5 Pro 是表现最强的商业模型，性能显著优于开源模型。该数据集及其研究结果强调了未来在处理超长序列和多模态整合数据方面进一步优化模型的紧迫性。",
    "image": "",
    "code": "https://video-mme.github.io/"
  },
  {
    "title": "Benchmarking Large Multimodal Models against Common Corruptions",
    "venue": "NAACL",
    "date": "2024",
    "link": "https://arxiv.org/pdf/2401.11943.pdf",
    "source": "auto-script",
    "tags": [
      "Evaluation"
    ],
    "authors": "Jiawei Zhang, Tianyu Pang, Chao Du, Yi Ren, Bo Li, Min Lin",
    "description": "本技术报告旨在填补大型多模态模型（LMMs）评估领域的空白，通过专门考察模型输出在面对常见扰动（corruptions）时的自一致性，来深入研究文本、图像和语音之间的跨模态交互，涵盖了文本转图像、图像转文本、文本转语音及语音转文本这四个核心生成任务。为此，研究团队构建了一个名为 MMCBench 的全面基准测试，覆盖了 100 多个主流多模态大模型（总计超过 150 个模型检查点）。这种在常见扰动下的彻底评估对于模型的实际部署至关重要，并有助于更透彻地理解前沿多模态大模型的可靠性。",
    "image": "",
    "code": "https://github.com/sail-sg/MMCBench"
  },
  {
    "title": "Eyes Wide Shut? Exploring the Visual Shortcomings of Multimodal LLMs",
    "venue": "arXiv",
    "date": "2024",
    "link": "https://arxiv.org/pdf/2401.06209.pdf",
    "source": "auto-script",
    "tags": [
      "Evaluation"
    ],
    "authors": "Shengbang Tong, Zhuang Liu, Yuexiang Zhai, Yi Ma, Yann LeCun, Saining Xie",
    "description": "本文探讨了视觉能力是否足以支撑语言模型的需求，指出尽管多模态模型的进步主要源于大语言模型的强大推理能力，但其视觉组件通常仅依赖于实例级的对比图文预训练模型（CLIP），而最新多模态大模型（MLLMs）在视觉能力上仍存在系统性缺陷。通过探索 CLIP 视觉嵌入空间与纯视觉自监督学习之间的差距，研究者发现了“CLIP 盲点对”，即 CLIP 认为相似但视觉差异明显的图像对，并据此构建了多模态视觉模式（MMVP）基准测试。MMVP 揭示了包括 GPT-4V 在内的顶尖系统在九种基础视觉模式的简单问题上表现挣扎，经常给出错误答案及幻觉解释。进一步评估显示，困扰 CLIP 模型的视觉模式与多模态大模型表现不佳的模式之间存在显著相关性。作为解决这一问题的初步尝试，研究者提出了一种特征混合（MoF）方法，证明将视觉自监督学习特征整合到多模态大模型中可显著增强其视觉对齐能力。本研究表明，视觉表示学习仍是一个待解决的挑战，而准确的视觉对齐是未来成功构建多模态系统的关键。",
    "image": "",
    "code": "https://tsb0601.github.io/mmvp_blog/"
  },
  {
    "title": "A Challenger to GPT-4V? Early Explorations of Gemini in Visual Expertise",
    "venue": "arXiv",
    "date": "2023",
    "link": "https://arxiv.org/pdf/2312.12436.pdf",
    "source": "auto-script",
    "tags": [
      "Evaluation"
    ],
    "authors": "Chaoyou Fu, Renrui Zhang, Zihan Wang, et al.",
    "description": "本文对谷歌最新发布的全原生多模态大语言模型 Gemini Pro 的视觉理解能力进行了初步探索，旨在评估其是否能挑战 GPT-4V 在多模态学习领域的领先地位。研究涵盖了基础感知、高级认知、挑战性视觉任务及各类专家能力四大领域，并将 Gemini Pro 与当前的顶尖模型 GPT-4V 以及最新的开源模型 Sphinx 进行了全面对比，以揭示开源系统与商业黑盒系统之间的差距。定性分析显示，尽管 GPT-4V 倾向于提供详尽的解释和中间步骤，而 Gemini 更偏好直接简洁的回答，但两者在视觉推理能力上表现相当，且均优于在领域泛化性上仍有差距的 Sphinx。在主流 MME 基准测试中的定量评估进一步证实了 Gemini 具备成为 GPT-4V 强有力竞争者的潜力。此外，研究也观察到了 Gemini 中存在的一些多模态大模型常见问题，表明目前的模型距离实现通用人工智能仍有相当长的路要走。",
    "image": "",
    "code": "https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models"
  },
  {
    "title": "BenchLMM: Benchmarking Cross-style Visual Capability of Large Multimodal Models",
    "venue": "arXiv",
    "date": "2023",
    "link": "https://arxiv.org/pdf/2312.02896.pdf",
    "source": "auto-script",
    "tags": [
      "Evaluation"
    ],
    "authors": "Rizhao Cai, Zirui Song, Dayan Guan, et al.",
    "description": "本文介绍了 BenchLMM，这是一个旨在评估大型多模态模型（LMMs）在面对不同风格偏移时稳健性的新型基准测试，涵盖了艺术图像风格、成像传感器风格和应用风格这三大类，且每类各包含五个子风格。通过利用 BenchLMM 对当前最先进的多模态模型进行全面评估，研究发现：首先，LMMs 在处理非通用风格图像时普遍会出现性能下降；其次，在通用图像风格中表现优异的模型并不一定在其他风格中也能保持领先；第三，通过提示模型先预测图像风格可以有效增强其推理能力，据此研究者提出了一种无需训练且通用的模型改进方法；最后，智能的多模态模型应当具备解释其在风格变化下产生错误原因的能力。研究团队希望该基准与相关分析能为开发更智能、更通用的多模态大模型提供新的思路。",
    "image": "",
    "code": "https://github.com/AIFEG/BenchLMM"
  },
  {
    "title": "How Many Unicorns Are in This Image? A Safety Evaluation Benchmark for Vision LLMs",
    "venue": "arXiv",
    "date": "2023",
    "link": "https://arxiv.org/pdf/2311.16101.pdf",
    "source": "auto-script",
    "tags": [
      "Evaluation"
    ],
    "authors": "Haoqin Tu, Chenhang Cui, Zijun Wang, et al.",
    "description": "本文关注视觉大语言模型（VLLMs）在视觉推理方面的潜力，与以往侧重于标准性能评估的研究不同，本研究通过引入一套涵盖分布外（OOD）泛化和对抗鲁棒性的综合安全评估体系，将重心转向安全性评估。在 OOD 评估方面，研究者提出了两个包含变体的新型 VQA 数据集，旨在测试模型在挑战性条件下的表现；在对抗鲁棒性探索中，提出了一种简单的攻击策略，用于诱导 VLLMs 产生与视觉无关的错误响应。此外，研究还评估了针对视觉或语言组件的两种破狱（jailbreaking）策略的有效性。通过对从开源模型到 GPT-4V 等 21 种不同模型的评估，研究得出了一些有趣的观察结果：首先，目前的 VLLMs 在应对分布外文本时表现挣扎，但除非视觉信息受限，否则对分布外图像具有一定的应对能力；其次，仅通过欺骗视觉编码器即可轻易误导这些模型，且其视觉语言训练过程往往会损害现有的安全协议。",
    "image": "",
    "code": "https://github.com/UCSC-VLAA/vllm-safety-benchmark"
  },
  {
    "title": "Charting New Territories: Exploring the Geographic and Geospatial Capabilities of Multimodal LLMs",
    "venue": "arXiv",
    "date": "2023",
    "link": "https://arxiv.org/pdf/2311.14656.pdf",
    "source": "auto-script",
    "tags": [
      "Evaluation"
    ],
    "authors": "Jonathan Roberts, Timo Lüddecke, Rehan Sheikh, Kai Han, Samuel Albanie",
    "description": "本文探讨了多模态大语言模型（MLLMs）在地理和地理空间领域的能力，尽管该领域对导航、环境研究、城市发展和灾害响应具有广泛的潜在益处，但相关研究尚处于起步阶段。研究团队通过一系列实验探索了 MLLMs 在这些领域的各种视觉能力，特别关注前沿模型 GPT-4V，并将其性能与开源模型进行了基准对比。研究方法包括使用一个小规模地理基准测试来挑战这些模型，该基准包含一套不同复杂程度的视觉任务，旨在全面测试其专业素养。分析结果不仅揭示了此类模型在哪些方面表现出色（包括部分表现优于人类的案例），还指出了它们的不足之处，从而为 MLLMs 在地理领域的应用能力提供了客观、均衡的视角。",
    "image": "",
    "code": ""
  },
  {
    "title": "MLLM-Bench, Evaluating Multi-modal LLMs using GPT-4V",
    "venue": "arXiv",
    "date": "2023",
    "link": "https://arxiv.org/pdf/2311.13951",
    "source": "auto-script",
    "tags": [
      "Evaluation"
    ],
    "authors": "Wentao Ge, Shunian Chen, Guiming Hardy Chen, et al.",
    "description": "本文介绍了一种名为 MLLM-Bench 的新型评估基准，旨在解决多模态大语言模型（MLLMs）在处理开放式和主观性创意任务时，因缺乏标准答案而难以进行自动化评估的问题。针对现有评估方法往往忽视用户体验且难以捕捉多模态任务细微差别的局限性，研究团队提出了一种全新的评估范式，即利用强大的 MLLM 作为裁判，并针对每个样本设定特定的评估标准。为了验证该范式的有效性，研究者精心策划了涵盖六个认知层面的评估样本，并对 21 种主流 MLLM 进行了两两对比测试。实验结果显示，该评估基准与人工评估的一致性高达 88.02%，充分证明了其有效性。该研究强调，通过引入单样本评估标准，可以激发 MLLM 作为高效评估工具的潜力，为多模态模型的性能衡量提供了更具参考价值的路径。",
    "image": "",
    "code": ""
  },
  {
    "title": "VLM-Eval: A General Evaluation on Video Large Language Models",
    "venue": "arXiv",
    "date": "2023",
    "link": "https://arxiv.org/pdf/2311.11865.pdf",
    "source": "auto-script",
    "tags": [
      "Evaluation"
    ],
    "authors": "Shuailin Li, Yuang Zhang, Yucheng Zhao, et al.",
    "description": "本文旨在填补视频大语言模型（Video LLMs）缺乏全面评估体系的空白，引入了一种涵盖视频描述、视频问答、视频检索及动作识别等多项任务的统一评估标准。除了采用传统评估指标外，该研究还展示了基于 GPT 的评估方法在多维度衡量响应质量时，能够达到媲美人类水平的评估效果。研究团队提出了一种名为 Video-LLaVA 的简单基线模型，该模型仅通过单一线性投影连接模态，性能却优于现有的视频大语言模型。最后，研究者将评估范围扩展至学术数据集之外，证明了在仅使用数百个视频-指令对进行微调的情况下，视频大语言模型在驾驶场景中展现出了令人振奋的识别与推理能力。本项工作为视频大语言模型提供了统一的评价参考，有助于将其应用拓展至更多实际场景。",
    "image": "",
    "code": ""
  },
  {
    "title": "Holistic Analysis of Hallucination in GPT-4V(ision): Bias and Interference Challenges",
    "venue": "arXiv",
    "date": "2023",
    "link": "https://arxiv.org/pdf/2311.03287.pdf",
    "source": "auto-script",
    "tags": [
      "Evaluation"
    ],
    "authors": "Chenhang Cui, Yiyang Zhou, Xinyu Yang, et al.",
    "description": "本文引入了一个名为 Bingo 的新型基准测试，旨在系统性地评估并揭示视觉语言模型中存在的偏见（Bias）与干扰（Interference）这两类常见的幻觉行为，填补了 GPT-4V 幻觉评估领域的空白。其中，偏见是指模型由于训练数据不平衡而倾向于产生某些特定类型的错误响应；干扰则指文本提示词的表述方式或输入图像的呈现形式对模型判断造成的误导。研究发现，GPT-4V 存在显著的地域偏见，其对西方图像或英文文本图像的解析能力明显优于其他国家或语言的图像；同时，该模型极易受到诱导性问题的干扰，且在同时处理多张图像时经常出现混淆。实验进一步证明，现有的自我修正（Self-correction）和思维链（CoT）推理等主流缓解方案在应对这些挑战时效果有限。此外，研究还在 LLaVA 和 Bard 等模型中发现了类似的偏见与干扰漏洞。该研究全面刻画了 GPT-4V 及当前最先进视觉语言模型面临的幻觉挑战，并强调了开发全新解决方案的必要性。",
    "image": "",
    "code": "https://github.com/gzcch/Bingo"
  },
  {
    "title": "On the Road with GPT-4V(ision): Early Explorations of Visual-Language Model on Autonomous Driving",
    "venue": "arXiv",
    "date": "2023",
    "link": "https://arxiv.org/pdf/2311.05332.pdf",
    "source": "auto-script",
    "tags": [
      "Evaluation"
    ],
    "authors": "Licheng Wen, Xuemeng Yang, Daocheng Fu, et al.",
    "description": "本文探讨了自动驾驶技术在整合感知、决策与控制系统过程中面临的瓶颈，指出传统的数据驱动和规则驱动方法难以理解复杂环境的细微差别及其他道路使用者的意图，尤其在缺乏常识推理和深层场景理解方面限制了驾驶安全性。研究指出，视觉语言模型（VLM）的兴起为实现全自动驾驶开辟了新前沿。本报告对顶尖模型 GPT-4V 在自动驾驶场景中的表现进行了详尽评估，涵盖了从基础场景识别到复杂因果推理及动态条件下的实时决策。研究发现，GPT-4V 在场景理解和因果推理方面显著优于现有的自动驾驶系统，展现出处理分布外（OOD）场景、意图识别及在真实驾驶语境中做出知情决策的巨大潜力。然而，该模型在方向判别、红绿灯识别、视觉对齐（vision grounding）以及空间推理任务上仍面临挑战，这些局限性也为未来的研究与开发指明了方向。",
    "image": "",
    "code": "https://github.com/PJLab-ADG/GPT4V-AD-Exploration"
  },
  {
    "title": "Towards Generic Anomaly Detection and Understanding: Large-scale Visual-linguistic Model (GPT-4V) Takes the Lead",
    "venue": "arXiv",
    "date": "2023",
    "link": "https://arxiv.org/pdf/2311.02782.pdf",
    "source": "auto-script",
    "tags": [
      "Evaluation"
    ],
    "authors": "Yunkang Cao, Xiaohao Xu, Chen Sun, Xiaonan Huang, Weiming Shen",
    "description": "本文探讨了利用强大的视觉语言模型 GPT-4V(ision) 以通用方式处理异常检测任务的潜力，旨在突破现有模型通常仅针对特定领域和模态设计的局限性。研究调查了 GPT-4V 在多模态和多领域异常检测任务中的应用，涵盖了图像、视频、点云及时间序列数据，应用范围涉及工业、医疗、逻辑、视频及 3D 异常检测与定位等多个领域。为了进一步提升性能，研究团队在提示词中引入了类别信息、专家知识和参考图像等辅助线索。实验结果表明，GPT-4V 在零样本或单样本异常检测中表现卓越，能够高效检测并解释全局及细粒度的语义模式，从而准确区分正常与异常实例。尽管本研究进行了广泛评估，但未来仍有进一步提升的空间，例如探索定量指标、扩大基准测试范围、引入多轮交互及人类反馈循环等。总体而言，GPT-4V 在通用异常检测和理解方面展现了极具前景的性能，为异常检测领域开辟了新的研究路径。",
    "image": "",
    "code": "https://github.com/caoyunkang/GPT4V-for-Generic-Anomaly-Detection"
  },
  {
    "title": "A Comprehensive Study of GPT-4V's Multimodal Capabilities in Medical Imaging",
    "venue": "arXiv",
    "date": "2023",
    "link": "https://arxiv.org/pdf/2310.20381.pdf",
    "source": "auto-script",
    "tags": [
      "Evaluation"
    ],
    "authors": "Yingshu Li, Yunyi Liu, Zhanyu Wang, et al.",
    "description": "本文对 GPT-4V 在医学图像分析方面的多模态能力进行了系统评估，重点涵盖了放射科报告生成、医学视觉问答和医学视觉对齐（Visual Grounding）三个代表性任务。研究团队为每个任务设计了一系列专门的提示词，以引导 GPT-4V 生成高质量输出，并采用定量分析、人工评估和个案研究三种方式进行了深入且广泛的测评。评估结果显示，GPT-4V 在理解医学图像方面表现卓越，能够生成高质量的放射科报告，并能有效回答有关医学图像的问题；然而，其在医学视觉对齐任务上的表现仍有待显著提升。此外，研究观察到定量分析结果与人工评估结果之间存在差异，这种不一致性揭示了传统评估指标在衡量 GPT-4V 等大语言模型性能时的局限性，同时也凸显了开发新型自动定量分析指标的必要性。",
    "image": "",
    "code": ""
  },
  {
    "title": "An Early Evaluation of GPT-4V(ision)",
    "venue": "arXiv",
    "date": "2023",
    "link": "https://arxiv.org/pdf/2310.16534.pdf",
    "source": "auto-script",
    "tags": [
      "Evaluation"
    ],
    "authors": "Yang Wu, Shilong Wang, Hao Yang, et al.",
    "description": "本文对 GPT-4V 在视觉理解、语言理解、视觉谜题解决以及对深度图、热成像、视频和音频等其他模态的理解能力进行了全面评估。通过手动构建的 656 个测试用例及其详尽的评估结果，研究得出以下核心发现：首先，GPT-4V 在以英语为核心的视觉基准测试中表现出色，但无法识别图像中简单的中文文本；其次，在回答涉及性别、种族和年龄等敏感特征的问题时，该模型表现出不一致的拒绝行为；第三，在通用语言理解和视觉常识知识评估中，GPT-4V 的表现优于早期的 GPT-4 (API)；第四，少样本提示（Few-shot prompting）能有效提升其在视觉与语言理解方面的性能；第五，模型在识别两张相似图像间的细微差别以及解决简单的数学图片谜题时面临困难；最后，GPT-4V 在处理视频和热成像等与图像相似的模态任务时展现了显著的能力。这些实验结果全面揭示了 GPT-4V 的能力边界与局限性，旨在为该模型的应用与后续研究提供参考见解。",
    "image": "",
    "code": "https://github.com/albertwy/GPT-4V-Evaluation"
  },
  {
    "title": "Exploring OCR Capabilities of GPT-4V(ision) : A Quantitative and In-depth Evaluation",
    "venue": "arXiv",
    "date": "2023",
    "link": "https://arxiv.org/pdf/2310.16809.pdf",
    "source": "auto-script",
    "tags": [
      "Evaluation"
    ],
    "authors": "Yongxin Shi, Dezhi Peng, Wenhui Liao, et al.",
    "description": "本文对近期发布的大型多模态模型 GPT-4V(ision) 的光学字符识别（OCR）能力进行了全面评估。研究涵盖了场景文本识别、手写体识别、手写数学公式识别、表格结构识别以及视觉丰富文档的信息提取等一系列 OCR 任务。评估结果显示，GPT-4V 在识别和理解拉丁语内容方面表现出色，但在处理多语言场景和复杂任务时显得力不从心。具体而言，该模型在处理非拉丁语系以及手写数学公式识别、表格结构解析、端到端语义实体识别和文档图像关系提取等挑战性任务时存在局限。基于这些观察，研究affirm了研发专用 OCR 模型的必要性及其持续的研究价值。总体来看，尽管 GPT-4V 在处理多样化 OCR 任务时展现了多功能性，但其表现尚未超越现有的最先进专用 OCR 模型。如何充分利用 GPT-4V 这种预训练通用多模态大模型来提升 OCR 下游任务的性能，仍是一个有待解决的问题，本研究为未来利用多模态大模型进行 OCR 研究提供了重要的参考。",
    "image": "",
    "code": "https://github.com/SCUT-DLVCLab/GPT-4V_OCR"
  },
  {
    "title": "HallusionBench: You See What You Think? Or You Think What You See? An Image-Context Reasoning Benchmark Challenging for GPT-4V(ision), LLaVA-1.5, and Other Multi-modality Models",
    "venue": "CVPR",
    "date": "2023",
    "link": "https://arxiv.org/pdf/2310.14566.pdf",
    "source": "auto-script",
    "tags": [
      "Evaluation"
    ],
    "authors": "Tianrui Guan, Fuxiao Liu, Xiyang Wu, et al.",
    "description": "本文介绍了 HallusionBench，这是一个专为评估图像上下文推理能力而设计的综合基准测试，它通过强调对视觉数据的细微理解与解析，对 GPT-4V、Gemini Pro Vision、Claude 3 和 LLaVA-1.5 等先进的大型视觉语言模型（LVLMs）提出了重大挑战。该基准包含由人类专家精心设计的 346 张图像和 1129 个问题，并引入了一种旨在建立对照组的新型视觉问题结构，从而能够对模型的回答倾向、逻辑一致性以及各种失败模式进行定量分析。在对 15 种不同模型的评估中，最先进的 GPT-4V 仅取得了 31.42% 的问题对准确率，而其余所有模型的准确率均低于 16%。此外，该研究不仅揭示了语言幻觉（Language Hallucination）和视觉错觉（Visual Illusion）等失败模式，还通过详尽的案例研究深化了对这些陷阱的理解。基于 HallusionBench 提供的见解，研究者为未来提升视觉语言模型的鲁棒性与准确性提出了潜在的改进路径。",
    "image": "",
    "code": "https://github.com/tianyi-lab/HallusionBench"
  },
  {
    "title": "MathVista: Evaluating Math Reasoning in Visual Contexts with GPT-4V, Bard, and Other Large Multimodal Models",
    "venue": "ICLR",
    "date": "2023",
    "link": "https://arxiv.org/pdf/2310.02255.pdf",
    "source": "auto-script",
    "tags": [
      "Evaluation"
    ],
    "authors": "Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, et al.",
    "description": "本文介绍了 MathVista，这是一个专门用于评估大型语言模型（LLMs）和大型多模态模型（LMMs）在视觉语境下数学推理能力的基准测试，旨在弥补该领域缺乏系统研究的空白。MathVista 包含 6,141 个案例，整合了 28 个现有的多模态数学相关数据集以及 3 个新创建的数据集（IQTest、FunctionQA 和 PaperQA），要求模型具备精细且深度的视觉理解及组合推理能力。研究团队利用该基准对 12 个主流基础模型进行了全面的定量评估，结果显示表现最优的 GPT-4V 达到了 49.9% 的准确率，显著领先第二名 Bard 约 15.1%，这主要归功于其增强的视觉感知和数学推理水平。然而，GPT-4V 在理解复杂图表和严谨推理方面仍面临困难，与人类表现相比仍有 10.4% 的差距，这一显著差距突显了 MathVista 在推动能够处理数学密集型和视觉丰富型任务的通用人工智能代理开发中的关键作用。此外，本文还进一步探索了 GPT-4V 的自我验证、自一致性应用以及交互式聊天机器人能力，展示了其在未来研究中的巨大潜力。",
    "image": "",
    "code": "https://mathvista.github.io/"
  },
  {
    "title": "Fool Your (Vision and) Language Model With Embarrassingly Simple Permutations",
    "venue": "arXiv",
    "date": "2023",
    "link": "https://arxiv.org/pdf/2310.01651.pdf",
    "source": "auto-script",
    "tags": [
      "Evaluation"
    ],
    "authors": "Yongshuo Zong, Tingyang Yu, Ruchika Chavhan, Bingchen Zhao, Timothy Hospedales",
    "description": "本文深入分析了大型语言模型（LLMs）和视觉语言模型（VLMs）在广泛应用背景下的稳健性，重点揭示了这些模型在多选题问答（MCQA）任务中存在的一个特定漏洞——排列敏感性（Permutation Sensitivity）。研究通过实证分析证明，尽管理想模型应当像人类一样能够忽略选项顺序的变化，但目前主流模型在面对选项集的对抗性排列（即随机改变选项顺序）时表现得非常脆弱，其预测结果会随选项顺序的变动而发生显著偏差。这种脆弱性在不同规模的模型中普遍存在，甚至在最新的语言和视觉语言模型中依然显著。该发现不仅对模型在实际应用中的可信度提出了挑战，也强调了在将此类模型部署于关键决策场景前，对其进行严谨稳健性分析的紧迫性。",
    "image": "",
    "code": "https://github.com/ys-zong/FoolyourVLLMs"
  },
  {
    "title": "Beyond Task Performance: Evaluating and Reducing the Flaws of Large Multimodal Models with In-Context Learning",
    "venue": "arXiv",
    "date": "2023",
    "link": "https://arxiv.org/pdf/2310.00647.pdf",
    "source": "auto-script",
    "tags": [
      "Evaluation"
    ],
    "authors": "Mustafa Shukor, Alexandre Rame, Corentin Dancette, Matthieu Cord",
    "description": "本文深入探讨了继大语言模型成功之后，作为迈向通用智能代理重要步骤的大型多模态模型（LMMs）所面临的局限性，指出仅凭视觉问答准确率等任务指标已不足以全面衡量模型的真实能力及其与人类预期的对齐程度。研究人员偏离了现有的评估范式，从幻觉、不当拒绝、组合性、可解释性和指令遵循五个维度，对 10 个参数量从 30 亿到 800 亿不等的最新开源多模态模型进行了系统评估，揭示了 LMMs 中存在的重大缺陷。不同于目前主流的指令微调或基于人类反馈的强化学习（RLHF）等训练对齐方法，本研究探索了无需训练的上下文学习（ICL）作为解决方案，分析了其对上述缺陷的影响，并进一步提出了多任务 ICL、事后链 ICL（Chain-of-Hindsight-ICL）和自我修正 ICL 等新型多模态 ICL 变体。研究结果表明：首先，LMMs 的缺陷无法仅通过增加参数规模来解决；其次，ICL 对模型缺陷的影响具有细微差别，虽然能有效提升可解释性和减少不当拒绝，但对指令遵循的提升微乎其微，无法改善组合能力，甚至会加剧幻觉；最后，所提出的 ICL 变体作为一种事后处理方法，在高效解决上述部分缺陷方面展现了良好的应用前景。",
    "image": "",
    "code": "https://github.com/mshukor/EvALign-ICL"
  },
  {
    "title": "Can We Edit Multimodal Large Language Models?",
    "venue": "arXiv",
    "date": "2023",
    "link": "https://arxiv.org/pdf/2310.08475.pdf",
    "source": "auto-script",
    "tags": [
      "Evaluation"
    ],
    "authors": "Siyuan Cheng, Bozhong Tian, Qingbin Liu, et al.",
    "description": "本文聚焦于多模态大语言模型（MLLMs）的编辑问题，指出与单模态模型编辑相比，多模态模型编辑由于面临更复杂的挑战，在编辑过程中需要更高程度的审查与考量。为推动该领域的研究，作者构建了一个名为 MMEdit 的新型多模态大模型编辑基准，并建立了一套创新的评估指标体系。通过对多种模型编辑基线方法进行全面实验，并深入分析编辑不同组件对多模态大模型的影响，研究发现现有基线方法虽然在一定程度上能实现对多模态大模型的编辑，但其实际效果仍不尽如人意，这表明了该任务具有极高的挑战性。作者希望这项工作能为自然语言处理社区提供有价值的见解。",
    "image": "",
    "code": "https://github.com/zjunlp/EasyEdit"
  },
  {
    "title": "REVO-LION: Evaluating and Refining Vision-Language Instruction Tuning Datasets",
    "venue": "arXiv",
    "date": "2023",
    "link": "https://arxiv.org/pdf/2310.06594.pdf",
    "source": "auto-script",
    "tags": [
      "Evaluation"
    ],
    "authors": "Ning Liao, Shaofeng Zhang, Renqiu Xia, Min Cao, Yu Qiao, Junchi Yan",
    "description": "本文针对当前多模态指令微调研究中普遍侧重评估模型性能而忽视数据集质量的问题，提出了一种全新的视觉-语言指令微调（VLIT）数据集评估范式。为了量化数据集及其样本的质量，研究者提出了一种“微调-交叉评估”（tune-cross-evaluation）策略，即在单一数据集上进行微调后，依次在其他数据集上进行交叉测试。在此基础上，本文定义了三个核心评价指标：元质量（Meta Quality, MQ），利用 BLEU、METEOR 和 ROUGE-L 等指标的均值来量化单次实验的产出质量；数据集质量（Dataset Quality, DQ），用于衡量数据集的全面性；以及样本质量（Sample Quality, SQ），用于评估每个样本的多维度贡献。基于这一整套评估体系，研究团队通过从现有各数据集中筛选高 SQ 样本，构建了全新的数据集 REVO-LION。实验表明，REVO-LION 表现出了极高的效率，仅需全部原始数据量的一半，即可训练出性能等同于直接汇总所有 VLIT 数据集的强大模型。此外，REVO-LION 还包含一个配套的评估集，旨在为未来该领域的研究提供一个便捷且稳健的基准测试标准。",
    "image": "",
    "code": ""
  },
  {
    "title": "The Dawn of LMMs: Preliminary Explorations with GPT-4V(vision)",
    "venue": "arXiv",
    "date": "2023",
    "link": "https://arxiv.org/pdf/2309.17421.pdf",
    "source": "auto-script",
    "tags": [
      "Evaluation"
    ],
    "authors": "Zhengyuan Yang, Linjie Li, Kevin Lin, et al.",
    "description": "本文对最新的大型多模态模型 GPT-4V(ision) 进行了深入分析，旨在深化对多模态大模型（LMMs）能力的理解。该研究重点探索了 GPT-4V 所能执行的各类有趣任务，通过精心策划并涵盖多个领域和任务的定性样本，探测了其能力的质量与通用性、支持的输入形式、工作模式以及有效的提示策略。观察结果表明，GPT-4V 在处理任意交错的多模态输入方面展现出前所未有的能力，其极强的通用性使其成为一个强大的多模态全能系统。此外，GPT-4V 能够理解输入图像上绘制的视觉标记，这一独特能力催生了如视觉指代提示（visual referring prompting）等新型人机交互方法。报告最后深入探讨了基于 GPT-4V 系统的各种新兴应用场景和未来研究方向，希望这一初步探索能为下一代多模态任务制定、利用和增强 LMMs 解决现实问题的方法，以及更好地理解多模态基础模型提供启发，并明确指出该模型完全是 OpenAI 创新工作的成果。",
    "image": "",
    "code": ""
  },
  {
    "title": "TouchStone: Evaluating Vision-Language Models by Language Models",
    "venue": "arXiv",
    "date": "2023",
    "link": "https://arxiv.org/pdf/2308.16890.pdf",
    "source": "auto-script",
    "tags": [
      "Evaluation"
    ],
    "authors": "Shuai Bai, Shusheng Yang, Jinze Bai, et al.",
    "description": "本文介绍了 TouchStone，这是一个旨在全面评估大型视觉语言模型（LVLMs）多维能力的新型评估框架，重点弥补了现有测评主要关注识别与推理而忽视对话技能及视觉叙事能力的缺陷。研究者首先构建了一个名为 TouchStone 的综合性视觉对话数据集，包含源自开放世界的图像和问题，涵盖了基础描述、视觉识别、视觉理解、视觉叙事及多图分析五大能力类别及 27 个子任务，将评估范围从基础认知扩展到了文学创作。其次，该框架通过整合详细的图像标注，将多模态输入转化为大语言模型（LLMs）可理解的文本形式，从而实现利用先进 LLM 作为裁判来直接、自动地评估多模态对话质量，无需人工干预。验证结果表明，GPT-4 等强大模型仅凭其文本处理能力即可有效评分，且评分结果与人类偏好高度一致。作者希望 TouchStone 能成为评估 LVLMs 的“试金石”，为构建更强大的多模态模型铺平道路。",
    "image": "",
    "code": "https://github.com/OFA-Sys/TouchStone"
  },
  {
    "title": "SciGraphQA: A Large-Scale Synthetic Multi-Turn Question-Answering Dataset for Scientific Graphs",
    "venue": "arXiv",
    "date": "2023",
    "link": "https://arxiv.org/pdf/2308.03349.pdf",
    "source": "auto-script",
    "tags": [
      "Evaluation"
    ],
    "authors": "Shengzhi Li, Nima Tajbakhsh",
    "description": "本文介绍了 SciGraphQA，这是一个针对学术图表的大规模合成多轮问答数据集。其规模是此前最大的图表视觉问答数据集 ChartVQA 的 13 倍，也是目前最大的包含真实（非合成）图表的开源图表 VQA 数据集。研究团队从 2010 年至 2020 年间发表的 29 万篇计算机科学或机器学习领域的 ArXiv 论文中提取图表，并利用 Palm-2 模型生成了 29.5 万个关于这些图表的开放词汇多轮问答对话。在生成过程中，Palm-2 结合了论文标题、摘要、正文引用段落以及图表自身的丰富文本上下文信息，平均每个图表包含 2.23 轮对话。GPT-4 对该数据集测试集的评估显示，问答质量得分高达 8.7/10。通过对 LLaVA、mPLUG-owl 等主流多模态模型的零样本能力测试，发现 LLaVA-13B 表现最优；而通过引入由 DePlot 模型提取的序列化数据表增强提示词，或直接利用该数据集进行微调，LLaVA 的性能得到了显著提升（CIDEr 分数从 0.08 升至 0.26）。研究者认为，未来结合分割掩码标记、更大的模型骨干及新兴提示技术，有望进一步提高准确率。目前，该项目的所有代码和数据已开源。",
    "image": "",
    "code": ""
  },
  {
    "title": "Tiny LVLM-eHub: Early Multimodal Experiments with Bard",
    "venue": "arXiv",
    "date": "2023",
    "link": "https://arxiv.org/pdf/2308.03729.pdf",
    "source": "auto-script",
    "tags": [
      "Evaluation"
    ],
    "authors": "Wenqi Shao, Meng Lei, Yutao Hu, Peng Gao, Kaipeng Zhang, et al.",
    "description": "本文通过提出一个轻量化评估基准 Tiny LVLM-eHub，对以谷歌 Bard 为代表的大型视觉语言模型（LVLMs）的多模态能力进行了早期且全面的系统性评估。相比于原始版本，Tiny LVLM-eHub 具有显著优势：首先，它通过对 42 个标准文本相关视觉基准的定量测评，涵盖了视觉感知、视觉知识获取、视觉推理、视觉常识、物体幻觉及具身智能六大核心能力类别；其次，它引入了基于 ChatGPT 的集成评估方法（CEE），与传统的单词匹配法相比，该方法能提供更稳健、准确的分析，且与人工评价的一致性更高；最后，该基准仅包含 2,100 个图文对，极大地降低了从业者评估线下模型的门槛。广泛的实验结果表明，Bard 在除物体幻觉以外的大多数多模态能力上均优于此前的模型，但仍表现出对物体幻觉的敏感性。Tiny LVLM-eHub 为各类视觉语言模型提供了基准评价参考，旨在激发更多推动多模态技术进步的创新策略。",
    "image": "",
    "code": "https://github.com/OpenGVLab/Multi-Modality-Arena"
  },
  {
    "title": "MM-Vet: Evaluating Large Multimodal Models for Integrated Capabilities",
    "venue": "arXiv",
    "date": "2023",
    "link": "https://arxiv.org/pdf/2308.02490.pdf",
    "source": "auto-script",
    "tags": [
      "Evaluation"
    ],
    "authors": "Weihao Yu, Zhengyuan Yang, Linjie Li, et al.",
    "description": "本文提出了 MM-Vet，这是一个旨在评估大型多模态模型（LMMs）处理复杂多模态任务能力的基准测试。针对当前 LMMs 展现出的如解数学题、分析新闻图像事件及解释视觉笑话等新兴能力，MM-Vet 解决了评估基准开发中的三大核心挑战：如何系统地构建和评估复杂任务、如何设计适用于多种问答类型的统一评估指标，以及如何提供超越简单排名深度的模型洞察。MM-Vet 的核心设计理念在于，解决复杂任务的能力通常源于通用模型对不同核心视觉-语言（VL）能力的整合，据此它定义了 6 种核心 VL 能力，并考察了由这些能力组合衍生出的 16 种整合能力。在评估指标方面，MM-Vet 提出了一种基于大语言模型（LLM）的开放式输出评估器，实现了跨问题类型和回答风格的统一评分。通过对代表性 LMMs 的评估，该基准不仅展示了不同模型及系统范式的性能表现，还为其各自的能力优劣提供了深入的见解。",
    "image": "",
    "code": "https://github.com/yuweihao/MM-Vet"
  },
  {
    "title": "SEED-Bench: Benchmarking Multimodal LLMs with Generative Comprehension",
    "venue": "CVPR",
    "date": "2023",
    "link": "https://arxiv.org/pdf/2307.16125.pdf",
    "source": "auto-script",
    "tags": [
      "Evaluation"
    ],
    "authors": "Bohao Li, Rui Wang, Guangzhi Wang, Yuying Ge, Yixiao Ge, Ying Shan",
    "description": "本文介绍了 SEED-Bench，这是一个旨在评估多模态大语言模型（MLLMs）生成式理解能力的综合性基准测试，作为全面衡量生成式模型能力的关键步骤。SEED-Bench 包含 1.9 万个经过人工精确标注的选择题，其规模是现有基准测试的 6 倍，涵盖了包括图像和视频模态理解在内的 12 个评估维度。研究团队开发了一套先进的流程来生成针对特定维度的选择题，并结合了自动过滤与人工验证环节。通过使用带有标准答案的选择题形式，该基准实现了客观且高效的模型性能评估，无需人工或 GPT 的干预。通过对 18 个主流模型在空间和时间理解等所有 12 个维度上的性能测评，SEED-Bench 揭示了现有模型在复杂多模态理解方面的局限性。研究者希望通过该基准为未来的研究提供动力和见解，并计划通过持续维护的排行榜为社区提供一个评估和探索模型能力的开放平台。",
    "image": "",
    "code": "https://github.com/AILab-CVC/SEED-Bench"
  },
  {
    "title": "MMBench: Is Your Multi-modal Model an All-around Player?",
    "venue": "arXiv",
    "date": "2023",
    "link": "https://arxiv.org/pdf/2307.06281.pdf",
    "source": "auto-script",
    "tags": [
      "Evaluation"
    ],
    "authors": "Yuan Liu, Haodong Duan, Yuanhan Zhang, et al.",
    "description": "本文介绍了 MMBench，这是一个专为视觉语言模型（VLMs）设计的双语评估基准，旨在解决传统基准缺乏细粒度评估以及主观评估难以规模化且存在偏见的问题。MMBench 系统地构建了一套全面的评估流程，其主要特点包括：首先，通过严格的质量控制方案精选测试内容，在评估问题的数量、多样性及覆盖能力维度上均超越了现有同类基准；其次，引入了严格的循环评估（CircularEval）策略，并利用大语言模型将自由回答转换为预设选项，从而确保对指令遵循能力较弱的模型也能产出准确的评估结果；最后，该基准包含英文和中文版本的选择题，实现了对模型在双语语境下性能的公平对比。总之，MMBench 是一个系统化设计的客观基准，能够对视觉语言模型进行稳健且全面的评价，旨在帮助研究社区更好地评估模型并推动该领域的未来发展。",
    "image": "",
    "code": "https://github.com/open-compass/VLMEvalKit"
  },
  {
    "title": "MME: A Comprehensive Evaluation Benchmark for Multimodal Large Language Models",
    "venue": "arXiv",
    "date": "2023",
    "link": "https://arxiv.org/pdf/2306.13394.pdf",
    "source": "auto-script",
    "tags": [
      "Evaluation"
    ],
    "authors": "Chaoyou Fu, Peixian Chen, Yunhang Shen, et al.",
    "description": "本文介绍了首个全面的多模态大语言模型（MLLM）评估基准 MME，旨在解决现有研究多依赖于个案展示而缺乏系统性评估的问题。该基准通过 14 个子任务从感知和认知两大维度衡量模型性能。为了防止因直接使用公开数据集而导致的训练数据泄露，MME 的所有指令-回答对均经过人工手动设计。该基准采用了简洁的指令设计，不仅避免了复杂的提示词工程对评估公平性的影响，也便于进行准确的定量统计。通过对 30 个先进的 MLLM 进行全面测评，研究发现现有模型在理解与推理方面仍有巨大的提升空间，并揭示了后续模型优化的潜在方向。",
    "image": "",
    "code": "https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models/tree/Evaluation"
  },
  {
    "title": "LVLM-eHub: A Comprehensive Evaluation Benchmark for Large Vision-Language Models",
    "venue": "arXiv",
    "date": "2023",
    "link": "https://arxiv.org/pdf/2306.09265.pdf",
    "source": "auto-script",
    "tags": [
      "Evaluation"
    ],
    "authors": "Peng Xu, Wenqi Shao, Kaipeng Zhang, Peng Gao, Shuo Liu, et al.",
    "description": "本文介绍了 LVLM-eHub，这是一个针对开源大型视觉语言模型（LVLMs）效能的综合评估枢纽，旨在填补该领域缺乏全面评估体系的空白。该框架集成了 InstructBLIP 和 MiniGPT-4 等 8 个代表性模型，并通过定量能力测评和在线竞技平台（Arena）进行双重评估：前者涵盖了视觉问答、具身智能等 6 大核心多模态能力，涉及 47 个标准视觉基准；后者则在开放世界问答场景下提供用户层面的主观评价。研究得出了一系列创新发现：首先，在海量域内数据上进行指令微调的模型（如 InstructBLIP）在现有任务中存在严重过拟合，且在开放世界场景中的泛化能力较差；其次，采用适量数据微调的模型容易出现物体幻觉问题，导致图像描述的准确性下降，使得 CIDEr 等传统评价指标失效；最后，研究发现采用多轮推理评估框架可以有效缓解物体幻觉问题。这些发现为开发更有效的 LVLM 评估流程以及提升零样本多模态技术提供了基础框架和策略指引。",
    "image": "",
    "code": "https://github.com/OpenGVLab/Multi-Modality-Arena"
  },
  {
    "title": "M3Exam: A Multilingual, Multimodal, Multilevel Benchmark for Examining Large Language Models",
    "venue": "arXiv",
    "date": "2023",
    "link": "https://arxiv.org/pdf/2306.05179.pdf",
    "source": "auto-script",
    "tags": [
      "Evaluation"
    ],
    "authors": "Wenxuan Zhang, Sharifah Mahani Aljunied, Chang Gao, Yew Ken Chia, Lidong Bing",
    "description": "本文提出了 M3Exam，这是一个源自真实官方人类考试题目的新型基准测试，旨在更有效地评估大语言模型（LLMs）的通用智能，因为人类考试天然要求语言理解、领域知识和问题解决等多维能力的综合运用。M3Exam 具有三个核心特征：首先是多语言性，涵盖了来自多个国家的试题，要求模型具备深度的多语言能力和文化背景知识；其次是多模态性，包含大量需要结合图像理解才能解答的考试题目；最后是多层次结构，整合了三个关键受教育阶段的试题，以全面评估模型在不同等级的熟练程度。该基准总计包含 9 种语言、三个教育水平的 12,317 道题目，其中约 23% 的题目涉及图像处理。通过对顶级大模型的测试，研究发现包括 GPT-4 在内的当前模型在处理多语言文本（尤其是低资源和非拉丁脚本语言）时仍面临巨大挑战，且多模态大模型在解决复杂的图像相关考题时表现欠佳。M3Exam 为全面追踪和评估大语言模型在多语言与多模态环境下的发展提供了重要的资源支持。",
    "image": "",
    "code": "https://github.com/DAMO-NLP-SG/M3Exam"
  },
  {
    "title": "On The Hidden Mystery of OCR in Large Multimodal Models",
    "venue": "arXiv",
    "date": "2023",
    "link": "https://arxiv.org/pdf/2305.07895.pdf",
    "source": "auto-script",
    "tags": [
      "Evaluation"
    ],
    "authors": "Yuliang Liu, Zhang Li, Mingxin Huang, et al.",
    "description": "本文对 GPT-4V 和 Gemini 等大型多模态模型在各类文本相关视觉任务中的表现进行了全面评估，涵盖了文本识别、场景文本视觉问答（VQA）、文档导向 VQA、关键信息提取（KIE）以及手写数学公式识别（HMER）等关键领域。为了系统性地衡量多模态模型的 OCR 能力，研究团队提出了目前最全面的 OCR 评估基准 OCRBench，该基准整合了 29 个相关数据集。研究深入揭示了这些模型在处理多语言文本、手写文本、无语义文本以及数学公式识别时的优劣势。最重要的是，本研究提供的基准测试结果为开发和评估旨在增强零样本多模态技术的新型策略奠定了基础框架。",
    "image": "",
    "code": "https://github.com/Yuliang-Liu/MultimodalOCR"
  },
  {
    "title": "R1-Reward: Training Multimodal Reward Model Through Stable Reinforcement Learning",
    "venue": "arXiv",
    "date": "2025",
    "link": "https://arxiv.org/pdf/2505.02835",
    "source": "auto-script",
    "tags": [
      "RLHF"
    ],
    "authors": "Yi-Fan Zhang, Xingyu Lu, Xiao Hu, et al.",
    "description": "本文探讨了强化学习（RL）在提升多模态奖励模型（MRMs）性能方面的作用，旨在解决现有研究在奖励模型长期推理能力及其激活机制上探索不足的问题。研究者将奖励建模问题重新定义为一项基于规则的强化学习任务，但发现直接应用 Reinforce++ 等现有 RL 算法往往会因算法局限导致训练不稳定甚至崩溃。为此，研究团队提出了 StableReinforce 算法，通过优化训练损失、优势估计策略和奖励设计，实现了更稳定的训练动态和更优的表现。为了支持模型训练，研究者从多个数据集中搜集了 20 万条偏好数据，并利用 StableReinforce 算法训练出了 R1-Reward 模型。实验结果表明，R1-Reward 在多模态奖励建模基准测试中显著提升了性能，相较于此前的最先进模型，在 VL Reward-Bench 和 Multimodal Reward Bench 上分别实现了 8.4% 和 14.3% 的提升。此外，通过增加推理端的计算量，R1-Reward 的表现得到了进一步增强，充分展示了强化学习算法在优化多模态奖励模型方面的巨大潜力。",
    "image": "",
    "code": "https://github.com/yfzhang114/r1_reward"
  },
  {
    "title": "Aligning Multimodal LLM with Human Preference: A Survey",
    "venue": "arXiv",
    "date": "2025",
    "link": "https://arxiv.org/pdf/2503.14504",
    "source": "auto-script",
    "tags": [
      "RLHF"
    ],
    "authors": "Tao Yu, Yi-Fan Zhang, Chaoyou Fu, Junkang Wu, Jinda Lu, et al.",
    "description": "本文对多模态大语言模型（MLLMs）的对齐算法进行了全面且系统的综述。尽管 MLLMs 在处理视听等多模态任务中展现出巨大潜力，但在真实性、安全性、类 o1 的推理能力以及与人类偏好对齐方面仍存在挑战，而对齐算法已被证明是解决这些问题的强大手段。文章主要从四个关键维度展开探讨：首先是应用场景，涵盖了通用图像理解、多图处理、视频、音频以及更广泛的多模态应用；其次是构建对齐数据集的核心要素，包括数据来源、模型响应生成以及偏好标注策略；第三是用于评估这些对齐算法的各类基准测试；最后，文章讨论了对齐算法未来发展的潜在方向。该综述旨在帮助研究人员梳理该领域的最新进展，并为开发更高效的对齐方法提供启发。",
    "image": "",
    "code": "https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models/tree/Alignment"
  },
  {
    "title": "MM-RLHF: The Next Step Forward in Multimodal LLM Alignment",
    "venue": "arXiv",
    "date": "2025",
    "link": "https://arxiv.org/pdf/2502.10391",
    "source": "auto-script",
    "tags": [
      "RLHF"
    ],
    "authors": "Yi-Fan Zhang, Tao Yu, Haochen Tian, Chaoyou Fu, Peiyan Li, et al.",
    "description": "本文介绍了 MM-RLHF，这是一个包含 12 万对精细化人工标注偏好对比数据的规模化数据集，旨在解决当前大多数顶尖多模态大语言模型（MLLMs）尚未经过系统性人类偏好对齐的问题，并探索对齐过程对模型能力的系统性提升。相较于现有资源，该数据集在规模、多样性、标注粒度和质量上均有显著突破。基于该数据集，研究团队提出了多项技术创新：首先是引入了基于评论的奖励模型（Critique-Based Reward Model），通过在评分前生成评论来提供比传统标量奖励更具可解释性和信息量的反馈；其次是提出了动态奖励缩放（Dynamic Reward Scaling）方法，根据奖励信号调整每个样本的损失权重，以优化高质量对比对的利用效率。实验在 10 个维度和 27 个基准测试上进行了严谨评估，结果显示该方法能显著且稳定地提升模型性能，其中经 MM-RLHF 和相关算法微调后的 LLaVA-ov-7B 在对话能力上提升了 19.5%，安全性提升了 60%。目前，该项目的偏好数据集、奖励模型、训练评估代码以及奖励建模与安全性基准已全部开源。",
    "image": "",
    "code": "https://mm-rlhf.github.io/"
  },
  {
    "title": "Enhancing Multimodal LLM for Detailed and Accurate Video Captioning using Multi-Round Preference Optimization",
    "venue": "arXiv",
    "date": "2024",
    "link": "https://arxiv.org/pdf/2410.06682",
    "source": "auto-script",
    "tags": [
      "RLHF"
    ],
    "authors": "Changli Tang, Yixuan Li, Yudong Yang, et al.",
    "description": "本文介绍了 video-SALMONN 2，这是一款采用低秩自适应（LoRA）技术的先进视听大语言模型，旨在通过直接偏好优化（DPO）增强对包含配音的视频生成详细且准确描述的能力。为了优化视频描述的完整性与准确性，研究团队提出了全新的评估指标，并引入了一种创新的多轮 DPO（mrDPO）方法，该方法通过周期性更新参考模型、合并并重新初始化 LoRA 模块以实现参数的高效更新，并利用标准视频描述（Ground-truth）作为引导以确保训练过程的稳定性。针对 mrDPO 可能导致模型丧失非描述类能力的灾难性遗忘问题，研究者进一步提出了“重生微调”（rebirth tuning）策略，即以 mrDPO 训练后的模型生成的描述作为监督标签，对 DPO 前的模型进行微调。实验结果表明，mrDPO 显著提升了 video-SALMONN 2 的生成精度，将其全局和局部错误率分别降低了 40% 和 20%，同时减少了 35% 的内容重复率。最终，仅拥有 70 亿参数的 video-SALMONN 2 在视频描述任务上超越了 GPT-4o 和 Gemini-1.5-Pro 等顶尖模型，并在广泛使用的视频问答基准测试中，其表现与同规模的最先进模型相比极具竞争力。该研究的代码、模型权重及训练测试数据将在论文接收后公开发布。",
    "image": "",
    "code": "https://video-salmonn-2.github.io/"
  },
  {
    "title": "Silkie: Preference Distillation for Large Visual Language Models",
    "venue": "arXiv",
    "date": "2023",
    "link": "https://arxiv.org/pdf/2312.10665.pdf",
    "source": "auto-script",
    "tags": [
      "RLHF"
    ],
    "authors": "Lei Li, Zhihui Xie, Mukai Li, Shunian Chen, Peiyi Wang, et al.",
    "description": "本文探讨了大型视觉语言模型（LVLMs）的偏好蒸馏技术，旨在提升模型在视觉语境下生成既有帮助又真实可信回答的能力。研究者首先利用人工智能标注构建了一个名为 VLFeedback 的视觉语言反馈数据集，其响应内容由 12 个不同性能的 LVLMs 针对来自多个数据集的多模态指令生成，并采用 GPT-4V 从帮助性、视觉真实性及伦理考量三个维度对这些输出进行评估。随后，通过直接偏好优化（DPO）方法将这些偏好监督信息蒸馏至 Qwen-VL-Chat 模型中，由此产生的 Silkie 模型在 MME 基准测试的感知和认知能力上分别实现了 6.9% 和 9.5% 的相对提升。此外，Silkie 在 MMHal-Bench 基准测试中取得了 3.02 的新最高分，有效减少了模型幻觉。进一步分析表明，利用 VLFeedback 数据集进行 DPO 优化主要增强了 LVLMs 的细粒度感知和复杂认知能力，相比于仅使用人工标注的偏好数据集，该方法带来了更全面的性能提升。",
    "image": "",
    "code": "https://vlf-silkie.github.io/"
  },
  {
    "title": "RoVRM: A Robust Visual Reward Model Optimized via Auxiliary Textual Preference Data",
    "venue": "arXiv",
    "date": "2024",
    "link": "https://arxiv.org/pdf/2408.12109",
    "source": "auto-script",
    "tags": [
      "RLHF"
    ],
    "authors": "Chenglong Wang, Yang Gan, Yifu Huo, Yongyu Mu, et al.",
    "description": "本文介绍了一种名为 RoVRM 的稳健视觉奖励模型，旨在解决大型视觉语言模型（LVLMs）因缺乏与人类偏好对齐而导致的幻觉及内容误导等问题。针对视觉偏好数据稀缺这一制约视觉奖励模型（VRM）训练的核心瓶颈，RoVRM 创新性地引入了辅助文本偏好数据，并采用了三阶段渐进式训练方案以及基于最优传输（Optimal Transport）的偏好数据选择策略，从而有效缓解了数据匮乏的困境。研究团队在 LLaVA-1.5-7B 和 13B 模型上进行了广泛的视觉语言任务实验，结果表明 RoVRM 的表现一致优于传统的视觉奖励模型。此外，实验进一步证明，该研究所提出的渐进式训练和数据选择方法在提升模型性能方面，相较于直接偏好优化（DPO）等基于排名的主流对齐技术具有更为显著且稳定的优势。",
    "image": "",
    "code": ""
  },
  {
    "title": "TS-LLaVA: Constructing Visual Tokens through Thumbnail-and-Sampling for Training-Free Video Large Language Models",
    "venue": "arXiv",
    "date": "2024",
    "link": "https://arxiv.org/pdf/2411.11066",
    "source": "auto-script",
    "tags": [
      "Others"
    ],
    "authors": "Tingyu Qu, Mingxiao Li, Tinne Tuytelaars, Marie-Francine Moens",
    "description": "本文介绍了 TS-LLaVA，这是一种旨在通过扩展图像大语言模型（LLMs）来解决视频理解任务的新方法，旨在克服高质量视频-文本配对数据稀缺带来的训练难题。考虑到图像与视频之间存在的高度相似性，研究人员探索了从多帧中压缩视觉标记（tokens）的有效策略，以充分利用预训练图像模型的强大能力。在深入分析现有免训练视频模型压缩策略局限性的基础上，TS-LLaVA 创新性地提出了“缩略图与采样”（Thumbnail-and-Sampling）策略：该方法从所有输入帧中选取少量等间距帧构建一张缩略图以提供精细的视觉线索，并辅以从全视频帧中提取的采样视觉标记。实验结果表明，该方法在多个基准测试中刷新了免训练视频大语言模型的最先进性能，其中 34B 版本的模型在 MVBench 基准上超越了 GPT-4V，并在极具挑战性的 MLVU 基准上达到了与 72B 规模的基于训练的视频模型 Video-LLaMA2 相当的水平。",
    "image": "",
    "code": "https://github.com/tingyu215/TS-LLaVA"
  },
  {
    "title": "Safety Fine-Tuning at (Almost) No Cost: A Baseline for Vision Large Language Models",
    "venue": "arXiv",
    "date": "2024",
    "link": "https://arxiv.org/pdf/2402.02207.pdf",
    "source": "auto-script",
    "tags": [
      "Others"
    ],
    "authors": "Yongshuo Zong, Ondrej Bohdal, Tingyang Yu, Yongxin Yang, Timothy Hospedales",
    "description": "本文针对当前视觉大语言模型（VLLMs）容易产生有害内容且在简单破解攻击面前表现脆弱的问题进行了研究。通过初步分析发现，这类安全隐患源于视觉-语言指令微调阶段有害数据的引入，以及微调过程导致模型遗忘了底层语言模型此前习得的安全对齐策略。为解决这一问题，研究者构建了一个涵盖多种有害类别的视觉-语言安全指令遵循数据集 VLGuard。实验结果表明，将该数据集整合到标准微调流程中，或用于事后安全微调，均能有效提升 VLLMs 的安全性，且在增强安全性的同时，对模型原本的帮助性影响极小甚至有所提升。VLGuard 数据集的通用性使其能够成为测试现有模型安全性、训练新模型或加固预训练模型的宝贵资源。实证研究进一步证明，经过微调的 VLLMs 能有效拒绝不安全指令，并使多种黑盒对抗攻击的成功率显著降低，在许多场景下甚至趋近于零。",
    "image": "",
    "code": "https://github.com/ys-zong/VLGuard"
  },
  {
    "title": "VCoder: Versatile Vision Encoders for Multimodal Large Language Models",
    "venue": "arXiv",
    "date": "2023",
    "link": "https://arxiv.org/pdf/2312.14233.pdf",
    "source": "auto-script",
    "tags": [
      "Others"
    ],
    "authors": "Jitesh Jain, Jianwei Yang, Humphrey Shi",
    "description": "本文提出了 VCoder 框架，旨在解决现有捕捉多模态大语言模型（MLLMs）在识别或计数图像实体等基础感知任务上表现不佳的问题，通过为模型配备“感知之眼”来提升其视觉感知与推理能力。研究者将分割图或深度图等额外感知模态输入 VCoder，从而显著增强了 MLLMs 对视觉世界的理解深度。为了训练并评估模型的物体级感知能力，研究团队利用 COCO 数据集及现成视觉感知模型的输出，构建了全新的“COCO分割文本”（COST）数据集，并配套设计了一套专门用于衡量感知性能的评估指标。大量实验证据表明，VCoder 在物体级感知技能上相较于现有的 MLLMs（包括 GPT-4V）具有明显优势。为了推动相关领域的进一步研究，该项目已将其数据集、代码和模型权重全部开源。",
    "image": "",
    "code": "https://github.com/SHI-Labs/VCoder"
  },
  {
    "title": "Prompt Highlighter: Interactive Control for Multi-Modal LLMs",
    "venue": "arXiv",
    "date": "2023",
    "link": "https://arxiv.org/pdf/2312.04302.pdf",
    "source": "auto-script",
    "tags": [
      "Others"
    ],
    "authors": "Yuechen Zhang, Shengju Qian, Bohao Peng, Shu Liu, Jiaya Jia",
    "description": "本文针对多模态大语言模型（LLMs & VLMs）推理中的一个核心问题——显式可控文本生成进行了深入研究。由于这类模型采用自回归生成机制，往往存在可解释性差以及过度依赖提示词（prompt）内容的问题，虽然调整提示词格式能改善输出，但为每个任务设计精准的提示词既困难又低效。为此，研究者提出了一种名为 Prompt Highlighter 的创新推理方法，允许用户通过突出显示（highlight）特定的提示词片段，交互式地控制模型在生成过程中的关注焦点。受无分类器扩散引导（classifier-free diffusion guidance）的启发，该方法基于突出显示的标记构建常规与无条件上下文对，证明了模型的自回归生成过程可以以无分类器的方式进行引导。特别地，研究发现通过注意力权重利用突出标记来引导模型，能够产生更符合预期的输出。该方法无需额外训练即可适配现有的 LLMs 和 VLMs，并实现出色的定制化生成效果。实验结果证实，Prompt Highlighter 能有效使模型聚焦于输入上下文并生成可靠内容，在未经调优的 LLaVA-v1.5 上，该方法在 MMBench 测试中获得 70.7 分，在 MME 感知测试中获得 1552.5 分。",
    "image": "",
    "code": "https://github.com/JIA-Lab-research/Prompt-Highlighter"
  },
  {
    "title": "Planting a SEED of Vision in Large Language Model",
    "venue": "arXiv",
    "date": "2023",
    "link": "https://arxiv.org/pdf/2307.08041.pdf",
    "source": "auto-script",
    "tags": [
      "Others"
    ],
    "authors": "Yuying Ge, Yixiao Ge, Ziyun Zeng, Xintao Wang, Ying Shan",
    "description": "本文介绍了 SEED，这是一种精心设计的图像分词器（tokenizer），它赋予了大语言模型（LLMs）同时具备“观察”与“绘画”的突现能力。针对此前量化视觉分词器在多模态理解（逊于 BLIP-2 等）或生成（逊于 Stable Diffusion 等）性能不佳的困境，研究团队坚持利用离散分词器统一视觉与文本表示的潜力，并提出了两个关键设计原则以简化与 LLM 的对齐：首先，图像分词应脱离 2D 物理位置，转而采用符合 LLM 自回归预测机制的 1D 因果依赖生成；其次，图像分词应捕捉与单词抽象程度一致的高层语义，并在训练阶段同时针对辨别性和重建能力进行优化。得益于此，现成的 LLM 仅需通过高效的 LoRA 微调整合 SEED，即可实现图像到文本和文本到图像的双向生成。该版本的 SEED 仅利用 500 万对公开图文数据，在 64 张 V100 GPU 上历时 5.7 天训练完成，初步研究强调了离散视觉分词器在构建多功能多模态大模型中的巨大潜力以及开发合适图像分词器的重要意义。",
    "image": "",
    "code": "https://github.com/AILab-CVC/SEED"
  },
  {
    "title": "Can Large Pre-trained Models Help Vision Models on Perception Tasks?",
    "venue": "arXiv",
    "date": "2023",
    "link": "https://arxiv.org/pdf/2306.00693.pdf",
    "source": "auto-script",
    "tags": [
      "Others"
    ],
    "authors": "Ning Ding, Yehui Tang, Zhongqian Fu, Chao Xu, Kai Han, Yunhe Wang",
    "description": "本文提出了一个名为 GPT4Image 的全新学习框架，旨在利用现成的预训练大模型来增强常规视觉模型（如 CNN 和 ViT）在图像分类等感知任务中的表征能力。尽管大语言模型（LLMs）展现了卓越的多模态理解能力，但其高昂的内存和计算成本使得传统轻量化模型在许多视觉任务中依然不可或缺。GPT4Image 框架的核心在于提取大模型的知识以辅助传统模型学习更优的表征：首先，通过提示多模态大模型为训练图像生成高质量且详尽的文本描述；随后，将这些描述输入预训练编码器以提取包含丰富语义的文本嵌入；在训练过程中，这些文本嵌入作为额外的监督信号，与视觉模型学习到的图像表征进行对齐。这种对齐过程借助预训练大模型的语义理解能力，显著提升了视觉模型的性能。通过在多种视觉感知任务和异构模型架构上进行的大量实验，验证了该算法在提升普通视觉模型效果方面的有效性。",
    "image": "",
    "code": "https://github.com/huawei-noah/Efficient-Computing/tree/master/GPT4Image/"
  },
  {
    "title": "Contextual Object Detection with Multimodal Large Language Models",
    "venue": "arXiv",
    "date": "2023",
    "link": "https://arxiv.org/pdf/2305.18279.pdf",
    "source": "auto-script",
    "tags": [
      "Others"
    ],
    "authors": "Yuhang Zang, Wei Li, Jun Han, Kaiyang Zhou, Chen Change Loy",
    "description": "本文针对多模态大语言模型（MLLMs）在图像描述和问答方面表现出色但缺乏目标检测这一核心感知能力的问题，提出了“上下文目标检测”（Contextual Object Detection）这一新型研究课题，旨在理解不同人机交互语境下的可见目标。研究涵盖了语言完型填空、视觉描述和问答三个代表性场景，并为此开发了统一的多模态模型 ContextDET。该模型能够对视听上下文进行端到端的可微建模，从而实现视觉目标的定位、识别及其与语言输入的关联。ContextDET 由视觉编码器、预训练大语言模型和视觉解码器三个核心子模块组成，通过一种创新的“先生成后检测”框架，能够检测出人类词汇表中的任何目标词汇。大量实验证明，ContextDET 在其配套的 CODE 基准测试、开放词汇检测以及指代图像分割任务中均展现出显著优势。",
    "image": "",
    "code": "https://github.com/yuhangzang/ContextDET"
  },
  {
    "title": "Generating Images with Multimodal Language Models",
    "venue": "arXiv",
    "date": "2023",
    "link": "https://arxiv.org/pdf/2305.17216.pdf",
    "source": "auto-script",
    "tags": [
      "Others"
    ],
    "authors": "Jing Yu Koh, Daniel Fried, Ruslan Salakhutdinov",
    "description": "本文提出了一种将冻结的纯文本大语言模型（LLMs）与预训练图像编码器及解码器模型融合的方法，通过在它们各自的嵌入空间之间建立映射来实现多模态交互。该模型展示了广泛的多模态能力，包括图像检索、原创图像生成以及多模态对话，是首个能够基于任意交错的图文输入生成连贯图像及文本输出的方法。为了在图像生成方面获得优异表现，研究者设计了一种高效的映射网络，将大语言模型的隐藏表征转化为现成文本到图像生成模型的嵌入空间，从而充分利用大语言模型强大的文本表征能力来指导视觉输出。实验表明，该方法在处理更长、更复杂的语言描述任务时，性能优于基线生成模型。此外，该模型还具备从预设数据集中检索图像的能力，并能在推理时通过一个基于大语言模型隐藏表征学习到的决策模块，自主判断应当执行检索还是生成操作。相比以往的多模态语言模型，该模型展现了更全面的功能，能够处理图文混合输入并产生检索图像、生成图像及生成文本，在多项衡量上下文依赖性的文本到图像任务中超越了非大语言模型架构的生成模型。",
    "image": "",
    "code": "https://jykoh.com/gill"
  },
  {
    "title": "On Evaluating Adversarial Robustness of Large Vision-Language Models",
    "venue": "arXiv",
    "date": "2023",
    "link": "https://arxiv.org/pdf/2305.16934.pdf",
    "source": "auto-script",
    "tags": [
      "Others"
    ],
    "authors": "Yunqing Zhao, Tianyu Pang, Chao Du, et al.",
    "description": "本文针对 GPT-4 等大型视觉语言模型（VLMs）在多模态生成任务中的安全风险进行了深入研究，指出攻击者可以通过操纵最为脆弱的模态（如视觉）来规避系统的安全机制。研究重点评估了开源大型 VLMs 在最贴近现实且高风险的黑盒设置下的鲁棒性，即攻击者仅能访问系统接口，并试图诱导模型生成特定的目标响应。研究者首先针对 CLIP 和 BLIP 等预训练模型构建了目标对抗样本，随后将这些样本迁移至 MiniGPT-4、LLaVA、UniDiffuser、BLIP-2 和 Img2Prompt 等多种 VLMs。实验观察到，通过对这些模型进行黑盒查询，可以进一步提升对抗样本的攻击效能，从而以惊人的成功率诱导模型输出预设的违规响应。这些发现为大型视觉语言模型的对抗脆弱性提供了定量分析，并强调了在实际部署前对其潜在安全缺陷进行更彻底审查的紧迫性。",
    "image": "",
    "code": "https://github.com/yunqing-me/AttackVLM"
  },
  {
    "title": "Grounding Language Models to Images for Multimodal Inputs and Outputs",
    "venue": "ICML",
    "date": "2023",
    "link": "https://arxiv.org/pdf/2301.13823.pdf",
    "source": "auto-script",
    "tags": [
      "Others"
    ],
    "authors": "Jing Yu Koh, Ruslan Salakhutdinov, Daniel Fried",
    "description": "本文提出了一种高效的方法，旨在将预训练的纯文本语言模型与视觉领域相结合，使其能够处理任意交错的图文数据，并生成带有检索图像的文本内容。该方法充分利用了大语言模型通过大规模纯文本预训练所获得的上下文学习和自由文本生成能力，通过保持语言模型处于冻结状态，仅微调输入和输出线性层来实现跨模态交互。这使得模型不仅能处理复杂的图文混合输入，还能在生成自由文本的同时穿插检索出的相关图像。研究结果显示，该方法在上下文图像检索和多模态对话等关联任务中表现出了强大的零样本性能，并展现了出色的交互能力。由于该方法兼容任何现成的语言模型，它为在视觉关联场景中有效利用预训练语言模型提供了一种普适且高效的解决方案。",
    "image": "",
    "code": "https://jykoh.com/fromage"
  },
  {
    "title": "Identifiability Results for Multimodal Contrastive Learning",
    "link": "https://arxiv.org/abs/2303.09166",
    "venue": "ICLR",
    "date": "2023",
    "tags": [
      "Representation"
    ],
    "source": "pliang_list",
    "code": "https://github.com/imantdaunhawer/multimodal-contrastive-learning",
    "authors": "Imant Daunhawer, Alice Bizeul, Emanuele Palumbo, Alexander Marx, Julia E. Vogt",
    "description": "本文探讨了多模态对比学习中的可辨识性（Identifiability）问题，旨在为这一支撑图像-文本对等表征学习的核心技术提供理论基础。尽管对比学习在实践中取得了巨大成功，但其有效性的内在机制尚未被完全理解。以往的研究主要集中在单一生成机制的“多视角”（multi-view）设置上，而本文将其扩展到了更具普适性的“多模态”场景，即由不同机制（如摄像头与麦克风）产生的数据。研究者通过重新定义包含模态特定隐变量的生成过程，证明了对比学习能够实现对不同模态间共享潜在因子的“块辨识”（block-identify），即使在这些因子之间存在非平凡的依赖关系时依然有效。通过数值模拟以及在复杂的图文对数据集上的实验，本文验证了上述理论结果，不仅解释了多模态对比学习在何种设置下切实有效，也为未来的多模态表征学习研究提供了坚实的理论支撑。",
    "image": ""
  },
  {
    "title": "Unpaired Vision-Language Pre-training via Cross-Modal CutMix",
    "link": "https://arxiv.org/abs/2206.08919",
    "venue": "ICML",
    "date": "2022",
    "tags": [
      "Representation"
    ],
    "source": "pliang_list",
    "code": "",
    "authors": "Teng Wang, Wenhao Jiang, Zhichao Lu, et al.",
    "description": "本文提出了一种名为跨模态 CutMix（CMC）的数据增强方法，旨在减少视觉语言预训练（VLP）对高质量图文配对数据的依赖，通过直接利用大规模纯文本和纯图像语料库实现隐式跨模态对齐学习。具体而言，CMC 将文本视图中的自然语言句子转化为多模态视图，通过将句子中具有视觉语义的词随机替换为语义相似的多样化图像块。该方法具备多项优势：首先，它在保持语义完整性的同时增强了数据多样性，有效应对了对齐数据稀缺的问题；其次，通过在单模态数据中引入跨模态噪声，引导模型学习跨模态的标记级（token-level）交互以实现更好的去噪。在此基础上，研究团队进一步提出了一种名为 VLMixer 的新型非配对 VLP 预训练方法，该方法将 CMC 与对比学习相结合，旨在拉近单模态视图与多模态视图之间的距离，从而在不同模态间实现更优的实例级对齐。在五个下游任务上的广泛实验表明，VLMixer 的性能超越了此前最先进的非配对视觉语言预训练方法。",
    "image": ""
  },
  {
    "title": "Balanced Multimodal Learning via On-the-fly Gradient Modulation",
    "link": "https://arxiv.org/abs/2203.15332",
    "venue": "CVPR",
    "date": "2022",
    "tags": [
      "Representation"
    ],
    "source": "pliang_list",
    "code": "https://github.com/GeWu-Lab/OGM-GE_CVPR2022",
    "authors": "Xiaokang Peng, Yake Wei, Andong Deng, Dong Wang, Di Hu",
    "description": "本文深入探讨了多模态学习中各输入模态未能得到充分利用的问题，指出尽管多模态模型通常优于单模态模型，但现有的判别式模型往往因采用统一的学习目标而导致优化失衡。具体而言，当某一模态在特定场景中占据主导地位时（例如刮风场景中的声音或绘图场景中的视觉），其他模态的表征能力往往会出现欠优化现象。为了缓解这种优化不平衡，研究者提出了一种“即时梯度调制”（on-the-fly gradient modulation）技术，通过监控各模态对学习目标的贡献差异，动态且自适应地调节每个模态的优化进度。此外，为了防止梯度调制可能导致的泛化能力下降，该方法还引入了动态变化的特征高斯噪声。实验结果表明，该策略在多种多模态任务上均显著优于传统的融合方法，且作为一种通用的简单策略，它能有效提升现有各种多模态模型的性能，充分证明了其有效性与普适性。",
    "image": ""
  },
  {
    "title": "Unsupervised Voice-Face Representation Learning by Cross-Modal Prototype Contrast",
    "link": "https://arxiv.org/abs/2204.14057",
    "venue": "IJCAI",
    "date": "2021",
    "tags": [
      "Representation"
    ],
    "source": "pliang_list",
    "code": "https://github.com/Cocoxili/CMPC",
    "authors": "Boqing Zhu, Kele Xu, Changjian Wang, et al.",
    "description": "本文提出了一种从说话人视频中学习声纹-人脸表征的方法，且无需任何身份标签。针对以往研究在跨模态实例判别任务中忽略视频语义内容、导致引入伪负例噪声，以及在真实数据中音视频自然关联较弱导致正例偏离的问题，研究者提出了跨模态原型对比学习（CMPC）框架。该方法一方面通过在不同模态下进行无监督聚类，构建语义层面的正例，从而学习类内不变性；另一方面，通过对比跨模态实例相似度与跨模态原型相似度，动态重新校准不可学实例对整体损失的贡献，从而抵御伪负例和偏离正例的负面影响。实验结果表明，该方法在多项声纹-人脸关联评估协议下均超越了现有的最先进无监督方法。此外，在低资源监督设定下，该方法相较于传统的实例级对比学习也展现出了显著的性能提升。",
    "image": ""
  },
  {
    "title": "Towards a Unified Foundation Model: Jointly Pre-Training Transformers on Unpaired Images and Text",
    "link": "https://arxiv.org/abs/2112.07074",
    "venue": "arXiv",
    "date": "2021",
    "tags": [
      "Representation"
    ],
    "source": "pliang_list",
    "code": "",
    "authors": "Qing Li, Boqing Gong, Yin Cui, et al.",
    "description": "本文探索了构建一种能够同时适配纯视觉和纯文本任务的统一基础模型的可行性。研究者以 BERT 和 ViT 为基础，设计了一个由模态特定分词器、共享 Transformer 编码器以及任务特定输出头组成的统一架构。为了在非配对的图像和文本数据上高效进行联合预训练，文章提出了两项核心技术：首先，利用预先训练好的 BERT 和 ViT 分别作为老师模型，通过知识蒸馏为联合训练提供额外且准确的监督信号；其次，提出了一种创新的梯度掩码（gradient masking）策略，以平衡图像和文本预训练损失带来的参数更新量。通过在图像分类和自然语言理解任务上的微调评估，实验结果表明，该统一基础模型在视觉和文本任务中均表现出惊人的效果，且所提出的知识蒸馏与梯度掩码策略能有效提升性能，使其接近独立训练的单模态模型水平。",
    "image": ""
  },
  {
    "title": "FLAVA: A Foundational Language And Vision Alignment Model",
    "link": "https://arxiv.org/abs/2112.04482",
    "venue": "arXiv",
    "date": "2021",
    "tags": [
      "Representation",
      "Transformers"
    ],
    "source": "pliang_list",
    "code": "https://flava-model.github.io/",
    "authors": "Amanpreet Singh, Ronghang Hu, Vedanuj Goswami, et al.",
    "description": "本文介绍了 FLAVA，这是一种旨在成为通用“基础”的多模态模型，旨在解决现有视觉及视听模型通常只能在跨模态对比学习或早期融合多模态学习中择其一，且往往仅针对特定模态或任务的局限性。研究者认为，一个真正的视觉与语言基础模型应当能够同时胜任纯视觉任务、纯语言任务以及跨模态和多模态的视听综合任务。FLAVA 通过单一的全局模型架构实现了对所有这些模态的覆盖，并在涵盖上述目标模态的 35 项任务中展示了令人印象深刻的性能表现。",
    "image": ""
  },
  {
    "title": "Transformer is All You Need: Multimodal Multitask Learning with a Unified Transformer",
    "link": "https://arxiv.org/abs/2102.10772",
    "venue": "arXiv",
    "date": "2021",
    "tags": [
      "Representation",
      "Pretraining"
    ],
    "source": "pliang_list",
    "code": "",
    "authors": "Ronghang Hu, Amanpreet Singh",
    "description": "本文提出了 UniT（Unified Transformer），这是一种能够同时学习跨领域核心任务的统一 Transformer 模型，涵盖了从目标检测到自然语言理解及多模态推理的广泛应用。该模型基于 Transformer 的编码器-解码器架构，通过特定模态的编码器对输入进行处理，并利用共享解码器在编码后的表征基础上进行任务预测，最后衔接任务特定的输出头。整个模型采用各任务损失函数进行端到端的联合训练。与以往基于 Transformer 的多任务学习研究不同，UniT 在所有任务间共享模型参数，而非为每个任务单独微调模型，从而能够处理跨领域且复杂度更高的多样化任务。实验结果表明，UniT 在 8 个数据集上的 7 项任务联合学习中，以显著更少的参数量在各项任务中均取得了强劲的性能表现。",
    "image": ""
  },
  {
    "title": "MultiBench: Multiscale Benchmarks for Multimodal Representation Learning",
    "link": "https://arxiv.org/abs/2107.07502",
    "venue": "NeurIPS",
    "date": "2021",
    "tags": [
      "Representation"
    ],
    "source": "pliang_list",
    "code": "https://github.com/pliang279/MultiBench",
    "authors": "Paul Pu Liang, Yiwei Lyu, Xiang Fan, et al.",
    "description": "本文介绍了 MultiBench，这是一个系统化且统一的大规模多模态基准测试，旨在解决多模态研究在跨域泛化、模型复杂度以及对噪声或缺失模态的鲁棒性等方面资源匮乏的问题。该基准涵盖了 15 个数据集、10 种模态、20 个预测任务以及包括多媒体、机器人、医疗等在内的 6 个核心研究领域。MultiBench 提供了一套自动化的端到端机器学习流水线，实现了数据加载、实验设置和模型评估的标准化。为了进行全方位评估，它还引入了一套综合方法论，用于衡量模型的泛化能力、时空复杂度及模态鲁棒性。此外，MultiBench 还随附了 20 种核心多模态学习方法的标准化实现，实验发现，仅通过在不同领域间迁移应用这些方法，就能在 9 个数据集上刷新最先进的性能记录。总之，MultiBench 的推出是统一多模态研究领域中分散工作的里程碑，它在确保易用性、可访问性和可重复性的同时，为深入理解多模态模型的潜力和局限性铺平了道路，其相关代码、基准及排行榜已向社区公开发布并持续维护。",
    "image": ""
  },
  {
    "title": "Perceiver: General Perception with Iterative Attention",
    "link": "https://arxiv.org/abs/2103.03206",
    "venue": "ICML",
    "date": "2021",
    "tags": [
      "Representation",
      "Transformers"
    ],
    "source": "pliang_list",
    "code": "https://github.com/deepmind/deepmind-research/tree/master/perceiver",
    "authors": "Andrew Jaegle, Felix Gimeno, Andrew Brock, et al.",
    "description": "本文介绍了 Perceiver，这是一种基于 Transformer 架构的新型感知模型，旨在打破传统深度学习模型对特定模态（如视觉模型中对局部网格结构的依赖）的假设限制。生物系统能够同时处理视觉、听觉、触觉等多种高维输入，而 Perceiver 正是效仿这一特性，通过极少的架构假设来处理不同模态间的关系，并能像卷积神经网络一样扩展到处理数十万个输入点。该模型的核心在于引入了一种非对称注意力机制，通过将海量输入迭代地蒸馏到一个紧凑的潜空间瓶颈（latent bottleneck）中，从而解决了大规模输入带来的计算压力。实验结果表明，该架构在图像、点云、音频、视频以及视听结合等多种模态的分类任务中，均能达到甚至超越各领域的专用模型；在 ImageNet 测试中，Perceiver 在完全不使用 2D 卷积、直接处理 50,000 个像素点的情况下，取得了与 ResNet-50 和 ViT 相当的性能，并在 AudioSet 的所有模态测试中展现出极强的竞争力。",
    "image": ""
  },
  {
    "title": "Learning Transferable Visual Models From Natural Language Supervision",
    "link": "https://arxiv.org/abs/2103.00020",
    "venue": "arXiv",
    "date": "2021",
    "tags": [
      "Representation"
    ],
    "source": "pliang_list",
    "code": "https://github.com/OpenAI/CLIP",
    "authors": "Alec Radford, Jong Wook Kim, Chris Hallacy, et al.",
    "description": "本文介绍了 CLIP（对比语言-图像预训练），这是一种打破传统计算机视觉系统局限性的创新方法，旨在通过直接从互联网上获取的 4 亿对“图像-文本”原始数据中学习，而非依赖预定义的固定类别标签进行监督。该研究证明，通过执行“预测哪段描述与哪张图像相匹配”这一简单的预训练任务，可以高效且大规模地从零开始学习最先进的图像表征。预训练完成后，模型能够利用自然语言来引用已习得的视觉概念或描述新概念，从而实现向下游任务的零样本迁移（Zero-shot Transfer）。通过在 OCR、视频动作识别、地理定位以及各种细粒度物体分类等 30 多个不同视觉数据集上进行测试，结果表明 CLIP 在大多数任务中表现出色，且在无需任何特定数据集训练的情况下，即可与全监督基线模型相媲美。例如，CLIP 在 ImageNet 上的零样本准确率达到了原始 ResNet-50 的水平，而完全没有使用该数据集原本庞大的 128 万张训练样本。",
    "image": ""
  },
  {
    "title": "VinVL: Revisiting Visual Representations in Vision-Language Models",
    "link": "https://arxiv.org/abs/2101.00529",
    "venue": "arXiv",
    "date": "2021",
    "tags": [
      "Representation"
    ],
    "source": "pliang_list",
    "code": "https://github.com/pzzhang/VinVL",
    "authors": "Pengchuan Zhang, Xiujun Li, Xiaowei Hu, et al.",
    "description": "本文详细研究了如何提升视觉语言（VL）任务中的视觉表征，并开发了一种改进的目标检测模型，旨在提供以物体为中心的图像表征。相比于广泛使用的“自下而上与自上而下”（Bottom-Up and Top-Down）模型，新模型规模更大、针对 VL 任务进行了更优的设计，并在融合了多个公共标注数据集的大规模语料库上进行了预训练，从而能够生成涵盖更丰富视觉对象和概念的表征。以往的 VL 研究多聚焦于改进视觉-语言融合模型，而忽略了目标检测模型的优化，本文则证明了视觉特征在 VL 模型中具有至关重要的作用。在实验中，研究者将新模型生成的视觉特征输入到基于 Transformer 的融合模型 Oscar 中，并利用改进的 VILLA 方法进行预训练，随后在广泛的下游任务上进行微调。结果表明，这种新的视觉特征显著提升了所有 VL 任务的性能，并在七个公共基准测试中刷新了最先进的记录。",
    "image": ""
  },
  {
    "title": "12-in-1: Multi-Task Vision and Language Representation Learning",
    "link": "https://arxiv.org/abs/1912.02315",
    "venue": "CVPR",
    "date": "2020",
    "tags": [
      "Representation"
    ],
    "source": "pliang_list",
    "code": "https://github.com/facebookresearch/vilbert-multi-task",
    "authors": "Jiasen Lu, Vedanuj Goswami, Marcus Rohrbach, Devi Parikh, Stefan Lee",
    "description": "本文针对视觉与语言研究中各独立任务和数据集往往被孤立研究的现状，探讨了成功完成这些任务所需的视觉驱动语言理解能力之间存在的显著重叠。研究通过开发一种大规模多任务训练机制，构建了一个能同时处理来自四大类任务（包括视觉问答、基于描述的图像检索、指代词定位以及多模态验证）的12个数据集的单一模型。与独立训练的单任务模型相比，该方法将总参数量从约30亿大幅缩减至2.7亿，同时在所有任务上的平均性能提升了2.05个百分点。通过该多任务框架，研究团队深入分析了不同任务联合训练的效果，并进一步证明，以该多任务模型为基础进行特定任务的微调，能够取得达到或超越现有最先进水平（SOTA）的性能表现。",
    "image": ""
  },
  {
    "title": "Watching the World Go By: Representation Learning from Unlabeled Videos",
    "link": "https://arxiv.org/abs/2003.07990",
    "venue": "arXiv",
    "date": "2020",
    "tags": [
      "Representation"
    ],
    "source": "pliang_list",
    "code": "https://github.com/danielgordon10/vince",
    "authors": "Daniel Gordon, Kiana Ehsani, Dieter Fox, Ali Farhadi",
    "description": "本文针对近期单图无监督表征学习技术中存在的局限性进行了研究，指出当前基于实例判别的方法主要依赖裁剪、颜色抖动等人工数据增强手段，这些手段仅能对图像进行表层修改，无法反映物体在现实中的真实变化（如遮挡、变形、视角切换等）。研究者提出视频能够自然且免费地提供这些真实世界的增强信息，展现物体的全新视角、变形过程，甚至能连接语义相似但视觉特征截然不同的概念。为此，本文提出了“视频噪声对比估计”（Video Noise Contrastive Estimation）方法，旨在利用无标签视频学习强有力且可迁移的单图表征。实验结果表明，该方法在多种时间相关和非时间相关的任务中，不仅优于现有的无监督单图学习技术，甚至超越了完全监督下的 ImageNet 预训练效果。",
    "image": ""
  },
  {
    "title": "Learning Video Representations using Contrastive Bidirectional Transformer",
    "link": "https://arxiv.org/abs/1906.05743",
    "venue": "arXiv",
    "date": "2019",
    "tags": [
      "Representation"
    ],
    "source": "pliang_list",
    "code": "",
    "authors": "Chen Sun, Fabien Baradel, Kevin Murphy, Cordelia Schmid",
    "description": "本文提出了一种视频特征的自监督学习方法，相比现有方法，该方法在视频分类、字幕生成和分割等下游任务上的性能得到了显著提升。研究者将用于处理文本序列的 BERT 模型扩展到了实值特征向量序列领域，并通过引入噪声对比估计（NCE）损失函数取代了传统的 Softmax 损失。此外，文章展示了如何从视觉特征序列以及源自自动语音识别（ASR）的单词序列中协同学习表征，并证明这种跨模态训练（在可行的情况下）能够进一步增强模型的效果。",
    "image": ""
  },
  {
    "title": "Visual Concept-Metaconcept Learning",
    "link": "https://papers.nips.cc/paper/8745-visual-concept-metaconcept-learning.pdf",
    "venue": "NeurIPS",
    "date": "2019",
    "tags": [
      "Representation"
    ],
    "source": "pliang_list",
    "code": "http://vcml.csail.mit.edu/",
    "authors": "Chi Han, Jiayuan Mao, Chuang Gan, Joshua B. Tenenbaum, Jiajun Wu",
    "description": "本文介绍了视觉概念-元概念学习器（VCML），这是一种旨在从图像及关联的问答对中同时学习概念与元概念的创新方法。该研究受人类推理能力的启发，即人类不仅能识别“红色”和“绿色”等具体视觉概念，还能理解它们描述的是物体的同一种属性（即颜色）这一元概念。VCML 的核心在于利用视觉概念与元概念之间的双向关联：一方面，视觉表征为预测未见概念对之间的关系提供了定位线索，例如在得知“红色”和“绿色”具有相同属性后，模型可以泛化并理解“立方体”和“球体”同样描述了物体的同一种属性（形状）；另一方面，元概念知识使模型能够从受限、含噪甚至有偏的数据中高效学习视觉概念，例如仅通过少量“紫色立方体”的样本，模型就能准确理解新颜色“紫色”是指代颜色特征而非形状特征。在合成数据集和真实世界数据集上的评估均证实了该方法的有效性。",
    "image": ""
  },
  {
    "title": "OmniNet: A Unified Architecture for Multi-modal Multi-task Learning",
    "link": "https://arxiv.org/abs/1907.07804",
    "venue": "arXiv",
    "date": "2019",
    "tags": [
      "Representation"
    ],
    "source": "pliang_list",
    "code": "https://github.com/subho406/OmniNet",
    "authors": "Subhojeet Pramanik, Priyanka Agrawal, Aman Hussain",
    "description": "本文介绍了 OmniNet，这是一种基于 Transformer 的扩展统一架构，旨在处理涉及图像、文本和视频等多种模态的任务。该架构引入了一种创新的时空缓存（Spatio-Temporal Cache）机制，使模型不仅能学习时间输入序列对应的隐藏状态，还能捕捉输入的空间维度信息。OmniNet 支持多模态输入以及异步多任务学习，其实例可以同时学习并执行词性标注、图像字幕生成、视觉问答和视频动作识别等任务。研究表明，与独立训练这些任务相比，联合训练能使模型压缩约三倍，同时保持性能不减。此外，在特定模态上预训练的 OmniNet 能够有效辅助视频字幕生成和视频问答等未见任务的学习，充分展示了其时空缓存自注意力机制强大的泛化能力。",
    "image": ""
  },
  {
    "title": "Learning Representations by Maximizing Mutual Information Across Views",
    "link": "https://arxiv.org/abs/1906.00910",
    "venue": "arXiv",
    "date": "2019",
    "tags": [
      "Representation"
    ],
    "source": "pliang_list",
    "code": "https://github.com/Philip-Bachman/amdim-public",
    "authors": "Philip Bachman, R Devon Hjelm, William Buchwalter",
    "description": "本文提出了一种基于最大化多视图间互信息（Mutual Information）的自监督表征学习方法，通过从共享语境中提取不同位置（如不同相机视角）或不同模态（如触觉、听觉或视觉）的特征，或者通过对单张图像反复应用数据增强来构建多视图，进而捕捉跨视图的高层核心因子，如特定物体或事件的存在。该研究开发的模型在图像表征学习任务上表现卓越，在 ImageNet 标准线性评估中达到了 68.1% 的准确率，相比之前的研究提升了 12% 以上，并领先同期研究 7%。此外，当模型扩展到使用基于混合的表征时，图像分割能力会作为一种自然的副作用自发涌现。",
    "image": ""
  },
  {
    "title": "ViCo: Word Embeddings from Visual Co-occurrences",
    "link": "https://arxiv.org/abs/1908.08527",
    "venue": "ICCV",
    "date": "2019",
    "tags": [
      "Representation"
    ],
    "source": "pliang_list",
    "code": "https://github.com/BigRedT/vico",
    "authors": "Tanmay Gupta, Alexander Schwing, Derek Hoiem",
    "description": "本文提出了一种从视觉共现（Visual Co-occurrences）中学习词嵌入的方法，即如果两个词同时适用于同一张图像或图像区域，则认为它们在视觉上共现。研究者从 VisualGenome 和 ImageNet 等大规模带文本标注的视觉数据库中，提取了物体与属性词之间的四种视觉共现类型，并训练了一个多任务对数双线性模型，将每种共现类型所代表的单词“含义”压缩编码为单一的视觉词向量。通过无监督聚类、监督划分以及类零样本泛化分析，结果表明该词嵌入能够有效补充如 GloVe 等纯文本嵌入，更好地表征那些难以仅从文本语料库中获取的视觉概念间的相似性与差异。在包括四个视听语言任务在内的五项下游应用评估中，利用该视觉词嵌入增强 GloVe 均提升了任务表现。此外，研究还发现了一个违背传统认知的现象：在所有受监督的视听语言任务中，随机嵌入的表现与学习到的嵌入相当。",
    "image": ""
  },
  {
    "title": "Unified Visual-Semantic Embeddings: Bridging Vision and Language With Structured Meaning Representations",
    "link": "http://openaccess.thecvf.com/content_CVPR_2019/papers/Wu_Unified_Visual-Semantic_Embeddings_Bridging_Vision_and_Language_With_Structured_Meaning_CVPR_2019_paper.pdf",
    "venue": "CVPR",
    "date": "2019",
    "tags": [
      "Representation"
    ],
    "source": "pliang_list",
    "code": "https://github.com/vacancy/SceneGraphParser",
    "authors": "Hao Wu, Jiayuan Mao, Yufeng Zhang, et al.",
    "description": "本文提出了一种名为 Unified VSE（统一视觉语义嵌入）的模型，旨在建立一个能够统一表示物体、属性、关系和完整场景等不同层级概念的联合空间 。该模型将句子语义视为不同语义组件（如物体和关系）的组合，并将其与对应的图像局部区域进行对齐 。针对传统模型容易受到文本域对抗攻击（如将“墙”改为“架子”）的问题，研究者引入了“语义覆盖”（semantic coverage）机制，强制要求标题嵌入必须涵盖句子中出现的所有语义组件，从而增强了模型的鲁棒性 。此外，研究还展示了该学习空间能够利用视觉线索来辅助解析新句子的词依赖关系，进而促进语言习得 。实验结果表明，Unified VSE 在跨模态检索任务中优于现有的基线模型 。",
    "image": ""
  },
  {
    "title": "Multi-Task Learning of Hierarchical Vision-Language Representation",
    "link": "https://arxiv.org/abs/1812.00500",
    "venue": "CVPR",
    "date": "2019",
    "tags": [
      "Representation"
    ],
    "source": "pliang_list",
    "code": "",
    "authors": "Duy-Kien Nguyen, Takayuki Okatani",
    "description": "本文提出了一种多任务学习方法，旨在解决现有视觉语言（VL）系统因针对单一任务独立设计网络和数据集，而导致难以理解任务间关系及知识迁移受阻的挑战 。该方法通过整合多样化的数据集，学习一种被多个任务共享的通用视觉语言表征 。该表征具有层次化结构，针对每个具体任务的预测均基于该层次结构中对应级别的表征计算得出 。实验结果表明，该方法在图像标题检索、视觉问答（VQA）和视觉定位（Visual Grounding）等任务上的表现一致优于以往的单任务学习方法 。此外，研究者还通过对网络生成的注意力图进行可视化，深入分析了其习得的层次化表征特性 。",
    "image": ""
  },
  {
    "title": "Learning Factorized Multimodal Representations",
    "link": "https://arxiv.org/abs/1806.06176",
    "venue": "ICLR",
    "date": "2019",
    "tags": [
      "Representation"
    ],
    "source": "pliang_list",
    "code": "https://github.com/pliang279/factorized/",
    "authors": "Yao-Hung Hubert Tsai, Paul Pu Liang, Amir Zadeh, Louis-Philippe Morency, Ruslan Salakhutdinov",
    "description": "本文针对多模态表征学习中异构信息源带来的复杂挑战，提出了一种跨多模态数据与标签的联合生成-判别优化目标，以解决如何学习复杂的模态内及跨模态交互，以及如何应对测试时模态缺失或噪声的问题。该模型将表征分解为两组独立的因子：多模态判别因子和模态特定生成因子；前者在所有模态间共享，包含情感预测等判别任务所需的联合特征，后者则为各模态独有，包含生成原始数据所需的信息。实验结果表明，该模型在六个多模态数据集上均达到了最先进或具有竞争力的性能，并展现出灵活的条件生成能力，能够在不显著影响性能的前提下重构缺失模态。此外，通过对分解表征的解释，本研究进一步揭示了影响多模态学习的底层交互机制。",
    "image": ""
  },
  {
    "title": "A Probabilistic Framework for Multi-view Feature Learning with Many-to-many Associations via Neural Networks",
    "link": "https://arxiv.org/abs/1802.04630",
    "venue": "ICML",
    "date": "2018",
    "tags": [
      "Representation"
    ],
    "source": "pliang_list",
    "code": "",
    "authors": "Akifumi Okuno, Tetsuya Hada, Hidetoshi Shimodaira",
    "description": "本文提出了一种名为概率多视图图嵌入（PMvGE）的简单框架，用于处理具有多对多关联的多视图特征学习，从而泛化了多种现有的多视图方法。PMvGE 作为一个概率模型，通过对数据向量节点及其关联链路进行图嵌入来预测新的关联，利用神经网络将具有多对多关联的多视图数据向量转换为共享空间中的特征向量，并以特征向量的内积来建模两个数据向量之间产生新关联的概率。现有的多视图特征学习技术通常只能处理多对多关联或非线性变换中的一种，而 PMvGE 能够同时兼顾两者。通过结合默瑟定理（Mercer's theorem）和通用近似定理，本文证明了 PMvGE 能够学习跨视图的广泛相似性度量。此外，基于似然的估计器使得该模型可以通过小批量随机梯度下降（SGD）在大规模数据集上高效计算数据向量的非线性变换，数值实验也表明 PMvGE 的性能优于现有的多视图方法。",
    "image": ""
  },
  {
    "title": "Do Neural Network Cross-Modal Mappings Really Bridge Modalities?",
    "link": "https://aclweb.org/anthology/P18-2074",
    "venue": "ACL",
    "date": "2018",
    "tags": [
      "Representation"
    ],
    "source": "pliang_list",
    "code": "https://liir.cs.kuleuven.be/software.php",
    "authors": "Guillem Collell, Marie-Francine Moens",
    "description": "本文研究了前馈神经网络在跨模态任务（如图像与文本之间的映射）中是否真的起到了“桥接”模态的作用 。研究的核心发现是：映射后的预测向量在语义结构（即邻域结构）上，一致地更接近其输入模态，而非目标模态 。研究者通过三个跨模态基准测试，学习了大量的文本转视觉和视觉转文本的神经映射（多达五层），结果发现映射向量保留了输入向量的大部分语义信息 。此外，实验还显示，即使是未经过训练的网络也不会显著破坏输入向量的语义结构 。这一发现揭示了一个长期被忽略的现象：神经网络映射产生的向量往往更像输入而非目标。为此，作者建议使用基于邻域的语义标准（如本文提出的 mNNO 测度）来评估预测向量的质量，而非单纯依赖均方误差（MSE）等几何指标 。",
    "image": ""
  },
  {
    "title": "Learning Robust Visual-Semantic Embeddings",
    "link": "https://arxiv.org/abs/1703.05908",
    "venue": "ICCV",
    "date": "2017",
    "tags": [
      "Representation"
    ],
    "source": "pliang_list",
    "code": "",
    "authors": "Yao-Hung Hubert Tsai, Liang-Kang Huang, Ruslan Salakhutdinov",
    "description": "本文提出了一个端到端的学习框架，旨在利用深度神经网络在无监督学习方面的成功经验，提取跨领域更具鲁棒性的多模态表征。由于现有许多学习图文联合嵌入的方法仅依赖配对图像及其文本属性的监督信息，该方法创新性地将表征学习模型（即自动编码器）与跨域学习准则（即最大均值差异损失）相结合，以此学习语义和视觉特征的联合嵌入。此外，研究还引入了一种新型的无监督数据自适应推理技术，为有标签和无标签数据构建更全面的嵌入表示。通过在 Animals with Attributes 和 Caltech-UCSD Birds 200-2011 数据集上的评估，该方法在从归纳到转导设置的零样本及少样本图像识别与检索等广泛应用中，均证明了其性能优于当前最先进的技术。",
    "image": ""
  },
  {
    "title": "Deep Multimodal Representation Learning from Temporal Data",
    "link": "https://arxiv.org/abs/1704.03152",
    "venue": "CVPR",
    "date": "2017",
    "tags": [
      "Representation"
    ],
    "source": "pliang_list",
    "code": "",
    "authors": "Xitong Yang, Palghat Ramesh, Radha Chitta, et al.",
    "description": "本文提出了一种名为关联循环神经网络（CorrRNN）的新型时间融合模型，旨在解决深度学习在处理视频、音频及传感器信号等具有固有时间结构的多模态数据融合时的难题。该模型具备三大核心特性：首先，它能够同时学习模态间的联合表征及其时间依赖性；其次，其目标函数包含多个损失项，特别通过最大相关性损失项来强化跨模态信息的提取；最后，模型引入了注意力机制，以动态调节不同输入模态对联合表征的贡献度。通过在基于视频和传感器的动作分类以及视听语音识别两项任务上的实验验证，本文深入分析了 CorrRNN 各组件的作用，并证明了该模型在多个数据集上均表现出强大的鲁棒性、有效性以及达到最先进水平的性能。",
    "image": ""
  },
  {
    "title": "Is an Image Worth More than a Thousand Words? On the Fine-Grain Semantic Differences between Visual and Linguistic Representations",
    "link": "https://www.aclweb.org/anthology/C16-1264",
    "venue": "COLING",
    "date": "2016",
    "tags": [
      "Representation"
    ],
    "source": "pliang_list",
    "code": "https://liir.cs.kuleuven.be/software.php",
    "authors": "Guillem Collell, Marie-Francine Moens",
    "description": "本文对比研究了视觉表征（基于卷积神经网络 CNN）和语言表征（基于分布语义模型）在捕捉概念属性（如颜色、形状、功能等）方面的能力。研究发现，虽然语言和视觉在大多数属性类别上表现相似，但它们具有明显的互补性：视觉表征在捕捉“形状”和“颜色”等视觉属性方面表现更优，而语言表征在捕捉“功能”和“百科全书式”属性（如是否属于某个特定类别）方面更具优势。通过分析这些表征在多大程度上能够预测人类定义的属性，研究证明了将两种模态结合可以产生更完整的概念表征。此外，文章还揭示了即使在视觉属性上，由于语言模型捕捉到了物体间的类别相似性，其表现有时也出人意料地好。",
    "image": ""
  },
  {
    "title": "Combining Language and Vision with a Multimodal Skip-gram Model",
    "link": "https://www.aclweb.org/anthology/N15-1016",
    "venue": "NAACL",
    "date": "2015",
    "tags": [
      "Representation"
    ],
    "source": "pliang_list",
    "code": "",
    "authors": "Angeliki Lazaridou, Nghia The Pham, Marco Baroni",
    "description": "本文通过将视觉信息引入 Mikolov 等人提出的 Skip-gram 模型，开发了多模态 Skip-gram 模型（MMSKIP-GRAM）。该模型不仅通过文本语料库预测上下文来学习词向量，还针对一部分特定的词汇，将其对应的自然图像特征（视觉表示）整合进学习过程，要求模型共同预测语言和视觉特征。研究结果显示，该模型在多种语义基准测试中表现优异。由于模型能将视觉信息传播到所有词汇（包括未在图像中出现的词），它显著提升了零样本（zero-shot）设置下的图像标注和检索性能。此外，该模型还发现了一些抽象词汇有趣的视觉属性，为具身认知理论的实现提供了支持。",
    "image": ""
  },
  {
    "title": "Deep Fragment Embeddings for Bidirectional Image Sentence Mapping",
    "link": "https://arxiv.org/abs/1406.5679",
    "venue": "NeurIPS",
    "date": "2014",
    "tags": [
      "Representation"
    ],
    "source": "pliang_list",
    "code": "",
    "authors": "Andrej Karpathy, Armand Joulin, Li Fei-Fei",
    "description": "本文提出了一种通过视觉和自然语言数据的多模态嵌入实现图像与句子双向检索的模型。与以往直接将整张图像或整个句子映射到公共嵌入空间的模型不同，该模型在更细粒度的层面上运行，将图像片段（物体）和句子片段（具有类型的依存树关系）嵌入到公共空间中。除了此前研究中常见的排序目标外，这种方法还允许引入一种全新的片段对齐目标，从而学习跨模态直接关联这些细粒度片段。广泛的实验评估表明，同时在图像与句子的全局层面以及各自片段的细粒度层面进行推理，能够显著提升图像-句子检索任务的性能。此外，由于推断出的跨模态片段对齐是显性的，该模型还提供了具有可解释性的预测结果。",
    "image": ""
  },
  {
    "title": "Multimodal Learning with Deep Boltzmann Machines",
    "link": "https://dl.acm.org/citation.cfm?id=2697059",
    "venue": "JMLR",
    "date": "2014",
    "tags": [
      "Representation"
    ],
    "source": "pliang_list",
    "code": "",
    "authors": "Nitish Srivastava, Ruslan Salakhutdinov",
    "description": "本文提出了一种基于深度玻尔兹曼机（Deep Boltzmann Machine）的多模态数据生成模型，旨在处理具有不同统计特性的多种模态数据，如带有文本标签的图像或伴有音频的视频。该模型通过跨模态组合特征来构建融合表征，实验证明这些表征在分类和信息检索任务中具有极高价值。模型的一个核心优势在于其生成能力：通过对各数据模态的条件分布进行采样，即使在部分数据模态缺失的情况下，依然能够生成完整的融合表征。研究者在双模态图像-文本及音频-视频数据上进行了实验，结果显示该模型在 MIR-Flickr 数据集上的分类表现优异，达到或超越了其他深度学习模型以及基于多核学习（Multiple Kernel Learning）的 SVM 模型。此外，该模型还展示了强大的泛化性能，即在测试阶段仅提供单模态数据时，依然能有效辅助分类与检索任务。",
    "image": ""
  },
  {
    "title": "Learning Grounded Meaning Representations with Autoencoders",
    "link": "https://www.aclweb.org/anthology/P14-1068",
    "venue": "ACL",
    "date": "2014",
    "tags": [
      "Representation"
    ],
    "source": "pliang_list",
    "code": "",
    "authors": "Carina Silberer, Mirella Lapata",
    "description": "本文提出了一种利用深度学习技术将词汇的分布表征与视觉信息相结合的新模型。该模型使用堆叠自动编码器（Stacked Autoencoders），分别从文本和图像中自动提取属性向量（Attributes），并将其作为输入来学习更高层级的多模态嵌入。研究者通过模拟人类的相似性判断和概念分类任务对模型进行了评估。实验结果表明，该模型在捕捉语义关联和类别划分方面均优于传统的基线模型及相关的多模态模型，证明了通过自动编码器进行多模态融合能有效提升词汇语义表示的质量。",
    "image": ""
  },
  {
    "title": "DeViSE: A Deep Visual-Semantic Embedding Model",
    "link": "https://papers.nips.cc/paper/5204-devise-a-deep-visual-semantic-embedding-model",
    "venue": "NeurIPS",
    "date": "2013",
    "tags": [
      "Representation"
    ],
    "source": "pliang_list",
    "code": "",
    "authors": "Andrea Frome, Greg S Corrado, Jon Shlens, et al.",
    "description": "本文提出了一种新型的深度视觉语义嵌入模型（DeViSE），旨在解决现代视觉识别系统在扩展到海量物体类别时面临的标注图像数据获取困难的问题。该模型通过联合利用带标签的图像数据以及从大规模无标注文本中提取的语义信息，来训练视觉模型并对其预测进行约束。实验证明，该模型在包含1000个类别的 ImageNet 物体识别挑战赛中达到了最先进的性能水平，且其产生的错误在语义上更为合理；更重要的是，研究表明该模型利用习得的语义知识，能够对训练过程中从未观察到的数万个图像标签进行预测。这种语义知识将此类零样本（zero-shot）预测的准确率提升了高达65%，在数千个视觉模型从未见过的全新标签上实现了高达10%的命中率。",
    "image": ""
  },
  {
    "title": "Multimodal Deep Learning",
    "link": "https://dl.acm.org/citation.cfm?id=3104569",
    "venue": "ICML",
    "date": "2011",
    "tags": [
      "Representation"
    ],
    "source": "pliang_list",
    "code": "",
    "authors": "Jiquan Ngiam, Aditya Khosla, Mingyu Kim, et al.",
    "description": "本文提出了一种将深度网络应用于多模态无监督特征学习的新方法，旨在解决如何在多种信息源（如文本、图像或音频）共存时学习更优特征。研究通过一系列多模态学习任务展示了深度网络的效能，特别证明了“跨模态特征学习”的优势，即在特征学习阶段引入多种模态（如同时使用音频和视频）可以为单一模态（如仅视频）提取到更高质量的特征。此外，该模型成功学习到了模态间的共享表征，并在一项极具挑战性的任务中通过了验证：分类器仅使用音频数据进行训练，却能在仅有视频数据的测试集上取得良好表现，反之亦然。通过在 CUAVE 和 AVLetters 数据集上的视听语音分类实验，该模型在 AVLetters 上刷新了视觉语音分类的最佳纪录，充分验证了其学习有效共享表征的能力。",
    "image": ""
  },
  {
    "title": "Robust Contrastive Learning against Noisy Views",
    "link": "https://arxiv.org/abs/2201.04309",
    "venue": "arXiv",
    "date": "2022",
    "tags": [
      "Fusion"
    ],
    "source": "pliang_list",
    "code": "https://github.com/chingyaoc/RINCE",
    "authors": "Ching-Yao Chuang, R Devon Hjelm, Xin Wang, et al.",
    "description": "本文针对对比学习中普遍存在的一个核心假设——即正对包含相关的视图（如图像块或视频中的多模态信号）且共享特定底层信息——在实际场景下可能被违背的问题进行了研究，并指出当该假设因噪声视图（如没有明显共享信息的伪正对）而失效时，对比学习会产生次优的表征。为此，研究者提出了一种对噪声视图具有鲁棒性的新型对比损失函数，并通过理论证明其与噪声二元分类中的鲁棒对称损失之间存在关联，同时基于 Wasserstein 距离衡量标准为互信息最大化建立了新的对比界限。该损失函数完全不依赖于特定模态，可以作为 InfoNCE 损失的简单替代方案轻松应用于现有的对比学习框架中。实验结果表明，该方法在包含多种现实噪声模式的图像、视频和图（Graph）对比学习基准测试中，均一致地优于现有最先进的技术。",
    "image": ""
  },
  {
    "title": "Cooperative Learning for Multi-view Analysis",
    "link": "https://arxiv.org/abs/2112.12337",
    "venue": "arXiv",
    "date": "2022",
    "tags": [
      "Fusion"
    ],
    "source": "pliang_list",
    "code": "",
    "authors": "Daisy Yi Ding, Shuangning Li, Balasubramanian Narasimhan, Robert Tibshirani",
    "description": "本文提出了一种针对多组特征（即“多视图”）进行监督学习的新方法，该方法在生物学和医学领域具有重要意义，因为这些领域常涉及对同一组样本进行基因组学、蛋白质组学和影像组学等多维组学数据测量。这种被称为“协作学习”（Cooperative Learning）的方法将传统的预测平方误差损失与一种“一致性”惩罚项相结合，旨在鼓励来自不同数据视图的预测结果趋于一致。通过调节一致性惩罚的权重，该方法能够产生一系列连续的解决方案，涵盖了广为人知的早期融合（Early Fusion）和晚期融合（Late Fusion）方法。协作学习通过验证集或交叉验证自适应地确定融合程度，以优化测试集的预测误差。该方法的一个版本具有模块化特性，允许针对不同的数据视图选择最合适的拟合机制，如 Lasso、随机森林、Boosting 或神经网络等。在协作正则化线性回归的框架下，该方法将 Lasso 惩罚与一致性惩罚相结合，从而实现特征的稀疏性。当不同视图的信号中存在可被挖掘的底层关联时，该方法在增强信号强度方面表现尤为出色。实验结果证明，协作学习在模拟数据以及分娩发动预测的真实多组学案例中均取得了更高的预测准确度，为多组学数据融合提供了一种强大且灵活的手段。",
    "image": ""
  },
  {
    "title": "What Makes Multi-modal Learning Better than Single (Provably)",
    "link": "https://arxiv.org/abs/2106.04538",
    "venue": "NeurIPS",
    "date": "2021",
    "tags": [
      "Fusion"
    ],
    "source": "pliang_list",
    "code": "",
    "authors": "Yu Huang, Chenzhuang Du, Zihui Xue, et al.",
    "description": "本文针对深度多模态学习领域中理论支撑匮乏的现状，探讨了多模态学习是否在证明上确实优于单模态学习这一核心问题。研究者在一个最常用的多模态融合框架下对此进行了深入分析，该框架首先将不同模态的特征编码到公共潜空间中，再将潜表征映射到任务空间。通过严谨的数学证明，本文证实了使用多模态学习比仅使用其子集模态具有更小的总体风险（Population Risk），其核心直觉在于多模态方法能对潜空间表征进行更准确的估计。据研究所知，这是首个从泛化视角捕捉到真实多模态应用中重要定性现象的理论研究。结合实验结果，本文证明了多模态学习确实具备极具吸引力的正式性能保证。",
    "image": ""
  },
  {
    "title": "Efficient Multi-Modal Fusion with Diversity Analysis",
    "link": "https://dl.acm.org/doi/abs/10.1145/3474085.3475188",
    "venue": "ACMMM",
    "date": "2021",
    "tags": [
      "Fusion"
    ],
    "source": "pliang_list",
    "code": "",
    "authors": "Shuhui Qu, Yan Kang, Janghwan Lee",
    "description": "本文针对多模态机器学习在处理复杂现实问题中的应用进行了深入探讨，指出虽然经验表明具有高多样性的多分支融合模型通常表现更佳，但这种经验缺乏充分的理论支持，也无法保证模型达到最优性能。研究者通过分析几种主流的融合方法，提出了一种衡量融合模型性能的理论估计方法，该方法通过测量各分支模型的个体性能以及分支之间的“距离”来计算整体效果，并经过数值实验验证了该理论的有效性。基于此理论，本文进一步提出了一个分支模型选择框架，旨在识别出能够使融合模型达到最优多模态性能的候选分支组合。通过在多种数据集上的实验，该框架证明了能够通过有效筛选分支模型组合，显著提升多模态任务的最终表现。",
    "image": ""
  },
  {
    "title": "Attention Bottlenecks for Multimodal Fusion",
    "link": "https://arxiv.org/abs/2107.00135",
    "venue": "NeurIPS",
    "date": "2021",
    "tags": [
      "Fusion"
    ],
    "source": "pliang_list",
    "code": "",
    "authors": "Arsha Nagrani, Shan Yang, Anurag Arnab, et al.",
    "description": "本文提出了一种新型的基于 Transformer 的架构，旨在解决人类能够同时处理和融合多模态输入，而机器视觉模型往往仍依赖于在最后阶段融合各模态表示或预测（即“后期融合”）的现状。该模型引入了“融合瓶颈”（Fusion Bottlenecks）机制，在多个层级实现模态融合，与传统的成对自注意力机制不同，该模型强制不同模态之间的信息必须通过少量瓶颈潜变量进行传递。这种设计要求模型整理并压缩各模态中最相关的信号，仅共享必要的信息，从而在提升融合性能的同时显著降低计算成本。通过详尽的消融实验，该方法在 Audioset、Epic-Kitchens 和 VGGSound 等多个视听分类基准测试上均取得了最先进的结果。",
    "image": ""
  },
  {
    "title": "VMLoc: Variational Fusion For Learning-Based Multimodal Camera Localization",
    "link": "https://arxiv.org/abs/2003.07289",
    "venue": "AAAI",
    "date": "2021",
    "tags": [
      "Fusion"
    ],
    "source": "pliang_list",
    "code": "https://github.com/kaichen-z/VMLoc",
    "authors": "Kaichen Zhou, Changhao Chen, Bing Wang, et al.",
    "description": "本文针对单次相机定位领域中如何有效融合多模态数据（如图像与深度）以及如何处理输入退化或缺失的问题进行了深入研究，并指出先前深度融合方法由于仅采取简单的加法或拼接，未能充分利用各模态的优势，导致其性能并未显著优于单模态模型。为此，研究者提出了一个名为 VMLoc 的端到端框架，该框架通过变分专家乘积（variational Product-of-Experts, PoE）机制将不同的传感器输入融合至一个公共潜空间，并辅以基于注意力的融合手段。不同于以往直接沿用普通变分自编码器目标函数的多模态研究，本文展示了如何通过基于重要性采样的无偏目标函数来精确估算相机位姿。在 RGB-D 数据集上的广泛评估结果证明了该模型在处理多模态融合及鲁棒定位方面的有效性。",
    "image": ""
  },
  {
    "title": "Trusted Multi-View Classification",
    "link": "https://openreview.net/forum?id=OOsR8BzCnl5",
    "venue": "ICLR",
    "date": "2021",
    "tags": [
      "Fusion"
    ],
    "source": "pliang_list",
    "code": "https://github.com/hanmenghan/TMC",
    "authors": "Zongbo Han, Changqing Zhang, Huazhu Fu, Joey Tianyi Zhou",
    "description": "本文提出了一种名为“可信多视图分类”（Trusted Multi-view Classification）的新型多视图学习范式，旨在解决多视图分类（MVC）中如何动态评估不同样本的视图质量并提供可靠不确定性估计的关键问题。与传统仅关注分类准确度的多视图集成方法不同，该算法通过在证据层面动态整合不同视图，能够同时提升分类的可靠性（测试时的不确定性估计）和稳健性（训练时的分布外检测意识）。该框架利用狄利克雷分布（Dirichlet distribution）来建模类别概率分布，并以各视图提供的证据作为参数，结合德姆斯特-沙费尔理论（Dempster-Shafer theory）进行融合。这种统一的学习框架不仅能生成精确的不确定性度量，还赋予了模型对分布外（OOD）样本的识别能力，使其预测结果更加可信。广泛的实验结果验证了该模型在准确性、可靠性和稳健性方面的有效性。",
    "image": ""
  },
  {
    "title": "Deep-HOSeq: Deep Higher-Order Sequence Fusion for Multimodal Sentiment Analysis",
    "link": "https://arxiv.org/pdf/2010.08218.pdf",
    "venue": "ICDM",
    "date": "2020",
    "tags": [
      "Fusion"
    ],
    "source": "pliang_list",
    "code": "https://github.com/sverma88/Deep-HOSeq--ICDM-2020",
    "authors": "Sunny Verma, Jiwei Wang, Zhefeng Ge, et al.",
    "description": "本文针对多模态情感分析中过度依赖注意力机制所导致的注意力掩码误导及训练动力学不稳定等问题，提出了一种名为 Deep-HOSeq 的新型融合方案。该方案首先利用基础 LSTM 和基于张量的卷积网络构建通用网络，用于同时挖掘模态内和模态间的动态特征，随后设计了独特的网络结构来封装模态间的时间粒度信息，这对于从异步序列中提取信息至关重要。Deep-HOSeq 通过融合层整合这两类关键信息，避开了复杂且难以优化的自定义注意力架构，在 CMU-MOSEI 和 CMU-MOSI 基准数据集上的实验结果表明，该模型能够高效发现多模态序列中的重要信息，验证了结合通用与独特序列信息的有效性。本文针对多模态情感分析中过度依赖注意力机制所导致的注意力掩码误导及训练动力学不稳定等问题，提出了一种名为 Deep-HOSeq 的新型融合方案。该方案首先利用基础 LSTM 和基于张量的卷积网络构建通用网络，用于同时挖掘模态内和模态间的动态特征，随后设计了独特的网络结构来封装模态间的时间粒度信息，这对于从异步序列中提取信息至关重要。Deep-HOSeq 通过融合层整合这两类关键信息，避开了复杂且难以优化的自定义注意力架构，在 CMU-MOSEI 和 CMU-MOSI 基准数据集上的实验结果表明，该模型能够高效发现多模态序列中的重要信息，验证了结合通用与独特序列信息的有效性。",
    "image": ""
  },
  {
    "title": "Removing Bias in Multi-modal Classifiers: Regularization by Maximizing Functional Entropies",
    "link": "https://arxiv.org/abs/2010.10802",
    "venue": "NeurIPS",
    "date": "2020",
    "tags": [
      "Fusion"
    ],
    "source": "pliang_list",
    "code": "https://github.com/itaigat/removing-bias-in-multi-modal-classifiers",
    "authors": "Itai Gat, Idan Schwartz, Alexander Schwing, Tamir Hazan",
    "description": "本文针对多模态数据集（如视觉问答 VQA 中的图像、问题和答案）在训练深度学习分类器时，各模态被利用程度不均、导致模型产生过度依赖某些模态的偏差（Bias）这一问题，提出了一种基于函数熵（Functional Entropy）的新型正则化项。该正则化项旨在直观地平衡各模态对分类结果的贡献，为解决函数熵正则化难以直接计算的挑战，研究者利用对数索博列夫不等式（log-Sobolev inequality）开发了一种新方法，通过函数菲舍尔信息（Functional-Fisher-information）为函数熵设定下界，从而最大化各模态贡献的信息量。在 VQA-CPv2 和 SocialIQ 两个具有挑战性的多模态数据集上的实验结果表明，该方法在实现更均匀利用各模态信息的同时达到了最先进的性能，并在 Colored MNIST 数据集上进一步验证了其有效性。",
    "image": ""
  },
  {
    "title": "Deep Multimodal Fusion by Channel Exchanging",
    "link": "https://arxiv.org/abs/2011.05005?context=cs.LG",
    "venue": "NeurIPS",
    "date": "2020",
    "tags": [
      "Fusion"
    ],
    "source": "pliang_list",
    "code": "https://github.com/yikaiw/CEN",
    "authors": "Yikai Wang, Wenbing Huang, Fuchun Sun, et al.",
    "description": "本文提出了一种名为通道交换网络（Channel-Exchanging-Network, CEN）的无参数多模态融合框架，旨在解决现有基于聚合或对齐的融合方法在平衡模态间融合与模态内处理时存在的不足，从而突破性能提升的瓶颈。该框架的核心思想是在不同模态的子网络之间动态交换通道，且这一交换过程完全由通道重要性自引导，其重要性通过训练期间批归一化（Batch-Normalization, BN）缩放因子的量级来衡量。为了保证通道交换的有效性，CEN 采用共享卷积滤波器但保留独立 BN 层的方式，这种设计不仅强化了特征融合，还使得多模态架构在参数量上几乎与单模态网络一样精简。通过在基于 RGB-D 数据的语义分割和多域输入的图像翻译任务上的广泛实验，证明了 CEN 相比于当前先进方法的优越性，而详细的消融实验则进一步证实了其各组件的有效性。",
    "image": ""
  },
  {
    "title": "What Makes Training Multi-Modal Classification Networks Hard?",
    "link": "https://arxiv.org/abs/1905.12681",
    "venue": "CVPR",
    "date": "2020",
    "tags": [
      "Fusion"
    ],
    "source": "pliang_list",
    "code": "",
    "authors": "Weiyao Wang, Du Tran, Matt Feiszli",
    "description": "本文针对在多模态任务中，端到端训练的多模态网络性能反而往往劣于性能最佳的单模态网络这一反直觉现象进行了研究，并指出导致该性能下降的两个主要原因：一是多模态网络由于容量增加更容易产生过拟合，二是不同模态在训练过程中的过拟合与泛化速率各异，采用单一优化策略进行联合训练会导致次优结果。为此，研究者提出了一种名为“梯度混合”（Gradient Blending）的技术，该技术能根据各模态的过拟合表现计算出最优的混合比例。实验证明，梯度混合在克服过拟合方面优于广泛使用的基准方法，并在人体动作识别、自我中心动作识别以及声学事件检测等多种任务中达到了最先进的准确率。",
    "image": ""
  },
  {
    "title": "Dynamic Fusion for Multimodal Data",
    "link": "https://arxiv.org/abs/1911.03821",
    "venue": "arXiv",
    "date": "2019",
    "tags": [
      "Fusion"
    ],
    "source": "pliang_list",
    "code": "https://github.com/demfier/philo/",
    "authors": "Gaurav Sahu, Olga Vechtomova",
    "description": "本文针对视频、语音和文本等异构多模态数据有效融合的难题，提出了一系列旨在高效建模跨模态上下文的自适应融合技术。该研究摒弃了传统的如特征拼接等确定性融合操作，转而让网络自主学习如何更有效地组合给定的多模态特征。为此，文中设计了两种网络结构：其一是 Auto-Fusion，它能在保留上下文信息的同时，学习压缩来自不同模态的数据；其二是 GAN-Fusion，它利用互补模态提供的上下文信息对习得的潜空间进行正则化约束。在多模态机器翻译和情感识别任务上的定量评估结果表明，相较于许多采用大规模 Transformer 架构的现有方法，这种轻量级的自适应网络能够更好地建模跨模态上下文。",
    "image": ""
  },
  {
    "title": "DeepCU: Integrating Both Common and Unique Latent Information for Multimodal Sentiment Analysis",
    "link": "https://www.ijcai.org/proceedings/2019/503",
    "venue": "IJCAI",
    "date": "2019",
    "tags": [
      "Fusion"
    ],
    "source": "pliang_list",
    "code": "https://github.com/sverma88/DeepCU-IJCAI19",
    "authors": "Sunny Verma, Chen Wang, Liming Zhu, Wei Liu",
    "description": "本文针对多模态情感分析中如何有效整合视觉、文本和声学信息的问题，指出当前融合方案往往仅侧重于利用神经网络获取模态间的公共信息，或是通过低秩表征建模模态特有的唯一信息，而忽略了这两类信息对于刻画跨模态和模态内关系同样至关重要。为此，本研究提出了一个名为 DeepCU（具有公共与唯一潜信息的深度网络）的新型多模态数据融合架构。该架构包含一个用于提取多模态表征中公共信息的深度网络，以及多个用于获取模态特定唯一信息的独立网络，以增强系统的泛化性能。DeepCU 通过融合层将这两类信息整合，实现了对所有关键潜在信息的共同利用与挖掘。在多个真实世界数据集上的综合实验表明，DeepCU 能够同时发现并利用公共与唯一信息，证明了该架构在情感预测任务中的有效性。",
    "image": ""
  },
  {
    "title": "Deep Multimodal Multilinear Fusion with High-order Polynomial Pooling",
    "link": "https://papers.nips.cc/paper/9381-deep-multimodal-multilinear-fusion-with-high-order-polynomial-pooling",
    "venue": "NeurIPS",
    "date": "2019",
    "tags": [
      "Fusion"
    ],
    "source": "pliang_list",
    "code": "",
    "authors": "Ming Hou, Jiajia Tang, Jianhai Zhang, Wanzeng Kong, Qibin Zhao",
    "description": "本文针对现有基于张量的多模态融合技术仅限于双线性或三线性池化、难以充分释放多线性融合表达能力且忽略了复杂局部相关性这一局限性，提出了一种多模态特征融合的新方案。研究者首先设计了多项式张量池化（PTP）模块，通过考虑高阶矩来整合多模态特征，并辅以张量化全连接层；随后以 PTP 为基础构建了层次化多项式融合网络（HPFN），通过递归方式将局部相关性转化为全局相关性。由于 HPFN 等效于极深的卷积算术电路，其表达能力随层数增加呈指数级增长。多项实验结果表明，该方法能够有效建模跨模态的复杂交互，并达到了最先进的性能水平。",
    "image": ""
  },
  {
    "title": "XFlow: Cross-modal Deep Neural Networks for Audiovisual Classification",
    "link": "https://ieeexplore.ieee.org/abstract/document/8894404",
    "venue": "IEEE TNNLS",
    "date": "2019",
    "tags": [
      "Fusion"
    ],
    "source": "pliang_list",
    "code": "https://github.com/catalina17/XFlow",
    "authors": "Cătălin-Daniel Vidana, Andrei Eliade, Bogdan Alexe, Petru Celebi",
    "description": "本文提出了一种名为 XFlow 的新型架构，旨在解决视听分类任务中多模态特征的深度集成问题。现有的多模态学习方法通常采用后期融合（Late Fusion）或简单的连接方式，而 XFlow 引入了跨模态连接（Cross-Modal Connections），允许信息在训练过程中从一个模态流向另一个模态。该架构特别设计了两种流向：一种是从音频到视觉，另一种是从视觉到音频，通过这种双向的特征交换，模型能够学习到模态间更深层的关联。此外，XFlow 采用了一种新颖的代价函数，结合了分类损失和特征空间中的跨模态对齐损失，从而增强了特征的判别力。实验在多个标准视听数据集（如 DCASE 和 UCF101）上进行，结果证明 XFlow 在处理含有噪声或模态缺失的情况时，比传统的融合方法具有更强的鲁棒性和更高的分类准确率。",
    "image": ""
  },
  {
    "title": "MFAS: Multimodal Fusion Architecture Search",
    "link": "https://arxiv.org/abs/1903.06496",
    "venue": "CVPR",
    "date": "2019",
    "tags": [
      "Fusion"
    ],
    "source": "pliang_list",
    "code": "",
    "authors": "Juan-Manuel Pérez-Rúa, Valentin Vielzeuf, Stéphane Pateux, Moez Baccouche, Frédéric Jurie",
    "description": "本文针对多模态分类问题中寻找优秀架构的难题，提出了一个新颖且通用的搜索空间，该空间涵盖了大量可能的融合架构。为了在该搜索空间内为特定数据集找到最优架构，研究者利用了一种专门为此问题定制的高效序列模型探索方法。通过在玩具数据集以及两个真实多模态数据集上的广泛实验，本文证明了将多模态融合建模为神经架构搜索（NAS）问题的价值。该方法发现的融合架构在不同领域和不同规模的数据集上均表现出了最先进的性能，其中包括目前最大的多模态动作识别数据集 NTU RGB+D。",
    "image": ""
  },
  {
    "title": "The Neuro-Symbolic Concept Learner: Interpreting Scenes, Words, and Sentences From Natural Supervision",
    "link": "https://arxiv.org/abs/1904.12584",
    "venue": "ICLR",
    "date": "2019",
    "tags": [
      "Fusion"
    ],
    "source": "pliang_list",
    "code": "http://nscl.csail.mit.edu/",
    "authors": "Jiayuan Mao, Chuang Gan, Pushmeet Kohli, Joshua B. Tenenbaum, Jiajun Wu",
    "description": "本文提出了神经符号概念学习器（NS-CL），这是一种无需显式监督即可同时学习视觉概念、单词及句子语义解析的模型；该模型仅通过观察图像并阅读对应的问答对进行学习。NS-CL 构建了基于对象的场景表示，并将句子转化为可执行的符号程序，通过神经符号推理模块在潜场景表示上执行这些程序，从而将两大模块的训练连接起来。类比人类的概念学习过程，其感知模块根据语言描述学习视觉概念，而习得的视觉概念反过来又促进了新词的学习和新句子的解析。研究者利用课程学习（Curriculum Learning）引导模型在庞大的图像与语言组合空间中进行搜索。实验证明，该模型在学习视觉概念、词表征和语义解析方面具有极高的准确性和效率，且能轻松泛化至新的物体属性、组合、语言概念及场景，甚至可应用于新的程序域，并在视觉问答和双向图文检索等应用中展现出强大能力。",
    "image": ""
  },
  {
    "title": "Unifying and merging well-trained deep neural networks for inference stage",
    "link": "https://www.ijcai.org/Proceedings/2018/0283.pdf",
    "venue": "IJCAI",
    "date": "2018",
    "tags": [
      "Fusion"
    ],
    "source": "pliang_list",
    "code": "https://github.com/ivclab/NeuralMerger",
    "authors": "Yi-Min Chou, Yi-Ming Chan, Jia-Hong Lee, Chih-Yi Chiu, Chu-Song Chen",
    "description": "本文提出了一种名为 NeuralMerger 的创新方法，旨在推理阶段将多个针对不同任务且架构可能各异的预训练深度神经网络合并为一个统一且紧凑的模型。该方法首先通过对齐原始网络的层级结构，并利用卷积分解和向量量化技术提取权重的代表性编码，实现不同网络间权重的共享与共用，从而有效消除模型间的冗余。随后，合并后的模型会通过端到端的微调过程来优化性能，确保在处理原始任务时能够保持高精度。实验结果证明，该方法能产生极具压缩性的模型，在 Sound20、Fashion-MNIST 以及服装与性别分类等任务中，不仅能实现 10 到 20 倍以上的模型体积压缩和显著的推理速度提升，且准确率下降极小，非常适合在资源受限的设备或边缘端同时运行多个 AI 任务。",
    "image": ""
  },
  {
    "title": "Efficient Low-rank Multimodal Fusion with Modality-Specific Factors",
    "link": "https://arxiv.org/abs/1806.00064",
    "venue": "ACL",
    "date": "2018",
    "tags": [
      "Fusion"
    ],
    "source": "pliang_list",
    "code": "https://github.com/Justin1904/Low-rank-Multimodal-Fusion",
    "authors": "Zhun Liu, Ying Shen, Varun Bharadhwaj Lakshminarasimhan, et al.",
    "description": "本文针对人工智能新兴领域多模态研究中的核心问题——多模态融合展开讨论，旨在将多个单模态表征整合为紧凑的统一表征。针对以往利用张量进行多模态表征的方法往往面临维度爆炸及计算复杂度随张量变换呈指数级增长的局限性，本文提出了低秩多模态融合（Low-rank Multimodal Fusion）方法，通过引入低秩张量来显著提升融合效率。研究者在多模态情感分析、说话人特质分析和情绪识别这三项任务上对模型进行了评估，结果显示该模型在保持极具竞争力的性能指标的同时，大幅降低了计算开销。此外，补充实验进一步证实，该模型在多种低秩设定下均表现稳健，且在训练和推理效率上均远优于其他采用张量表征的融合方法。",
    "image": ""
  },
  {
    "title": "Memory Fusion Network for Multi-view Sequential Learning",
    "link": "https://arxiv.org/abs/1802.00927",
    "venue": "AAAI",
    "date": "2018",
    "tags": [
      "Fusion"
    ],
    "source": "pliang_list",
    "code": "https://github.com/pliang279/MFN",
    "authors": "Amir Zadeh, Paul Pu Liang, Navonil Mazumder, et al.",
    "description": "本文针对多视图序列学习这一机器学习基础问题，提出了一种名为记忆融合网络（Memory Fusion Network, MFN）的新型神经架构，旨在同时显式建模并随时间持续追踪多视图序列中的两种关键交互：视图特定交互（view-specific interactions）和跨视图交互（cross-view interactions）。MFN 的首个核心组件是多 LSTM 系统（System of LSTMs），通过为每个视图分配独立的 LSTM 函数来隔绝并学习各视图内部的特定动态；随后，模型利用一种名为增量记忆注意力网络（Delta-memory Attention Network, DMAN）的特殊注意力机制来识别视图间的跨视图交互，并采用多视图门控记忆（Multi-view Gated Memory）对这些交互信息进行跨时间的汇总。通过在多个公开基准数据集上的广泛实验，MFN 在与各种现有多视图序列学习方法的对比中均表现更优，不仅超越了所有现有的多视图方案，还刷新了这些多视图数据集的最先进性能纪录。",
    "image": ""
  },
  {
    "title": "Tensor Fusion Network for Multimodal Sentiment Analysis",
    "link": "https://arxiv.org/abs/1707.07250",
    "venue": "EMNLP",
    "date": "2017",
    "tags": [
      "Fusion"
    ],
    "source": "pliang_list",
    "code": "https://github.com/A2Zadeh/TensorFusionNetwork",
    "authors": "Amir Zadeh, Minghai Chen, Soujanya Poria, Erik Cambria, Louis-Philippe Morency",
    "description": "多模态情感分析是一个日益流行的研究领域，它将传统的基于语言的情感分析定义扩展到语言伴随其他相关模态的多模态框架中。在本文中，我们将多模态情感分析问题定义为对模态内（intra-modality）和模态间（inter-modality）动态的建模，并引入了一种名为张量融合网络（Tensor Fusion Network, TFN）的新型模型，该模型能够以端到端的方式同时学习这两类动态特征。所提出的方法专门针对在线视频中口语的波动性以及伴随的姿态和声音进行了优化。实验结果表明，该模型在多模态和单模态情感分析任务中均优于目前最先进的方法。",
    "image": ""
  },
  {
    "title": "Jointly Modeling Deep Video and Compositional Text to Bridge Vision and Language in a Unified Framework",
    "link": "http://web.eecs.umich.edu/~jjcorso/pubs/xu_corso_AAAI2015_v2t.pdf",
    "venue": "AAAI",
    "date": "2015",
    "tags": [
      "Fusion"
    ],
    "source": "pliang_list",
    "code": "",
    "authors": "Ran Xu, Caiming Xiong, Wei Chen, Jason J. Corso",
    "description": "本文提出了一个统一的联合建模框架，旨在通过深度视频模型和组合语义语言模型的结合，架起视频信号与自然语言描述之间的桥梁 。该框架由三个核心部分组成：首先是一个基于依赖树结构的组合语义语言模型，它将句子解析为 <主语, 谓语, 宾语>（SVO）三元组，并利用递归神经网络将这些词汇映射到连续的向量空间中，以保留视觉相关的语义和词序 ；其次是一个利用深度神经网络从视频帧序列中捕捉核心语义，并结合时间金字塔方案提取动作信息的视觉模型 ；最后是一个联合嵌入模型，通过最小化上述两个模型输出在联合空间中的距离，实现双向的端到端更新 。实验证明，该系统能够高效完成自然语言生成、视频检索和语言检索三项任务，在 SVO 三元组预测和自然句子生成方面的表现优于 SVM、CRF 和 CCA 等基准方法 。",
    "image": ""
  },
  {
    "title": "A co-regularized approach to semi-supervised learning with multiple views",
    "link": "https://web.cse.ohio-state.edu/~belkin.8/papers/CASSL_ICML_05.pdf",
    "venue": "ICML",
    "date": "2005",
    "tags": [
      "Fusion"
    ],
    "source": "pliang_list",
    "code": "",
    "authors": "Vikas Sindhwani, Partha Niyogi, Mikhail Belkin",
    "description": "本文提出了一种用于多视图半监督学习的协同正则化（Co-Regularization）框架，旨在有效结合极少数有标签样本与大量无标签数据来构建高性能分类器 。该框架不同于传统的协同训练（Co-Training）算法，它并非采用贪婪的迭代搜索，而是通过在再生核希尔伯特空间（RKHS）中引入反映不同视图间“一致性”和“平滑性”的正则化项，将多视图学习转化为凸函数优化问题 。在此框架下，作者提出了协同正则化最小二乘法（Co-RLS）以及结合图拉普拉斯算子的协同拉普拉斯支持向量机（Co-LapSVM）等算法，这些方法能够利用不同视图揭示的互补相似性结构，即使在某一视图数据受损或标签极少的情况下，也能通过跨视图的正则化约束恢复准确的决策边界 。实验结果表明，该方法在合成数据集和 WebKB 网页分类任务中的表现均优于标准的协同训练和单视图半监督学习方法，其分类性能可以达到与全监督学习相媲美的水平 。",
    "image": ""
  },
  {
    "title": "Reconsidering Representation Alignment for Multi-view Clustering",
    "link": "https://openaccess.thecvf.com/content/CVPR2021/html/Trosten_Reconsidering_Representation_Alignment_for_Multi-View_Clustering_CVPR_2021_paper.html",
    "venue": "CVPR",
    "date": "2021",
    "tags": [
      "Alignment"
    ],
    "source": "pliang_list",
    "code": "https://github.com/DanielTrosten/mvc",
    "authors": "Daniel J. Trosten, Sigurd Løkse, Robert Jenssen, Michael Kampffmeyer",
    "description": "本文深入探讨了深度多模态聚类中表征分布对齐的局限性，指出盲目使用对抗训练进行分布对齐会阻碍模型对视图的自适应优先级排序，并在聚类结构仅部分存在于各视图时导致聚类合并，从而降低表征空间的分类能力 。研究者首先提出了一个名为 SiMVC 的简单基准模型，该模型完全弃用对齐操作，转而通过学习线性的视图权重组合来抑制无信息视图，实验证明其性能已能媲美甚至超越现有的先进方法 。在此基础上，作者进一步提出了 CoMVC 模型，通过引入一种改进的选择性对比学习模块，在样本层面利用余弦相似度进行表征角度对齐，并设计了自适应权重因子使模型能够在视图信息不足时自动禁用对齐程序 。实验结果显示，CoMVC 在 VOC、CCV、E-MNIST 等多个标准多视图数据集上显著刷新了聚类准确率（ACC）和标准互信息（NMI）的最佳纪录 。",
    "image": ""
  },
  {
    "title": "CoMIR: Contrastive Multimodal Image Representation for Registration",
    "link": "https://arxiv.org/pdf/2006.06325.pdf",
    "venue": "NeurIPS",
    "date": "2020",
    "tags": [
      "Alignment"
    ],
    "source": "pliang_list",
    "code": "https://github.com/MIDA-group/CoMIR",
    "authors": "Nicolas Pielawski, Elisabeth Wetzer, Johan Öfverstedt, et al.",
    "description": "本文提出了一种名为 CoMIRs（对比多模态图像表征）的对比编码方法，旨在学习共享且密集的多模态图像表征，以解决因图像结构差异巨大而导致现有方法失效的多模态配准难题。CoMIRs 的核心思想是将复杂的多模态配准问题转化为简单的单模态配准问题，从而允许应用通用的基于强度或特征的配准算法。该方法通过在已对齐的图像上为每种模态训练一个神经网络，并采用基于噪声对比估计（InfoNCE）的对比损失进行优化。与用于分类任务的传统对比编码不同，该方法生成的表征具有类似图像的特性，并包含模态间共享的关键信息。此外，研究者还为 InfoNCE 引入了一种无超参数的创新修改，以强制表征具备旋转等变性，这对于配准任务至关重要。通过在 RGB 与近红外远程传感数据集以及外观相关性极低的明场与二次谐波显微镜生物医学数据集上的实验，结果表明 CoMIRs 在配准性能上显著优于基于 GAN 的图像翻译方法以及考虑了额外数据先验知识的先进专用方法。",
    "image": ""
  },
  {
    "title": "Multimodal Transformer for Unaligned Multimodal Language Sequences",
    "link": "https://arxiv.org/abs/1906.00295",
    "venue": "ACL",
    "date": "2019",
    "tags": [
      "Alignment"
    ],
    "source": "pliang_list",
    "code": "https://github.com/yaohungt/Multimodal-Transformer",
    "authors": "Yao-Hung Hubert Tsai, Shaojie Bai, Paul Pu Liang, et al.",
    "description": "本文针对人类语言多模态建模中存在的两个主要挑战——由于各模态序列采样率不同导致的数据内在不对齐，以及跨模态元素间的长距离依赖问题，提出了一种名为多模态 Transformer（MulT）的通用端到端框架。该模型的核心在于定向成对跨模态注意力机制（directional pairwise crossmodal attention），它能够在不同时间步长上关注多模态序列间的交互，并以隐式方式将一个模态的流自适应地转换为另一个模态，从而在无需显式数据对齐的情况下解决建模难题。在对齐和非对齐的多模态时间序列数据集上进行的全面实验表明，MulT 的性能大幅领先于现有最先进方法；此外，实证分析进一步证实，该模型提出的跨模态注意力机制能够有效捕捉到相关的跨模态信号。",
    "image": ""
  },
  {
    "title": "Temporal Cycle-Consistency Learning",
    "link": "https://arxiv.org/abs/1904.07846",
    "venue": "CVPR",
    "date": "2019",
    "tags": [
      "Alignment"
    ],
    "source": "pliang_list",
    "code": "https://github.com/google-research/google-research/tree/master/tcc",
    "authors": "Debidatta Dwibedi, Yusuf Aytar, Jonathan Tompson, Pierre Sermanet, Andrew Zisserman",
    "description": "本文提出了一种基于视频间时间对齐任务的自监督表征学习方法，通过引入时间循环一致性（Temporal Cycle Consistency, TCC）这一可微的循环一致性损失函数，在多个视频中寻找跨时间的对应关系。该方法训练出的逐帧嵌入（embedding）能够通过在习得的表征空间中进行简单的最近邻匹配来实现视频对齐。为了评估该嵌入的效能，研究者在 Pouring 和 Penn Action 视频数据集上进行了密集的动作阶段标注，实验结果表明：首先，习得的嵌入支持对动作阶段进行少样本分类，显著降低了对监督训练数据的需求；其次，TCC 与 Shuffle and Learn、时间对比网络（TCN）等其他视频自监督学习方法具有互补性。此外，该嵌入还被应用于一系列基于视频对间密集时间对应关系的任务，包括视频间同步模态元数据（如声音、时间语义标签）的迁移、多视频的同步播放以及异常检测。",
    "image": ""
  },
  {
    "title": "See, Hear, and Read: Deep Aligned Representations",
    "link": "https://people.csail.mit.edu/yusuf/see-hear-read/paper.pdf",
    "venue": "arXiv",
    "date": "2017",
    "tags": [
      "Alignment"
    ],
    "source": "pliang_list",
    "code": "",
    "authors": "Yusuf Aytar, Carl Vondrick, Antonio Torralba",
    "description": "本文利用大量易于获取的同步数据，学习了一种跨视觉、声音和语言三大自然模态的深度判别式共享表征。研究者通过利用超过一年的视频音频数据以及数百万计的图像-文本对，训练了一个深度卷积网络来实现多模态的对齐表征学习。实验结果表明，这种习得的表征在跨模态检索和模态间分类器迁移等任务中表现优异；特别是在模型仅使用“图像+文本”和“图像+声音”对进行训练的情况下，它依然能够实现在训练中从未观察过的文本与声音之间的模态转换。通过对表征的视觉化分析发现，网络内部自动涌现出了许多能够独立于模态检测特定概念的隐藏单元，有力证明了该统一表征空间在抽象语义理解上的有效性。",
    "image": ""
  },
  {
    "title": "On Deep Multi-View Representation Learning",
    "link": "http://proceedings.mlr.press/v37/wangb15.pdf",
    "venue": "ICML",
    "date": "2015",
    "tags": [
      "Alignment"
    ],
    "source": "pliang_list",
    "code": "",
    "authors": "Weiran Wang, Raman Arora, Karen Livescu, Jeff Bilmes",
    "description": "本文研究了在训练阶段可获得多个无标签视图、但在测试阶段仅有一个视图可用时的多视图表征学习问题。作者系统地分析并对比了以往基于深度神经网络的多种技术，包括以重建为目标的类自动编码器网络和以相关性为目标的配对前馈网络，并在此基础上提出了一种新型的深度典型相关自动编码器（Deep Canonically Correlated Autoencoders, DCCAE）。DCCAE 通过在保持每个视图重建能力的同时，最大化不同视图在潜空间中的相关性，从而学习到更具判别力的特征。在视觉（MNIST）、语音（XRMB）和语言（Flickr8k）等多个领域的实验结果表明，DCCAE 在大多数任务上均优于现有的深度 CCA（DCCA）以及多视图自动编码器等方法，是首个针对此类技术在多任务下进行的全面对比研究。",
    "image": ""
  },
  {
    "title": "Unsupervised Alignment of Natural Language Instructions with Video Segments",
    "link": "https://dl.acm.org/citation.cfm?id=2892753.2892769",
    "venue": "AAAI",
    "date": "2014",
    "tags": [
      "Alignment"
    ],
    "source": "pliang_list",
    "code": "",
    "authors": "Iftekhar Naim, Young Chol Song, Qiguang Liu, et al.",
    "description": "本文提出了一种无监督学习算法，旨在自动推断英语名词与相应视频对象之间的映射关系 。该算法针对自然语言指令序列及与之未对齐的视频记录，能够同步实现指令与对应视频片段的对齐，以及指令中名词与视频中对应物体的匹配 。不同于现有的依赖于“句子-视频片段”预对齐监督数据的接地语言获取（Grounded Language Acquisition）算法，该方法致力于从视频的时间结构和并行文本指令中自动推断对齐关系 。为此，研究者提出了两种生成模型，这些模型与统计机器翻译中使用的隐马尔可夫模型（HMM）和 IBM 1 词对齐模型密切相关 。通过在湿实验室（Wetlab）生物学实验视频上的评估，该算法证明了在没有任何直接监督的情况下，将视频片段与文本指令对齐以及将视频对象与名词匹配的能力 。",
    "image": ""
  },
  {
    "title": "Multimodal Alignment of Videos",
    "link": "https://dl.acm.org/citation.cfm?id=2654862",
    "venue": "MM",
    "date": "2014",
    "tags": [
      "Alignment"
    ],
    "source": "pliang_list",
    "code": "",
    "authors": "Mario Guggenberger",
    "description": "本文针对现有大多数多媒体同步方法仅关注音频或视频单一模态，且新兴的多模态融合方法在处理来自异构源的真实录制数据时效果欠佳、视频处理计算成本过高等问题，提出了一系列改进方案。该博士论文的研究目标主要包括：首先，深入探讨在实验室环境下开发的方法通常会忽略、但会严重损害实际应用效果的基础性问题；其次，构建并发布一个可供学术界公开使用的多媒体同步方法基准测试数据集；最后，开发一种基于底层视频特征的同步方法，并确保其计算复杂度不高于目前最先进的基于音频的同步方法，从而实现在社交活动等大规模异构录制场景下的高效自动同步。",
    "image": ""
  },
  {
    "title": "Deep Canonical Correlation Analysis",
    "link": "http://proceedings.mlr.press/v28/andrew13.html",
    "venue": "ICML",
    "date": "2013",
    "tags": [
      "Alignment"
    ],
    "source": "pliang_list",
    "code": "https://github.com/VahidooX/DeepCCA",
    "authors": "Galen Andrew, Raman Arora, Jeff Bilmes, Karen Livescu",
    "description": "本文提出了一种名为深度典型相关分析（Deep CCA, DCCA）的学习方法，旨在通过深度神经网络学习两个视图之间的非线性映射，使得变换后的特征在潜空间中具有最大的相关性 。与传统的线性典型相关分析（CCA）相比，DCCA 不受线性变换的局限，能够捕捉模态间更复杂的非线性依赖关系；同时，相比于基于核函数的 KCCA，DCCA 在处理大规模数据集时不仅更具扩展性，且无需存储大量训练样本 。该算法通过多层感知器构建两个视图的映射函数，并采用随机梯度下降法直接优化所有网络参数，以最大化输出层特征的典型相关性总和 。在视觉（MNIST 手写体）和语音（XRMB 嘴唇运动与声学数据）两个跨模态任务上的实验结果表明，DCCA 学习到的表征在相关性水平上显著优于线性 CCA 和 KCCA，并表现出更强的判别力 。",
    "image": ""
  }
]