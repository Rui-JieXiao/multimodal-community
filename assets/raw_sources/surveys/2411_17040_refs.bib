

@misc{vaswani2017attention,
	title = {Attention Is All You Need},
	publisher = {{arXiv}},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
	eprinttype = {arxiv},
	eprint = {1706.03762 [cs]},
    number = {{arXiv}:1706.03762},
}

@article{Tianyang2022Asurvey,
title = {A survey of transformers},
journal = {AI Open},
volume = {3},
pages = {111-132},
year = {2022},
issn = {2666-6510},
author = {Tianyang Lin and Yuxin Wang and Xiangyang Liu and Xipeng Qiu},
}

@article{tang2019fast,
  title={Fast and robust dynamic hand gesture recognition via key frames extraction and feature fusion},
  author={Tang, Hao and Liu, Hong and Xiao, Wei and Sebe, Nicu},
  journal={Elsevier Neurocomputing},
  volume={331},
  pages={424--433},
  year={2019}
}

@inproceedings{duan2021audio,
  title={Audio-visual event localization via recursive fusion by joint co-attention},
  author={Duan, Bin and Tang, Hao and Wang, Wei and Zong, Ziliang and Yang, Guowei and Yan, Yan},
  booktitle={WACV},
  year={2021}
}

@inproceedings{tang2020local,
  title={Local class-specific and global image-level generative adversarial networks for semantic-guided scene generation},
  author={Tang, Hao and Xu, Dan and Yan, Yan and Torr, Philip HS and Sebe, Nicu},
  booktitle={CVPR},
  year={2020}
}

@inproceedings{shi2022charformer,
  title={Charformer: A glyph fusion based attentive framework for high-precision character image denoising},
  author={Shi, Daqian and Diao, Xiaolei and Shi, Lida and Tang, Hao and Chi, Yang and Li, Chuntao and Xu, Hao},
  booktitle={ACM MM},
  year={2022}
}

@article{tang2021attentiongan,
  title={Attentiongan: Unpaired image-to-image translation using attention-guided generative adversarial networks},
  author={Tang, Hao and Liu, Hong and Xu, Dan and Torr, Philip HS and Sebe, Nicu},
  journal={IEEE TNNLS},
  volume={34},
  number={4},
  pages={1972--1987},
  year={2021}
}

@article{tang2025enhanced,
  title={Enhanced Multi-Scale Cross-Attention for Person Image Generation},
  author={Tang, Hao and Shao, Ling and Sebe, Nicu and Van Gool, Luc},
  journal={IEEE TPAMI},
  year={2025}
}

@article{yang2022continual,
  title={Continual attentive fusion for incremental learning in semantic segmentation},
  author={Yang, Guanglei and Fini, Enrico and Xu, Dan and Rota, Paolo and Ding, Mingli and Tang, Hao and Alameda-Pineda, Xavier and Ricci, Elisa},
  journal={IEEE TMM},
  volume={25},
  pages={3841--3854},
  year={2022}
}

@misc{islam2023comprehensive,
	title = {A Comprehensive Survey on Applications of Transformers for Deep Learning Tasks},
	number = {{arXiv}:2306.07303},
	publisher = {{arXiv}},
	author = {Islam, Saidul and Elmekki, Hanae and Elsebai, Ahmed and Bentahar, Jamal and Drawel, Najat and Rjoub, Gaith and Pedrycz, Witold},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2306.07303 [cs]},
}

@article{Khan2022assosiation,
    author = {Khan, Salman and Naseer, Muzammal and Hayat, Munawar and Zamir, Syed Waqas and Khan, Fahad Shahbaz and Shah, Mubarak},
    title = {Transformers in Vision: A Survey},
    year = {2022},
    issue_date = {January 2022},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    volume = {54},
    number = {10s},
    issn = {0360-0300},
    doi = {10.1145/3505244},
    journal = {ACM Comput. Surv.},
    month = {sep},
    articleno = {200},
    numpages = {41},
}

@inproceedings{radford_language_nodate,
  title={Language Models are Unsupervised Multitask Learners},
  author={Alec Radford and Jeff Wu and Rewon Child and David Luan and Dario Amodei and Ilya Sutskever},
  year={2019},
  booktitle = {},
}

@misc{openai_gpt-4_2024,
	title = {{GPT}-4 Technical Report},
	doi = {10.48550/arXiv.2303.08774},
	number = {{arXiv}:2303.08774},
	publisher = {{arXiv}},
	author = {{OpenAI} and Achiam, Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and et al. },
	eprinttype = {arxiv},
	eprint = {2303.08774 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@misc{atli2024i2imamba,
      title={I2I-Mamba: Multi-modal medical image synthesis via selective state space modeling}, 
      author={Omer F. Atli and Bilal Kabas and Fuat Arslan and Mahmut Yurt and Onat Dalmaz and Tolga Çukur},
      year={2024},
      eprint={2405.14022},
      archivePrefix={arXiv},
      primaryClass={eess.IV}
}

@misc{jafari2024jamba,
      title={JambaTalk: Speech-Driven 3D Talking Head Generation Based on Hybrid Transformer-Mamba Model}, 
      author={Farzaneh Jafari and Stefano Berretti and Anup Basu},
      year={2024},
      eprint={2408.01627},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{fang2024gfemambamamba,
      title={GFE-Mamba: Mamba-based AD Multi-modal Progression Assessment via Generative Feature Extraction from MCI}, 
      author={Zhaojie Fang and Shenghao Zhu and Yifei Chen and Binfeng Zou and Fan Jia and Linwei Qiu and Chang Liu and Yiyu Huang and Xiang Feng and Feiwei Qin and Changmiao Wang and Yeru Wang and Jin Fan and Changbiao Chu and Wan-Zhen Wu and Hu Zhao},
      year={2024},
      eprint={2407.15719},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{li2024uvmamba,
      title={UV-Mamba: A DCN-Enhanced State Space Model for Urban Village Boundary Identification in High-Resolution Remote Sensing Images}, 
      author={Lulin Li and Ben Chen and Xuechao Zou and Junliang Xing and Pin Tao},
      year={2024},
      eprint={2409.03431},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{wang2024pyramid,
      title={PyramidMamba: Rethinking Pyramid Feature Fusion with Selective Space State Model for Semantic Segmentation of Remote Sensing Imagery}, 
      author={Libo Wang and Dongxu Li and Sijun Dong and Xiaoliang Meng and Xiaokang Zhang and Danfeng Hong},
      year={2024},
      eprint={2406.10828},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{brown_language_2020,
	title = {Language Models are Few-Shot Learners},
	doi = {10.48550/arXiv.2005.14165},
	number = {{arXiv}:2005.14165},
	publisher = {{arXiv}},
	author = {Brown, Tom B. and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and et al.},
	date = {2020-07-22},
	eprinttype = {arxiv},
	eprint = {2005.14165 [cs]},
}

@misc{dubey_llama_2024,
	title = {The Llama 3 Herd of Models},
	number = {{arXiv}:2407.21783},
	publisher = {{arXiv}},
	author = {Dubey, Abhimanyu and Jauhri, Abhinav and Pandey, Abhinav and Kadian, Abhishek and Al-Dahle, Ahmad and Letman, Aiesha and et al. },
	date = {2024-08-15},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2407.21783 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition},
}

@inproceedings{Han2024OneLLM,
  title={OneLLM: One Framework to Align All Modalities with Language},
  author={Han et al.},
  booktitle={CVPR},
  year={2024}
}


@article{li2024dataprocessingtechniquesmodern,
  title={Data processing techniques for modern multimodal models},
  author={Li, Yinheng and Ding, Han and Chen, Hang},
  journal={arXiv preprint arXiv:2407.19180},
  year={2024}
}

@article{Liang2024Foundations,
    author = {Liang, Paul Pu and Zadeh, Amir and Morency, Louis-Philippe},
    title = {Foundations \& Trends in Multimodal Machine Learning: Principles, Challenges, and Open Questions},
    year = {2024},
    issue_date = {October 2024},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    volume = {56},
    number = {10},
    issn = {0360-0300},
    doi = {10.1145/3656580},
    journal = {ACM Comput. Surv.},
    month = {jun},
    articleno = {264},
    numpages = {42},
}

@inproceedings{shen2024aligning,
  title={Aligning and prompting everything all at once for universal visual perception},
  author={Shen, Yilun and Fu, Cheng and Chen, Peng and Zhang, Ming and Li, Kai and Sun, Xiaofeng and Wu, Yinan and Lin, Shuchang and Ji, Rongrong},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2024}
}

@misc{kirillov2023segment,
      title={Segment Anything}, 
      author={Alexander Kirillov and Eric Mintun and Nikhila Ravi and Hanzi Mao and Chloe Rolland and Laura Gustafson and Tete Xiao and Spencer Whitehead and Alexander C. Berg and Wan-Yen Lo and Piotr Dollár and Ross Girshick},
      year={2023},
      eprint={2304.02643},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{zhang2022dinodetrimproveddenoising,
      title={DINO: DETR with Improved DeNoising Anchor Boxes for End-to-End Object Detection}, 
      author={Hao Zhang and Feng Li and Shilong Liu and Lei Zhang and Hang Su and Jun Zhu and Lionel M. Ni and Heung-Yeung Shum},
      year={2022},
      eprint={2203.03605},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{oquab2024dinov2learningrobustvisual,
      title={DINOv2: Learning Robust Visual Features without Supervision}, 
      author={Maxime Oquab and Timothée Darcet and Théo Moutakanni and Huy Vo and Marc Szafraniec and Vasil Khalidov and et al.},
      year={2024},
      eprint={2304.07193},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@article{hotelling1936cca,
  author = {Harold Hotelling},
  title = {Relations Between Two Sets of Variates},
  journal = {Biometrika},
  volume = {28},
  number = {3-4},
  pages = {321--377},
  year = {1936},
  month = dec,
  doi = {10.1093/biomet/28.3-4.321},
}

@book{Anderson1984cca,
  author = {Anderson, T. W.},
  title = {An Introduction to Multivariate Statistical Analysis},
  edition = {2nd},
  year = {1984},
  publisher = {John Wiley and Sons}
}

@inproceedings{Akaho2001kcca,
  author = {Akaho, S.},
  title = {A kernel method for canonical correlation analysis},
  booktitle = {Proceedings of the International Meeting on Psychometric Society},
  year = {2001}
}

@inproceedings{Melzer2001kcca,
  author = {Melzer, T. and Reiter, M. and Bischof, H.},
  title = {Nonlinear feature extraction using generalized canonical correlation analysis},
  booktitle = {Proceedings of the International Conference on Artificial Neural Networks (ICANN)},
  year = {2001}
}

@article{Bach2002kcca,
  author = {Bach, F. R. and Jordan, M. I.},
  title = {Kernel independent component analysis},
  journal = {Journal of Machine Learning Research},
  volume = {3},
  pages = {1--48},
  year = {2002}
}

@article{Hardoon2004kcca,
  author = {Hardoon, D. R. and Szedmak, S. and Shawe-Taylor, J.},
  title = {Canonical correlation analysis: An overview with application to learning methods},
  journal = {Neural Computation},
  volume = {16},
  number = {12},
  pages = {2639--2664},
  year = {2004}
}

@inproceedings{Verma2014svm,
  author = {Y. Verma and C. V. Jawahar},
  title = {Im2Text and Text2Im: Associating images and texts for cross-modal retrieval},
  booktitle = {Proceedings of the British Machine Vision Conference (BMVC)},
  year = {2014},
  pages = {2},
}

@article{li2023clip,
  title={CLIP: Connecting Vision and Language},
  author={Li, J. and Zhao, Y.},
  journal={Proceedings of the IEEE},
  year={2023}
}

@article{wang2023ofa,
  title={OFA: Unified Multimodal Framework},
  author={Wang, Y. and Zhao, S.},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
  year={2023}
}

@article{yang2023videochat,
  title={VideoChat: Conversational Agents in Video Understanding},
  author={Yang, H. and Li, S.},
  journal={IEEE Transactions on Neural Networks and Learning Systems},
  year={2023}
}

@article{zhang2023hugginggpt,
  title={HuggingGPT: Leveraging Language Models for Multimodal Tasks},
  author={Zhang, Q. and Li, M.},
  journal={IEEE Transactions on Image Processing},
  year={2023}
}

@misc{chen2023instructblip,
      title={InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning}, 
      author={Wenliang Dai and Junnan Li and Dongxu Li and Anthony Meng Huat Tiong and Junqi Zhao and Weisheng Wang and Boyang Li and Pascale Fung and Steven Hoi},
      year={2023},
      eprint={2305.06500},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@article{chen2023instructblip2,
  title={InstructBLIP 2: Extending Vision-Language Models with Fine-Grained Instruction Tuning},
  author={Chen, H. and Xu, T.},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
  year={2023}
}

@article{gao2023shikra,
  title={Shikra: Region-level Multimodal Model for Fine-Grained Interaction},
  author={Gao, J. and Wang, Y.},
  journal={IEEE Transactions on Neural Networks and Learning Systems},
  year={2023}
}

@article{wang2023osprey,
  title={Osprey: Multimodal Framework with Click-Based Interaction},
  author={Wang, Y. and Li, X.},
  journal={IEEE Transactions on Multimedia},
  year={2023}
}

@article{wang2023imagebind,
  title={ImageBind: Multimodal Pretraining with Boundaries},
  author={Wang, Y. and Li, S.},
  journal={arXiv preprint arXiv:2305.00001},
  year={2023}
}

@article{liu2023nextgpt,
  title={NExT-GPT: A Vision-Language Model with Cross-Modality Generation},
  author={Liu, M. and Zhang, W.},
  journal={arXiv preprint arXiv:2307.00001},
  year={2023}
}

@article{liu2023viscpm,
  title={VisCPM: A Multilingual Vision-Language Model for Chinese and English},
  author={Liu, H. and Zhang, Y.},
  journal={IEEE Transactions on Neural Networks and Learning Systems},
  year={2023}
}

@article{liu2023visualinstructiontuning,
  title={Visual instruction tuning},
  author={Liu, Haotian and Li, Chunyuan and Wu, Qingyang and Lee, Yong Jae},
  journal={Advances in neural information processing systems},
  volume={36},
  year={2024}
}

@article{huang2023biomedgpt,
  title={BiomedGPT: Large Multimodal Model for Biomedical Image Understanding},
  author={Huang, L. and Zhao, T.},
  journal={IEEE TMI},
  year={2023}
}

@article{chen2023llavamed,
  title={LLaVA-Med: Medical Image Understanding with Large Language Models},
  author={Chen, K. and Sun, Y.},
  journal={IEEE TNNLS},
  year={2023}
}

@article{yang2024large,
  author    = {Yang, X. and Wang, Z. and Wang, Q. and Wei, K. and Zhang, K. and Shi, J.},
  title     = {Large language models for automated Q\&A involving legal documents: a survey on algorithms, frameworks and applications},
  journal   = {International Journal of Web Information Systems},
  volume    = {20},
  number    = {4},
  pages     = {413--435},
  year      = {2024},
  doi       = {10.1108/IJWIS-12-2023-0256},
}

@misc{xie2024openfinllmsopenmultimodallarge,
      title={Open-FinLLMs: Open Multimodal Large Language Models for Financial Applications}, 
      author={Qianqian Xie and Dong Li and Mengxi Xiao and Zihao Jiang and Ruoyu Xiang and Xiao Zhang and et al.},
      year={2024},
      eprint={2408.11878},
      archivePrefix={arXiv},
      primaryClass={cs.CL}, 
}

@inproceedings{zhang2024AMultimodalFoundation,
author = {Zhang, Wentao and Zhao, Lingxuan and Xia, Haochong and Sun, Shuo and Sun, Jiaze and Qin, Molei and et al.},
title = {A Multimodal Foundation Agent for Financial Trading: Tool-Augmented, Diversified, and Generalist},
year = {2024},
booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
pages = {4314–4325},
numpages = {12},
location = {Barcelona, Spain},
series = {KDD '24}
}


@inproceedings{gabeur_multi-modal_2020,
  title={Multi-modal transformer for video retrieval},
  author={Gabeur, Valentin and Sun, Chen and Alahari, Karteek and Schmid, Cordelia},
  booktitle={ECCV},
  pages={214--229},
  year={2020}
}

@article{tellamekala_cold_2024,
	title = {{COLD} Fusion: Calibrated and Ordinal Latent Distribution Fusion for Uncertainty-Aware Multimodal Emotion Recognition},
	volume = {46},
	issn = {1939-3539},
	pages = {805--822},
	number = {2},
	journal = {{IEEE} TPAMI},
	author = {Tellamekala, Mani Kumar and Amiriparian, Shahin and Schuller, Björn W. and André, Elisabeth and Giesbrecht, Timo and Valstar, Michel},
year = {2024}
}

@misc{li2023scalinglanguageimagepretrainingmasking,
      title={Scaling Language-Image Pre-training via Masking}, 
      author={Yanghao Li and Haoqi Fan and Ronghang Hu and Christoph Feichtenhofer and Kaiming He},
      year={2023},
      eprint={2212.00794},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{zhang2023internlmx,
      title={InternLM-XComposer: A Vision-Language Large Model for Advanced Text-image Comprehension and Composition}, 
      author={Pan Zhang and Xiaoyi Dong and Bin Wang and Yuhang Cao and Chao Xu and Linke Ouyang and Zhiyuan Zhao and Haodong Duan and Songyang Zhang and Shuangrui Ding and Wenwei Zhang and Hang Yan and Xinyue Zhang and Wei Li and Jingwen Li and Kai Chen and Conghui He and Xingcheng Zhang and Yu Qiao and Dahua Lin and Jiaqi Wang},
      year={2023},
      eprint={2309.15112},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{ai2024yiopen,
      title={Yi: Open Foundation Models by 01.AI}, 
      author={Alex Young and Bei Chen and Chao Li and Chengen Huang and Ge Zhang and Guanwei Zhang and Heng Li and Jiangcheng Zhu and Jianqun Chen and Jing Chang and Kaidong Yu and Peng Liu and Qiang Liu and Shawn Yue and Senbin Yang and Shiming Yang and Tao Yu and Wen Xie and Wenhao Huang and Xiaohui Hu and Xiaoyi Ren and Xinyao Niu and Pengcheng Nie and Yuchi Xu and Yudong Liu and Yue Wang and Yuxuan Cai and Zhenyu Gu and Zhiyuan Liu and Zonghong Dai},
      year={2024},
      eprint={2403.04652},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}


@misc{liu2024mapper,
      title={MaPPER: Multimodal Prior-guided Parameter Efficient Tuning for Referring Expression Comprehension}, 
      author={Ting Liu and Zunnan Xu and Yue Hu and Liangtao Shi and Zhiqiang Wang and Quanjun Yin},
      year={2024},
      eprint={2409.13609},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{zhang2024internlmx2.5,
      title={InternLM-XComposer-2.5: A Versatile Large Vision Language Model Supporting Long-Contextual Input and Output}, 
      author={Pan Zhang and Xiaoyi Dong and Yuhang Zang and Yuhang Cao and Rui Qian and Lin Chen and Qipeng Guo and Haodong Duan and Bin Wang and Linke Ouyang and Songyang Zhang and Wenwei Zhang and Yining Li and Yang Gao and Peng Sun and Xinyue Zhang and Wei Li and Jingwen Li and Wenhai Wang and Hang Yan and Conghui He and Xingcheng Zhang and Kai Chen and Jifeng Dai and Yu Qiao and Dahua Lin and Jiaqi Wang},
      year={2024},
      eprint={2407.03320},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{he2021maskedautoencodersscalablevision,
      title={Masked Autoencoders Are Scalable Vision Learners}, 
      author={Kaiming He and Xinlei Chen and Saining Xie and Yanghao Li and Piotr Dollár and Ross Girshick},
      year={2021},
      eprint={2111.06377},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@inproceedings{radford_clip_2021,
	title = {Learning Transferable Visual Models From Natural Language Supervision},
	pages = {8748--8763},
	booktitle = {ICML},
	author = {Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and Krueger, Gretchen and Sutskever, Ilya},
	year = {2021}
}


@misc{yin_survey_2023,
	title = {A Survey on Multimodal Large Language Models},
	abstract = {A review of recent progress in Multimodal Large Language Models (MLLMs), discussing their architectures, training strategies, and evaluations. It also covers research topics on extending MLLMs' capabilities and addressing current challenges.},
	author = {Yin, Shukang and Fu, Chaoyou and Zhao, Sirui and Li, Ke and Sun, Xing and Xu, Tong and Chen, Enhong}
}

@misc{vouitsis2024dataefficient,
      title={Data-Efficient Multimodal Fusion on a Single GPU}, 
      author={Noël Vouitsis and Zhaoyan Liu and Satya Krishna Gorti and Valentin Villecroze and Jesse C. Cresswell and Guangwei Yu and Gabriel Loaiza-Ganem and Maksims Volkovs},
      year={2024},
      eprint={2312.10144},
      archivePrefix={arXiv},
      primaryClass={cs.LG}, 
}

@inproceedings{Wan_2024_CVPR,
  author    = {Yongquan Wan and Wenhai Wang and Guobing Zou and Bofeng Zhang},
  title     = {Cross-modal Feature Alignment and Fusion for Composed Image Retrieval},
  booktitle = {CVPRW},
  year      = {2024},
  pages     = {8384--8388},
}


@ARTICLE{7945502,
  author={Noroozi, Fatemeh and Marjanovic, Marina and Njegus, Angelina and Escalera, Sergio and Anbarjafari, Gholamreza},
  journal={IEEE Transactions on Affective Computing}, 
  title={Audio-Visual Emotion Recognition in Video Clips}, 
  year={2019},
  volume={10},
  number={1},
  pages={60-75}
}


@article{barua_systematic_2023,
	title = {A Systematic Literature Review on Multimodal Machine Learning: Applications, Challenges, Gaps and Future Directions},
	volume = {11},
	pages = {14804--14831},
	journal = {IEEE Access},
	author = {Barua, Arnab and Ahmed, Mobyen Uddin and Begum, Shahina},
	year = {2023}
}

@misc{song2024setclip,
      title={Set-CLIP: Exploring Aligned Semantic From Low-Alignment Multimodal Data Through A Distribution View}, 
      author={Zijia Song and Zelin Zang and Yelin Wang and Guozheng Yang and Kaicheng yu and Wanyu Chen and Miaoyu Wang and Stan Z. Li},
      year={2024},
      eprint={2406.05766},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
}

@article{tadas2019multimodal,
	title = {Multimodal Machine Learning: A Survey and Taxonomy},
	volume = {41},
	issn = {0162-8828},
	number = {2},
	journal = {IEEE TPAMI},
	author = {Baltrusaitis, Tadas and Ahuja, Chaitanya and Morency, Louis-Philippe},
	date = {2019},
    year = {2018}
}

@article{zhou_state_2023,
	title = {The State of the Art for Cross-Modal Retrieval: A Survey},
	volume = {11},
	issn = {2169-3536},
	pages = {138568--138589},
	journal = {IEEE Access},
	author = {Zhou, Kun and Hassan, Fadratul Hafinaz and Hoon, Gan Keng},
	date = {2023},
	langid = {american}
}

@article{ma_multimodality_2021,
  title={Multimodality in meta-learning: A comprehensive survey},
  author={Ma, Yao and Zhao, Shilin and Wang, Weixiao and Li, Yaoman and King, Irwin},
  journal={Elsevier Knowledge-Based Systems},
  volume={250},
  pages={108976},
  year={2022}
}

@inproceedings{wu2023learning,
  title={Learning concordant attention via target-aware alignment for visible-infrared person re-identification},
  author={Wu, Jianbing and Liu, Hong and Su, Yuxin and Shi, Wei and Tang, Hao},
  booktitle={ICCV},
  year={2023}
}



@misc{zhu_visionx_2024,
	title = {Vision+X: A Survey on Multimodal Learning in the Light of Data},
	shorttitle = {Vision+X},
	number = {{arXiv}:2210.02884},
	publisher = {{arXiv}},
	author = {Zhu, Ye and Wu, Yu and Sebe, Nicu and Yan, Yan},
	date = {2024-06-07},
	langid = {english},
}

@article{zhao_deep_2024,
	title = {Deep Multimodal Data Fusion},
	volume = {56},
	issn = {0360-0300, 1557-7341},
	abstract = {Multimodal Artificial Intelligence (Multimodal {AI}), in general, involves various types of data (e.g., images, texts, or data collected from different sensors), },
	pages = {1--36},
	number = {9},
	journal = {{ACM} Computing Surveys},
	shortjournal = {{ACM} Comput. Surv.},
	author = {Zhao, Fei and Zhang, Chengcui and Geng, Baocheng},
	year = {2024},
	langid = {english},
}


@inproceedings{alayrac_flamingo_2022,
	title = {Flamingo: A Visual Language Model for Few-Shot Learning},
	author = {Alayrac, Jean-Baptiste and Donahue, Jeff and Luc, Pauline and Miech, Antoine and Barr, Iain and Hasson, Yana and Lenc, Karel and Mensch, Arthur and Millican, Katie and Reynolds, Malcolm and Ring, Roman and Rutherford, Eliza and Cabi, Serkan and Han, Tengda and Gong, Zhitao and Samangooei, Sina and Monteiro, Marianne and Menick, Jacob and Borgeaud, Sebastian and Brock, Andrew and Nematzadeh, Aida and Sharifzadeh, Sahand and Binkowski, Mikolaj and Barreira, Ricardo and Vinyals, Oriol and Zisserman, Andrew and Simonyan, Karen},
	booktitle = {NeurIPS},
	year = {2022}
}

@misc{wang_cogvlm_2024,
	title = {{CogVLM}: Visual Expert for Pretrained Language Models},
	shorttitle = {{CogVLM}},
	abstract = {We introduce {CogVLM}, a powerful open-source visual language foundation model. Different from the popular shallow alignment method which maps image features into the input space of language model, {CogVLM} bridges the gap between the frozen pretrained language model },
	author = {Wang, Weihan and Lv, Qingsong and Yu, Wenmeng and Hong, Wenyi and Qi, Ji and Wang, Yan and Ji, Junhui and Yang, Zhuoyi and Zhao, Lei and Song, Xixuan and Xu, Jiazheng and Xu, Bin and Li, Juanzi and Dong, Yuxiao and Ding, Ming and Tang, Jie},
	date = {2024-02-04},
	langid = {american},
	eprinttype = {arxiv},
}

@misc{zhang_llama-adapter_2024,
	title = {{LLaMA}-Adapter: Efficient Fine-tuning of Language Models with Zero-init Attention},
	shorttitle = {{LLaMA}-Adapter},
	abstract = {We present {LLaMA}-Adapter, a lightweight adaption method to efficiently fine-tune {LLaMA} into an instruction-following model. Using 52K self-instruct demonstrations, },
	number = {{arXiv}:2303.16199},
	publisher = {{arXiv}},
	author = {Zhang, Renrui and Han, Jiaming and Liu, Chris and Gao, Peng and Zhou, Aojun and Hu, Xiangfei and Yan, Shilin and Lu, Pan and Li, Hongsheng and Qiao, Yu},
	date = {2024-09-18},
	langid = {american},
}

@inproceedings{shutova_black_2016,
	location = {San Diego, California},
	title = {Black Holes and White Rabbits: Metaphor Identification with Visual Features},
	pages = {160--170},
	booktitle = {Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
	author = {Shutova, Ekaterina and Kiela, Douwe and Maillard, Jean},
	date = {2016},
}

@inproceedings{morvant_majority_2014,
	title = {Majority Vote of Diverse Classifiers for Late Fusion},
	pages = {153--162},
	booktitle = {Structural, Syntactic, and Statistical Pattern Recognition},
	publisher = {Springer},
	author = {Morvant, Emilie and Habrard, Amaury and Ayache, Stéphane},
	year = {2014},
}

@article{BinteRashid2024NavigatingTM,
  title={Navigating the Multimodal Landscape: A Review on Integration of Text and Image Data in Machine Learning Architectures},
  author={Maisha Binte Rashid and Md Shahidur Rahaman and Pablo Rivas},
  journal={Machine Learning and Knowledge Extraction},
  year={2024},
}

@article{Snoek2005EarlyVL,
  title={Early versus late fusion in semantic video analysis},
  author={Cees G. M. Snoek and Marcel Worring and Arnold W. M. Smeulders},
  journal={ACM MM},
  year={2005},
}

@inproceedings{Pereira2023OnCE,
  title={On Comparing Early and Late Fusion Methods},
  author={Luis Manuel Pereira and Addisson Salazar and Luis Vergara},
  booktitle={International Work-Conference on Artificial and Natural Neural Networks},
  year={2023},
}

@article{Vibashan2021ImageFT,
  title={Image Fusion Transformer},
  author={VS Vibashan and Jeya Maria Jose Valanarasu and Poojan Oza and Vishal M. Patel},
  journal={ICIP},
  year={2021}
}

@article{Zhang2023TransformerBasedMF,
  title={Transformer-Based Multimodal Fusion for Early Diagnosis of Alzheimer's Disease Using Structural MRI And PET},
  author={Yuanwang Zhang and Kaicong Sun and Yuxiao Liu and Dinggang Shen},
  journal={2023 IEEE 20th International Symposium on Biomedical Imaging (ISBI)},
  year={2023},
  pages={1-5},
}

@article{Zhang2022TransformerBasedEA,
  title={Transformer-Based End-to-End Anatomical and Functional Image Fusion},
  author={Jing Zhang and Aiping Liu and Dan Wang and Yu Liu and Z. Jane Wang and Xun Chen},
  journal={IEEE Transactions on Instrumentation and Measurement},
  year={2022},
  volume={71},
  pages={1-11},
}

@article{Tang2023BAFN,title={BAFN: Bi-Direction Attention Based Fusion Network for Multimodal Sentiment Analysis},author={Jiajia Tang and Dongjun Liu and Xuanyu Jin and Yong Peng and Qianchuan Zhao and Yu Ding and Wanzeng Kong},journal={IEEE Transactions on Circuits and Systems for Video Technology},year={2023},volume={33},pages={1966-1978},doi={10.1109/TCSVT.2022.3218018}}

@article{Wang2023Mutually,title={Mutually Beneficial Transformer for Multimodal Data Fusion},author={Jinping Wang and Xiaojun Tan},journal={IEEE Transactions on Circuits and Systems for Video Technology},year={2023},volume={33},pages={7466-7479},doi={10.1109/TCSVT.2023.3274545}}

@article{Badrinarayanan2015SegNetAD,
  title={SegNet: A Deep Convolutional Encoder-Decoder Architecture for Image Segmentation},
  author={Vijay Badrinarayanan and Alex Kendall and Roberto Cipolla},
  journal={IEEE TPAMI},
  year={2015}
}

@misc{desai2021redcapswebcuratedimagetextdata,
      title={RedCaps: web-curated image-text data created by the people, for the people}, 
      author={Karan Desai and Gaurav Kaul and Zubin Aysola and Justin Johnson},
      year={2021}
}

@inproceedings{Uezato2020GuidedDD,
  title={Guided Deep Decoder: Unsupervised Image Pair Fusion},
  author={Tatsumi Uezato and Danfeng Hong and Naoto Yokoya and Wei He},
  booktitle={ECCV},
  year={2020},
}

@article{Li2018DenseFuseAF,
  title={DenseFuse: A Fusion Approach to Infrared and Visible Images},
  author={Hui Li and Xiaojun Wu},
  journal={IEEE TIP},
  year={2018}
}

@article{Rvid2019TowardsRS,
  title={Towards Raw Sensor Fusion in 3D Object Detection},
  author={Andr{\'a}s R{\"o}vid and Viktor Remeli},
  journal={2019 IEEE 17th World Symposium on Applied Machine Intelligence and Informatics (SAMI)},
  year={2019},
  pages={293-298},
}

@misc{xu2018pointfusiondeepsensorfusion,
      title={PointFusion: Deep Sensor Fusion for 3D Bounding Box Estimation}, 
      author={Danfei Xu and Dragomir Anguelov and Ashesh Jain},
      year={2018},
      eprint={1711.10871},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@INPROCEEDINGS{kim2019anadvancedob,
  author={Kim, Jinsoo and Kim, Jongwon and Cho, Jeongho},
  booktitle={2019 13th International Conference on Signal Processing and Communication Systems (ICSPCS)}, 
  title={An advanced object classification strategy using YOLO through camera and LiDAR sensor fusion}, 
  year={2019},
  volume={},
  number={},
  pages={1-5},
  keywords={Object detection;Cameras;Laser radar;Feature extraction;Reflectivity;Sensor fusion;Three-dimensional displays;YOLO;real-time;object detection;sensor fusion;LIDAR},
  doi={10.1109/ICSPCS47537.2019.9008742}}


@misc{chen2016semanticimagesegmentationdeep,
      title={Semantic Image Segmentation with Deep Convolutional Nets and Fully Connected CRFs}, 
      author={Liang-Chieh Chen and George Papandreou and Iasonas Kokkinos and Kevin Murphy and Alan L. Yuille},
      year={2016},
      eprint={1412.7062},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@article{Wei2021DecisionLevelDF,
  title={Decision-Level Data Fusion in Quality Control and Predictive Maintenance},
  author={Yupeng Wei and Dazhong Wu and Janis P. Terpenny},
  journal={IEEE Transactions on Automation Science and Engineering},
  year={2021},
  volume={18},
  pages={184-194},
}

@article{kong2025autovit,
  title={AutoViT: Achieving Real-Time Vision Transformers on Mobile via Latency-aware Coarse-to-Fine Search},
  author={Kong, Zhenglun and Xu, Dongkuan and Li, Zhengang and Dong, Peiyan and Tang, Hao and Wang, Yanzhi and Mukherjee, Subhabrata},
  journal={Springer IJCV},
  pages={1--17},
  year={2025}
}

@article{ding2022looking,
  title={Looking outside the window: Wide-context transformer for the semantic segmentation of high-resolution remote sensing images},
  author={Ding, Lei and Lin, Dong and Lin, Shaofu and Zhang, Jing and Cui, Xiaojie and Wang, Yuebin and Tang, Hao and Bruzzone, Lorenzo},
  journal={IEEE TGRS},
  volume={60},
  pages={1--13},
  year={2022}
}

@inproceedings{tang2019multi,
  title={Multi-channel attention selection gan with cascaded semantic guidance for cross-view image translation},
  author={Tang, Hao and Xu, Dan and Sebe, Nicu and Wang, Yanzhi and Corso, Jason J and Yan, Yan},
  booktitle={CVPR},
  year={2019}
}

@article{tang2024graph,
  title={Graph transformer GANs with graph masked modeling for architectural layout generation},
  author={Tang, Hao and Shao, Ling and Sebe, Nicu and Van Gool, Luc},
  journal={IEEE TPAMI},
  volume={46},
  number={6},
  pages={4298--4313},
  year={2024}
}

@article{Steinbaeck2018DesignOA,
  title={Design of a Low-Level Radar and Time-of-Flight Sensor Fusion Framework},
  author={Josef Steinbaeck and Christian Steger and Gerald Holweg and Norbert Druml},
  journal={2018 21st Euromicro Conference on Digital System Design (DSD)},
  year={2018},
  pages={268-275},
}

@article{Scalzo2008FeatureFH,
  title={Feature Fusion Hierarchies for gender classification},
  author={F. Scalzo and George Bebis and Mircea Nicolescu and Leandro A. Loss and A. Tavakkoli},
  journal={2008 19th International Conference on Pattern Recognition},
  year={2008},
  pages={1-4},
}

@article{Li2020HierarchicalFF,
  title={Hierarchical Feature Fusion Network for Salient Object Detection},
  author={Xuelong Li and Dawei Song and Yongsheng Dong},
  journal={IEEE Transactions on Image Processing},
  year={2020},
  volume={29},
  pages={9165-9175},
}

@inproceedings{Mai2019DivideCA,
  title={Divide, Conquer and Combine: Hierarchical Feature Fusion Network with Local and Global Perspectives for Multimodal Affective Computing},
  author={Sijie Mai and Haifeng Hu and Songlong Xing},
  booktitle={Annual Meeting of the Association for Computational Linguistics},
  year={2019},
}

@article{Makris2011AHF,
  title={A hierarchical feature fusion framework for adaptive visual tracking},
  author={Alexandros Makris and Dimitrios I. Kosmopoulos and Stavros J. Perantonis and Sergios Theodoridis},
  journal={Image Vis. Comput.},
  year={2011},
  volume={29},
  pages={594-606},
}

@article{Missaoui2010ModelLF,
  title={Model level fusion of edge histogram descriptors and gabor wavelets for landmine detection with ground penetrating radar},
  author={Oualid Missaoui and Hichem Frigui and Paul D. Gader},
  journal={2010 IEEE International Geoscience and Remote Sensing Symposium},
  year={2010},
  pages={3378-3381},
}

@INPROCEEDINGS{Guo2023AMF,
  author={Guo, Chen and Zhang, Li},
  booktitle={2023 7th Asian Conference on Artificial Intelligence Technology (ACAIT)}, 
  title={A Model-Level Fusion-Based Multi-Modal Object Detection and Recognition Method}, 
  year={2023},
  volume={},
  number={},
  pages={34-38},
  keywords={Image recognition;Stacking;Object detection;Speech recognition;Data models;Robustness;Speech processing;multi-modal object detection;deep learning;model-level fusion;attention mechanisms},
  doi={10.1109/ACAIT60137.2023.10528389}}


@article{Jaiswal2015LearningTC,
  title={Learning to combine local models for facial Action Unit detection},
  author={Shashank Jaiswal and Brais Mart{\'i}nez and Michel F. Valstar},
  journal={2015 11th IEEE International Conference and Workshops on Automatic Face and Gesture Recognition (FG)},
  year={2015},
  volume={06},
  pages={1-6},
}

@article{Allaire2012FusingIF,
  title={Fusing information from multifidelity computer models of physical systems},
  author={Douglas L. Allaire and Karen E. Willcox},
  journal={2012 15th International Conference on Information Fusion},
  year={2012},
  pages={2458-2465},
}

@article{Wang2011KernelbasedDF,
  title={Kernel-based data fusion improves the drug-protein interaction prediction},
  author={Yong-Cui Wang and Chunhua Zhang and Naiyang Deng and Yong Wang},
  journal={Computational biology and chemistry},
  year={2011},
  volume={35 6},
  pages={
          353-62
        },
}

@article{Dov2016KernelBasedSF,
  title={Kernel-Based Sensor Fusion With Application to Audio-Visual Voice Activity Detection},
  author={David Dov and Ronen Talmon and Israel Cohen},
  journal={IEEE Transactions on Signal Processing},
  year={2016},
  volume={64},
  pages={6406-6416},
}

@inproceedings{Ayache2007ClassifierFF,
  title={Classifier Fusion for SVM-Based Multimedia Semantic Indexing},
  author={S. Ayache and Georges Qu{\'e}not and J{\'e}r{\^o}me Gensel},
  booktitle={European Conference on Information Retrieval},
  year={2007},
}

@article{Arany2012MultiaspectCF,
  title={Multi-aspect candidates for repositioning: data fusion methods using heterogeneous information sources.},
  author={Adam Arany and Bence Bolg{\'a}r and Bal{\'a}zs Balogh and P{\'e}ter Antal and P{\'e}ter M{\'a}tyus},
  journal={Current medicinal chemistry},
  year={2012},
  volume={20 1},
  pages={
          95-107
        },
}

@inproceedings{Muoz2008FunctionalLO,
  title={Functional Learning of Kernels for Information Fusion Purposes},
  author={Alberto Mu{\~n}oz and Javier Gonz{\'a}lez},
  booktitle={Iberoamerican Congress on Pattern Recognition},
  year={2008},
}

@article{Wang2012KernelCF,
  title={Kernel Cross-Modal Factor Analysis for Information Fusion With Application to Bimodal Emotion Recognition},
  author={Yongjin Wang and Ling Guan and Anastasios N. Venetsanopoulos},
  journal={IEEE Transactions on Multimedia},
  year={2012},
  volume={14},
  pages={597-607},
}

@inproceedings{etin2006DistributedFI,
  title={Distributed fusion in sensor networks: a graphical models perspective},
  author={M{\"u}jdat Çetin and Lei Chen and John W. Fisher III and Alexander T. Ihler and Randolph L. Moses and Martin J. Wainwright and Alan S. Willsky},
  year={2006},
}

@article{Chen2020HGMFHG,
  title={HGMF: Heterogeneous Graph-based Fusion for Multimodal Data with Incompleteness},
  author={Jiayi Chen and Aidong Zhang},
  journal={Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery \& Data Mining},
  year={2020},
}

@inproceedings{Blasch2014VisualizationOG,
  title={Visualization of graphical information fusion results},
  author={Erik Blasch and Georgiy M. Levchuk and Gennady Staskevich and Dustin Burke and Alex Aved},
  booktitle={Defense + Security Symposium},
  year={2014},
}

@inproceedings{Tong2015NonlinearGF,
  title={Nonlinear Graph Fusion for Multi-modal Classification of Alzheimer's Disease},
  author={Tong Tong and Katherine R. Gray and Qinquan Gao and Liang Chen and Daniel Rueckert},
  booktitle={Machine Learning for Multimodal Interaction},
  year={2015},
}

@article{Tong2017MultimodalCO,
  title={Multi-modal classification of Alzheimer's disease using nonlinear graph fusion},
  author={Tong Tong and Katherine R. Gray and Qinquan Gao and Liang Chen and Daniel Rueckert},
  journal={Pattern Recognit.},
  year={2017},
  volume={63},
  pages={171-181},
}

@misc{mai2019modalitytm,
      title={Modality to Modality Translation: An Adversarial Representation Learning and Graph Fusion Network for Multimodal Fusion}, 
      author={Sijie Mai and Haifeng Hu and Songlong Xing},
      year={2020},
      eprint={1911.07848},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{kim_vilt_2021,
  title = {{ViLT}: Vision-and-Language Transformer Without Convolution or Region Supervision},
  doi = {10.48550/arXiv.2102.03334},
  abstract = {Vision-and-Language Pre-training (VLP) has improved performance on various joint vision-and-language downstream tasks...},
  author = {Kim, Wonjae and Son, Bokyung and Kim, Ildoo},
  date = {2021-06-10},
}


@inproceedings{andrew_deep_nodate,
	title = {Deep Canonical Correlation Analysis},
	author = {Andrew, Galen and Arora, Raman and Bilmes, Jeff and Livescu, Karen},
  booktitle = {{Proceedings of the 30th International Conference on Machine Learning}},
  year = {2013},
}


@inproceedings{li_align_2021,
  title = {Align before Fuse: Vision and Language Representation Learning with Momentum Distillation},
  booktitle = {Advances in Neural Information Processing Systems},
  volume = {34},
  pages = {9694--9705},
  author = {Li, Junnan and Selvaraju, Ramprasaath and Gotmare, Akhilesh and Joty, Shafiq and Xiong, Caiming and Hoi, Steven Chu Hong},
  year = {2021},
}

@misc{bao_vlmo_nodate,
      title={VLMo: Unified Vision-Language Pre-Training with Mixture-of-Modality-Experts}, 
      author={Hangbo Bao and Wenhui Wang and Li Dong and Qiang Liu and Owais Khan Mohammed and Kriti Aggarwal and Subhojit Som and Furu Wei},
      year={2022},
      eprint={2111.02358},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@inproceedings{li_blip_2022,
  title = {{BLIP}: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation},
  eventtitle = {International Conference on Machine Learning},
  pages = {12888--12900},
  booktitle = {Proceedings of the 39th International Conference on Machine Learning},
  publisher = {PMLR},
  author = {Li, Junnan and Li, Dongxu and Xiong, Caiming and Hoi, Steven},
  year = {2022},
}

@misc{yu_coca_2022,
  title = {{CoCa}: Contrastive Captioners are Image-Text Foundation Models},
  shorttitle = {{CoCa}},
  number = {{arXiv}:2205.01917},
  booktitle = {arXiv},
  publisher = {arXiv},
  author = {Yu, Jiahui and Wang, Zirui and Vasudevan, Vijay and Yeung, Legg and Seyedhosseini, Mojtaba and Wu, Yonghui},
  date = {2022-06-14},
  eprinttype = {arxiv},
  eprint = {2205.01917},
}

@misc{wang_image_2022,
  title = {Image as a Foreign Language: {BEiT} Pretraining for All Vision and Vision-Language Tasks},
  shorttitle = {Image as a Foreign Language},
  number = {{arXiv}:2208.10442},
  booktitle = {arXiv},
  publisher = {arXiv},
  author = {Wang, Wenhui and Bao, Hangbo and Dong, Li and Bjorck, Johan and Peng, Zhiliang and Liu, Qiang and Aggarwal, Kriti and Mohammed, Owais Khan and Singhal, Saksham and Som, Subhojit and Wei, Furu},
  date = {2022-08-31},
  eprinttype = {arxiv},
  eprint = {2208.10442},
}

@misc{bao2022beit1,
      title={BEiT: BERT Pre-Training of Image Transformers}, 
      author={Hangbo Bao and Li Dong and Songhao Piao and Furu Wei},
      year={2022},
      eprint={2106.08254},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{bao2022vlbeit,
      title={VL-BEiT: Generative Vision-Language Pretraining}, 
      author={Hangbo Bao and Wenhui Wang and Li Dong and Furu Wei},
      year={2022},
      eprint={2206.01127},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{peng2022beitv2,
      title={BEiT v2: Masked Image Modeling with Vector-Quantized Visual Tokenizers}, 
      author={Zhiliang Peng and Li Dong and Hangbo Bao and Qixiang Ye and Furu Wei},
      year={2022},
      eprint={2208.06366},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{dosovitskiy_vit_2021,
  title = {An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},
  shorttitle = {An Image is Worth 16x16 Words},
  abstract = {While the Transformer architecture has become the de-facto standard for natural language processing tasks...},
  number = {{arXiv}:2010.11929},
  booktitle = {arXiv},
  publisher = {arXiv},
  author = {Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and Uszkoreit, Jakob and Houlsby, Neil},
  date = {2021-06-03},
  eprinttype = {arxiv},
  eprint = {2010.11929},
}


@article{ren_faster_2017,
	title = {Faster R-{CNN}: Towards Real-Time Object Detection with Region Proposal Networks},
	volume = {39},
	issn = {1939-3539},
	shorttitle = {Faster R-{CNN}},
	pages = {1137--1149},
	number = {6},
	journal = {{IEEE} Transactions on Pattern Analysis and Machine Intelligence},
	author = {Ren, Shaoqing and He, Kaiming and Girshick, Ross and Sun, Jian},
	date = {2017-06},
year = {2016}
}

@inproceedings{wang_vqa_2023,
  title={Multi-granularity Text Representation and Transformer-Based Fusion Method for Visual Question Answering},
  author={Wang, Xingang and Liu, Xiaoyu and Li, Xiaomin and Cui, Jin'an},
  booktitle={2023 26th International Conference on Computer Supported Cooperative Work in Design (CSCWD)},
  pages={965-971},
  year={2023},
  doi={10.1109/CSCWD57460.2023.10152629},
}

@misc{chen2020uniter,
      title={UNITER: UNiversal Image-TExt Representation Learning}, 
      author={Yen-Chun Chen and Linjie Li and Licheng Yu and Ahmed El Kholy and Faisal Ahmed and Zhe Gan and Yu Cheng and Jingjing Liu},
      year={2020},
      eprint={1909.11740},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@article{kruskal1983anoverview,
author = {Kruskal, Joseph B.},
title = {An Overview of Sequence Comparison: Time Warps, String Edits, and Macromolecules},
year = {1983},
issue_date = {Apr 1983},
publisher = {Society for Industrial and Applied Mathematics},
address = {USA},
volume = {25},
number = {2},
issn = {0036-1445},
journal = {SIAM Rev.},
month = apr,
pages = {201–237},
numpages = {37}
}


@InProceedings{Vosylius2023fewshot,
  title = 	 {Few-Shot In-Context Imitation Learning via Implicit Graph Alignment},
  author =       {Vosylius, Vitalis and Johns, Edward},
  booktitle = 	 {Proceedings of The 7th Conference on Robot Learning},
  pages = 	 {3194--3213},
  year = 	 {2023},
  volume = 	 {229},
  series = 	 {Proceedings of Machine Learning Research}
}

@misc{hu2022learningimplicitfeaturealignment,
      title={Learning Implicit Feature Alignment Function for Semantic Segmentation}, 
      author={Hanzhe Hu and Yinbo Chen and Jiarui Xu and Shubhankar Borse and Hong Cai and Fatih Porikli and Xiaolong Wang},
      year={2022},
      eprint={2206.08655},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@article{tang2021graph,
  title={Graph-based Multimodal Sequential Embedding for Sign Language Translation},
  author={Tang, Shengeng and Guo, Dixin and Hong, Rui and Wang, Min},
  journal={IEEE Transactions on Multimedia},
  year={2021},
  volume={23},
  pages={1056--1067},
  publisher={IEEE}
}

@article{yang2023macsa,
  title={Macsa: A Multimodal Aspect-Category Sentiment Analysis Dataset with Multimodal Fine-grained Aligned Annotations},
  author={Yang, Haoyan and Wu, Yifan and Si, Zhenyu and Zhao, Yijun and Liu, Jinfeng and Qin, Bing},
  journal={Multimedia Tools and Applications},
  year={2023},
  volume={82},
  pages={3839--3858},
  publisher={Springer}
}

@article{song2023scene,
  title={Scene-Driven Multimodal Knowledge Graph Construction for Embodied AI},
  author={Song, Yiming and Li, Zhen and Song, Wei},
  journal={IEEE Transactions on Robotics},
  year={2023},
  volume={39},
  number={1},
  pages={45--60},
  publisher={IEEE}
}

@article{zhang2021token,
  title={A Token-wise Graph-based Framework for Multimodal Named Entity Recognition},
  author={Zhang, Zhiwei and Mai, Wenyu and Xiong, Heng and Wu, Cheng},
  journal={IEEE Transactions on Knowledge and Data Engineering},
  year={2021},
  volume={33},
  number={10},
  pages={3121--3134},
  publisher={IEEE}
}

@article{xiong2020scene,
  title={Scene Graph-based Semantic Alignment for Multimodal Tasks},
  author={Xiong, Wei and Zhang, Yifan and Li, Wei},
  journal={IEEE Transactions on Multimedia},
  year={2020},
  volume={22},
  number={5},
  pages={1231--1243},
  publisher={IEEE}
}


@article{Nandy2022MappingBiomolecular,
  author    = {Nandy, A.},
  title     = {Mapping Biomolecular Sequences: Graphical Representations - Their Origins, Applications and Future Prospects},
  journal   = {Comb Chem High Throughput Screen},
  year      = {2022},
  volume    = {25},
  number    = {3},
  pages     = {354--364},
  doi       = {10.2174/1386207324666210510164743},
  pmid      = {33970841}
}

@article{Kolar2012GraphAlignment,
  author    = {Kolář, M. and Meier, J. and Mustonen, V. and others},
  title     = {GraphAlignment: Bayesian pairwise alignment of biological networks},
  journal   = {BMC Syst Biol},
  year      = {2012},
  volume    = {6},
  pages     = {144}
}



@incollection{2007DynamicTimeWarping,
  author    = {Unknown},
  title     = {Dynamic Time Warping},
  booktitle = {Information Retrieval for Music and Motion},
  publisher = {Springer},
  year      = {2007},
  address   = {Berlin, Heidelberg}
}



@misc{zhang2021vinvl,
      title={VinVL: Revisiting Visual Representations in Vision-Language Models}, 
      author={Pengchuan Zhang and Xiujun Li and Xiaowei Hu and Jianwei Yang and Lei Zhang and Lijuan Wang and Yejin Choi and Jianfeng Gao},
      year={2021},
      eprint={2101.00529},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{broedermann2024condition,
      title={Condition-Aware Multimodal Fusion for Robust Semantic Perception of Driving Scenes}, 
      author={Tim Broedermann and Christos Sakaridis and Yuqian Fu and Luc Van Gool},
      year={2024},
      eprint={2410.10791},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{lin2024vila,
      title={VILA: On Pre-training for Visual Language Models}, 
      author={Ji Lin and Hongxu Yin and Wei Ping and Yao Lu and Pavlo Molchanov and Andrew Tao and Huizi Mao and Jan Kautz and Mohammad Shoeybi and Song Han},
      year={2024},
      eprint={2312.07533},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@inproceedings{sarker_retrieval_2022,
  title={Deep Learning-Based Multimodal Image Retrieval Combining Image and Text},
  author={Sarker, Md Imran and Milanova, M},
  booktitle={2022 International Conference on Computational Science and Computational Intelligence (CSCI)},
  pages={1543-1546},
  year={2022}
}

@misc{li2020oscar,
      title={Oscar: Object-Semantics Aligned Pre-training for Vision-Language Tasks}, 
      author={Xiujun Li and Xi Yin and Chunyuan Li and Pengchuan Zhang and Xiaowei Hu and Lei Zhang and Lijuan Wang and Houdong Hu and Li Dong and Furu Wei and Yejin Choi and Jianfeng Gao},
      year={2020},
      eprint={2004.06165},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{devlin2019bertpretraining,
      title={BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding}, 
      author={Jacob Devlin and Ming-Wei Chang and Kenton Lee and Kristina Toutanova},
      year={2019},
      eprint={1810.04805},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{thai_medvqa_2023,
  title={UIT-Saviors at MEDVQA-GI 2023: Improving Multimodal Learning with Image Enhancement for Gastrointestinal Visual Question Answering},
  author={Thai, T. M. and Vo, A. T. and Tieu, Hao K. and Bui, Linh and Nguyen, T.},
  year={2023}
}

@article{siebert_remote_vqa_2022,
  title={Multi-modal fusion transformer for visual question answering in remote sensing},
  author={Siebert, Tim and Clasen, Kai Norman and Ravanbakhsh, Mahdyar and Demir, B},
  journal={SPIE},
  volume={12267},
  pages={122670L-9},
  year={2022}
}

@article{tang_matr_2022,
  title={MATR: Multimodal Medical Image Fusion via Multiscale Adaptive Transformer},
  author={Tang, Wei and He, Fazhi and Liu, Yefeng and Duan, Ying},
  journal={IEEE Transactions on Image Processing},
  volume={31},
  pages={5134-5149},
  year={2022}
}

@article{zhang_multimodal_intelligence_2020,
  title={Multimodal Intelligence: Representation Learning, Information Fusion, and Applications},
  author={Zhang, Chao and Yang, Zichao and He, Xiaodong and Deng, Li},
  journal={IEEE Journal of Selected Topics in Signal Processing},
  volume={14},
  pages={478-493},
  year={2020}
}

@article{zhang_deep_fusion_2021,
  title={Multimodal deep fusion for image question answering},
  author={Zhang, Weifeng and Yu, Jing and Wang, Yuxia and Wang, Wei},
  journal={Knowledge-Based Systems},
  volume={212},
  pages={106639},
  year={2021}
}

@inproceedings{ak_transformer_fusion_2023,
  title={Leveraging Efficient Training and Feature Fusion in Transformers for Multimodal Classification},
  author={Ak, Kenan E. and Lee, Gary and Xu, Yan and Shen, Mingwei},
  booktitle={2023 IEEE International Conference on Image Processing (ICIP)},
  pages={1420-1424},
  year={2023}
}

@article{srivastava_boltzmann_2012,
  title={Multimodal learning with deep Boltzmann machines},
  author={Srivastava, Nitish and Salakhutdinov, Ruslan},
  journal={Journal of Machine Learning Research},
  volume={15},
  pages={2949-2980},
  year={2012}
}

@article{nassar2017multimodal,
  title={Multimodal Network Alignment},
  author={Nassar, Huda and Gleich, David},
  journal={ArXiv},
  year={2017}
}

@inproceedings{akhmerov2019alignment,
  title={Research of spatial alignment techniques for multimodal image fusion},
  author={Akhmerov, A. and Vasilev, A. and Vasileva, A.V.},
  booktitle={Proceedings of the SPIE},
  year={2019},
  doi={10.1117/12.2526030},
  volume={11059},
  pages={1105916 - 1105916-9},
}

@article{qin2023telescopic,
  title={Alternative Telescopic Displacement: An Efficient Multimodal Alignment Method},
  author={Qin, Jiahao and Xu, Yitao and Luo, Zihong and Liu, Chengzhi and Lu, Zong and Zhang, Xiaojun},
  journal={ArXiv},
  year={2023}
}

@article{jiang2017mapping,
  title={Multimodal Image Alignment via Linear Mapping between Feature Modalities},
  author={Jiang, Yanyun and Zheng, Yuanjie and Hou, Sujuan and Chang, Yuchou and Gee, J.},
  journal={Journal of Healthcare Engineering},
  year={2017}
}

@inproceedings{ma2023sinkhorn,
  title={Att-Sinkhorn: Multimodal Alignment with Sinkhorn-based Deep Attention Architecture},
  author={Ma, Qianxia and Zhang, Ming and Tang, Yan and Huang, Zhen},
  booktitle={2023 28th International Conference on Automation and Computing (ICAC)},
  year={2023}
}

@article{dai2020multimodal,
  title={Multimodal MRI Synthesis Using Unified Generative Adversarial Networks},
  author={Dai, X. and Lei, Y. and Fu, Y. and Curran, W. and Liu, T. and Mao, H. and Yang, Xiaofeng},
  journal={Medical Physics},
  year={2020}
}

@inproceedings{pumarola2020cflow,
  title={C-Flow: Conditional Generative Flow Models for Images and 3D Point Clouds},
  author={Pumarola, Albert and Popov, Stefan and Moreno-Noguer, Francesc and Ferrari, Vittorio},
  booktitle={2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  pages={7946--7955},
  year={2020},
  doi={10.1109/cvpr42600.2020.00797},
}


@misc{wu2019multimodal,
      title={Multimodal Generative Models for Compositional Representation Learning}, 
      author={Mike Wu and Noah Goodman},
      year={2019},
      eprint={1912.05075},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{yilmaz2015multimodal,
  title={Multimodal factor analysis},
  author={Yilmaz, Y. and Hero, A.},
  journal={2015 IEEE 25th International Workshop on Machine Learning for Signal Processing (MLSP)},
  pages={1--6},
  year={2015},
  doi={10.1109/MLSP.2015.7324340},
}

@inproceedings{karpathy2015deep,
  title={Deep visual-semantic alignments for generating image descriptions},
  author={Karpathy, Andrej and Li, Fei-Fei},
  booktitle={2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  pages={3128--3137},
  year={2015},
  doi={10.1109/CVPR.2015.7298932},
}

@article{lee2022imagetext,
  title={Unconditional Image-Text Pair Generation with Multimodal Cross Quantizer},
  author={Lee, Hyungyu and Park, Sungjin and Choi, E.},
  journal={ArXiv},
  year={2022},
  doi={10.48550/arXiv.2204.07537.}
}

@misc{dourado2019multimodal,
      title={Multimodal Prediction based on Graph Representations}, 
      author={Icaro Cavalcante Dourado and Salvatore Tabbone and Ricardo da Silva Torres},
      year={2020},
      eprint={1912.10314},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{duque2022diffusion,
      title={Diffusion Transport Alignment}, 
      author={Andres F. Duque and Guy Wolf and Kevin R. Moon},
      year={2022},
      eprint={2206.07305},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

@misc{peng2023kosmos2,
      title={Kosmos-2: Grounding Multimodal Large Language Models to the World}, 
      author={Zhiliang Peng and Wenhui Wang and Li Dong and Yaru Hao and Shaohan Huang and Shuming Ma and Furu Wei},
      year={2023},
      eprint={2306.14824},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{driess2023palme,
      title={PaLM-E: An Embodied Multimodal Language Model}, 
      author={Danny Driess and Fei Xia and Mehdi S. M. Sajjadi and Corey Lynch and Aakanksha Chowdhery and Brian Ichter and Ayzaan Wahid and Jonathan Tompson and Quan Vuong and Tianhe Yu and Wenlong Huang and Yevgen Chebotar and Pierre Sermanet and Daniel Duckworth and Sergey Levine and Vincent Vanhoucke and Karol Hausman and Marc Toussaint and Klaus Greff and Andy Zeng and Igor Mordatch and Pete Florence},
      year={2023},
      eprint={2303.03378},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{zang2023contextdet,
      title={Contextual Object Detection with Multimodal Large Language Models}, 
      author={Yuhang Zang and Wei Li and Jun Han and Kaiyang Zhou and Chen Change Loy},
      year={2024},
      eprint={2305.18279},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{li2023seedbench2,
      title={SEED-Bench-2: Benchmarking Multimodal Large Language Models}, 
      author={Bohao Li and Yuying Ge and Yixiao Ge and Guangzhi Wang and Rui Wang and Ruimao Zhang and Ying Shan},
      year={2023},
      eprint={2311.17092},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{chen2023xllm,
      title={X-LLM: Bootstrapping Advanced Large Language Models by Treating Multi-Modalities as Foreign Languages}, 
      author={Feilong Chen and Minglun Han and Haozhi Zhao and Qingyang Zhang and Jing Shi and Shuang Xu and Bo Xu},
      year={2023},
      eprint={2305.04160},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{kelly2023unifiedvisiongpt,
      title={UnifiedVisionGPT: Streamlining Vision-Oriented AI through Generalized Multimodal Framework}, 
      author={Chris Kelly and Luhui Hu and Cindy Yang and Yu Tian and Deshun Yang and Bang Yang and Zaoshan Huang and Zihao Li and Yuexian Zou},
      year={2023},
      eprint={2311.10125},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{fu2023mme,
      title={MME: A Comprehensive Evaluation Benchmark for Multimodal Large Language Models}, 
      author={Chaoyou Fu and Peixian Chen and Yunhang Shen and Yulei Qin and Mengdan Zhang and Xu Lin and Jinrui Yang and Xiawu Zheng and Ke Li and Xing Sun and Yunsheng Wu and Rongrong Ji},
      year={2024},
      eprint={2306.13394},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{wu2023nextgpt,
      title={NExT-GPT: Any-to-Any Multimodal LLM}, 
      author={Shengqiong Wu and Hao Fei and Leigang Qu and Wei Ji and Tat-Seng Chua},
      year={2024},
      eprint={2309.05519},
      archivePrefix={arXiv},
      primaryClass={cs.AI}
}

@misc{wang2023snare,
      title={Can Linguistic Knowledge Improve Multimodal Alignment in Vision-Language Pretraining?}, 
      author={Fei Wang and Liang Ding and Jun Rao and Ye Liu and Li Shen and Changxing Ding},
      year={2023},
      eprint={2308.12898},
      archivePrefix={arXiv},
      primaryClass={cs.MM}
}

@misc{changpinyo2021conceptual12mpushingwebscale,
      title={Conceptual 12M: Pushing Web-Scale Image-Text Pre-Training To Recognize Long-Tail Visual Concepts}, 
      author={Soravit Changpinyo and Piyush Sharma and Nan Ding and Radu Soricut},
      year={2021},
      eprint={2102.08981},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{zeng2021xvlm,
      title={Multi-Grained Vision Language Pre-Training: Aligning Texts with Visual Concepts}, 
      author={Yan Zeng and Xinsong Zhang and Hang Li},
      year={2022},
      eprint={2111.08276},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{shukor2022vicha,
      title={Efficient Vision-Language Pretraining with Visual Concepts and Hierarchical Alignment}, 
      author={Mustafa Shukor and Guillaume Couairon and Matthieu Cord},
      year={2022},
      eprint={2208.13628},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{chen2023spiking,
      title={Artificial-Spiking Hierarchical Networks for Vision-Language Representation Learning}, 
      author={Yeming Chen and Siyu Zhang and Yaoru Sun and Weijian Liang and Haoran Wang},
      year={2023},
      eprint={2308.09455},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@inproceedings{chen2023contrastive,
  author={Chen, Yuxiao and Yuan, Jianbo and Tian, Yu and Geng, Shijie and Li, Xinyu and Zhou, Ding and Metaxas, Dimitris N. and Yang, Hongxia},
  booktitle={2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={Revisiting Multimodal Representation in Contrastive Learning: From Patch and Token Embeddings to Finite Discrete Tokens}, 
  year={2023},
  pages={15095-15104},
  keywords={Degradation;Visualization;Image coding;Grounding;Source coding;Semantics;Distance measurement;Multi-modal learning},
  doi={10.1109/CVPR52729.2023.01449}}

@misc{nguyen2023improving,
      title={Improving Multimodal Datasets with Image Captioning}, 
      author={Thao Nguyen and Samir Yitzhak Gadre and Gabriel Ilharco and Sewoong Oh and Ludwig Schmidt},
      year={2023},
      eprint={2307.10350},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{yu2023capsfusion,
      title={CapsFusion: Rethinking Image-Text Data at Scale}, 
      author={Qiying Yu and Quan Sun and Xiaosong Zhang and Yufeng Cui and Fan Zhang and Yue Cao and Xinlong Wang and Jingjing Liu},
      year={2024},
      eprint={2310.20550},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{gadre2023datacomp,
      title={DataComp: In search of the next generation of multimodal datasets}, 
      author={Samir Yitzhak Gadre and Gabriel Ilharco and Alex Fang and Jonathan Hayase and Georgios Smyrnis and Thao Nguyen and Ryan Marten and Mitchell Wortsman and Dhruba Ghosh and Jieyu Zhang and Eyal Orgad and Rahim Entezari and Giannis Daras and Sarah Pratt and Vivek Ramanujan and Yonatan Bitton and Kalyani Marathe and Stephen Mussmann and Richard Vencu and Mehdi Cherti and Ranjay Krishna and Pang Wei Koh and Olga Saukh and Alexander Ratner and Shuran Song and Hannaneh Hajishirzi and Ali Farhadi and Romain Beaumont and Sewoong Oh and Alex Dimakis and Jenia Jitsev and Yair Carmon and Vaishaal Shankar and Ludwig Schmidt},
      year={2023},
      eprint={2304.14108},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{kakaobrain2022coyo-700m,
  title         = {COYO-700M: Image-Text Pair Dataset},
  author        = {Byeon, Minwoo and Park, Beomhee and Kim, Haecheon and Lee, Sungjun and Baek, Woonhyuk and Kim, Saehoon},
  year          = {2022}
}

@misc{schuhmann2022laion5b,
      title={LAION-5B: An open large-scale dataset for training next generation image-text models}, 
      author={Christoph Schuhmann and Romain Beaumont and Richard Vencu and Cade Gordon and Ross Wightman and Mehdi Cherti and Theo Coombes and Aarush Katta and Clayton Mullis and Mitchell Wortsman and Patrick Schramowski and Srivatsa Kundurthy and Katherine Crowson and Ludwig Schmidt and Robert Kaczmarczyk and Jenia Jitsev},
      year={2022},
      eprint={2210.08402},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@inproceedings{NIPS2011_5dd9db5e,
 author = {Ordonez, Vicente and Kulkarni, Girish and Berg, Tamara},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Im2Text: Describing Images Using 1 Million Captioned Photographs},
 volume = {24},
 year = {2011}
}

@inproceedings{NEURIPS2022_6a386d70,
 author = {Liu, Yulong and Zhu, Guibo and Zhu, Bin and Song, Qi and Ge, Guojing and Chen, Haoran and Qiao, GuanHui and Peng, Ru and Wu, Lingxiang and Wang, Jinqiao},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Koyejo and S. Mohamed and A. Agarwal and D. Belgrave and K. Cho and A. Oh},
 pages = {16705--16717},
 publisher = {Curran Associates, Inc.},
 title = {TaiSu: A 166M Large-scale High-Quality Dataset for Chinese Vision-Language Pre-training},
 volume = {35},
 year = {2022}
}

@misc{lin2015microsoftcococommonobjects,
      title={Microsoft COCO: Common Objects in Context}, 
      author={Tsung-Yi Lin and Michael Maire and Serge Belongie and Lubomir Bourdev and Ross Girshick and James Hays and Pietro Perona and Deva Ramanan and C. Lawrence Zitnick and Piotr Dollár},
      year={2015},
      eprint={1405.0312},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{plummer2016flickr30kentitiescollectingregiontophrase,
      title={Flickr30k Entities: Collecting Region-to-Phrase Correspondences for Richer Image-to-Sentence Models}, 
      author={Bryan A. Plummer and Liwei Wang and Chris M. Cervantes and Juan C. Caicedo and Julia Hockenmaier and Svetlana Lazebnik},
      year={2016},
      eprint={1505.04870},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@INPROCEEDINGS{kim2020mulran,
  author={Kim, Giseop and Park, Yeong Sang and Cho, Younghun and Jeong, Jinyong and Kim, Ayoung},
  booktitle={2020 IEEE International Conference on Robotics and Automation (ICRA)}, 
  title={MulRan: Multimodal Range Dataset for Urban Place Recognition}, 
  year={2020},
  volume={},
  number={},
  pages={6246-6253},
  keywords={Laser radar;Radar imaging;Three-dimensional displays;Urban areas;Simultaneous localization and mapping},
}

@misc{krishna2016visualgenomeconnectinglanguage,
      title={Visual Genome: Connecting Language and Vision Using Crowdsourced Dense Image Annotations}, 
      author={Ranjay Krishna and Yuke Zhu and Oliver Groth and Justin Johnson and Kenji Hata and Joshua Kravitz and Stephanie Chen and Yannis Kalantidis and Li-Jia Li and David A. Shamma and Michael S. Bernstein and Fei-Fei Li},
      year={2016},
      eprint={1602.07332},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@article{Thomee_2016,
   title={YFCC100M: the new data in multimedia research},
   volume={59},
   ISSN={1557-7317},
   DOI={10.1145/2812802},
   number={2},
   journal={Communications of the ACM},
   publisher={Association for Computing Machinery (ACM)},
   author={Thomee, Bart and Shamma, David A. and Friedland, Gerald and Elizalde, Benjamin and Ni, Karl and Poland, Douglas and Borth, Damian and Li, Li-Jia},
   year={2016},
   month=jan, pages={64–73} }

@inproceedings{Sharma2018ConceptualCA,
  title={Conceptual Captions: A Cleaned, Hypernymed, Image Alt-text Dataset For Automatic Image Captioning},
  author={Piyush Sharma and Nan Ding and Sebastian Goodman and Radu Soricut},
  booktitle={Annual Meeting of the Association for Computational Linguistics},
  year={2018},
}

@inproceedings{srinivasan2021wit, series={SIGIR ’21},
   title={WIT: Wikipedia-based Image Text Dataset for Multimodal Multilingual Machine Learning},
   booktitle={Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval},
   publisher={ACM},
   author={Srinivasan, Krishna and Raman, Karthik and Chen, Jiecao and Bendersky, Michael and Najork, Marc},
   year={2021},
   month=jul, collection={SIGIR ’21} }

@misc{wang2023datareduction,
      title={Too Large; Data Reduction for Vision-Language Pre-Training}, 
      author={Alex Jinpeng Wang and Kevin Qinghong Lin and David Junhao Zhang and Stan Weixian Lei and Mike Zheng Shou},
      year={2023},
      eprint={2305.20087},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{shen2023scaling,
      title={Scaling Vision-Language Models with Sparse Mixture of Experts}, 
      author={Sheng Shen and Zhewei Yao and Chunyuan Li and Trevor Darrell and Kurt Keutzer and Yuxiong He},
      year={2023},
      eprint={2303.07226},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@inproceedings{wang2022tokenfusion,
  author={Morelli, Veronica Grazia and Barbato, Mirko Paolo and Piccoli, Flavio and Napoletano, Paolo},
  booktitle={2023 13th Workshop on Hyperspectral Imaging and Signal Processing: Evolution in Remote Sensing (WHISPERS)}, 
  title={Multimodal Fusion Methods with Vision Transformers for Remote Sensing Semantic Segmentation}, 
  year={2023},
  volume={},
  number={},
  pages={1-5},
  keywords={Semantic segmentation;Conferences;Data integration;Signal processing;Transformers;Remote sensing;Hyperspectral imaging;Remote sensing;Semantic Segmentation;Multimodal fusion;Vision Transformers},
  doi={10.1109/WHISPERS61460.2023.10430788}}

@misc{nagrani2021bottlenecks,
      title={Attention Bottlenecks for Multimodal Fusion}, 
      author={Arsha Nagrani and Shan Yang and Anurag Arnab and Aren Jansen and Cordelia Schmid and Chen Sun},
      year={2022},
      eprint={2107.00135},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@inproceedings{li2023pmf,
  author={Li, Yaowei and Quan, Ruijie and Zhu, Linchao and Yang, Yi},
  booktitle={2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={Efficient Multimodal Fusion via Interactive Prompting}, 
  year={2023},
  volume={},
  number={},
  pages={2604-2613},
  keywords={Training;Computer vision;Computational modeling;Memory management;Transformers;Market research;Natural language processing;Multi-modal learning},
  doi={10.1109/CVPR52729.2023.00256}}


@inproceedings{ma_learning_2024,
    title = {Learning Modality Knowledge Alignment for Cross-Modality Transfer},
    author = {Ma, Wenxuan and Li, Shuang and Cai, Lincan and Kang, Jingxuan},
    booktitle = 	 {Proceedings of the 41st International Conference on Machine Learning},
    pages = 	 {33777--33793},
    year = 	 {2024},
    volume = 	 {235},
    series = 	 {Proceedings of Machine Learning Research},
    month = 	 {21--27 Jul},
    publisher =    {PMLR},
}

@misc{nukrai2022capdec,
  doi = {10.48448/N7SQ-P557},
  author = {{Association for Computational Linguistics 2022} and Globerson, Amir and Mokady, Ron and Nukrai, David},
  keywords = {Natural Language Processing},
  title = {Text-Only Training for Image Captioning using Noise-Injected CLIP},
  publisher = {Underline Science Inc.},
  year = {2022}
}

@misc{chen2023minigptv2largelanguagemodel,
      title={MiniGPT-v2: large language model as a unified interface for vision-language multi-task learning}, 
      author={Jun Chen and Deyao Zhu and Xiaoqian Shen and Xiang Li and Zechun Liu and Pengchuan Zhang and Raghuraman Krishnamoorthi and Vikas Chandra and Yunyang Xiong and Mohamed Elhoseiny},
      year={2023},
      eprint={2310.09478},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{zhang_vtclip_2021,
      title={VT-CLIP: Enhancing Vision-Language Models with Visual-guided Texts}, 
      author={Longtian Qiu and Renrui Zhang and Ziyu Guo and Ziyao Zeng and Zilu Guo and Yafeng Li and Guangnan Zhang},
      year={2023},
      eprint={2112.02399},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{varma_villa_2023,
      title={ViLLA: Fine-Grained Vision-Language Representation Learning from Real-World Data}, 
      author={Maya Varma and Jean-Benoit Delbrouck and Sarah Hooper and Akshay Chaudhari and Curtis Langlotz},
      year={2023},
      eprint={2308.11194},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{jia2021ALIGN,
      title={Scaling Up Visual and Vision-Language Representation Learning With Noisy Text Supervision}, 
      author={Chao Jia and Yinfei Yang and Ye Xia and Yi-Ting Chen and Zarana Parekh and Hieu Pham and Quoc V. Le and Yunhsuan Sung and Zhen Li and Tom Duerig},
      year={2021},
      eprint={2102.05918},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{wang2022simvlm,
      title={SimVLM: Simple Visual Language Model Pretraining with Weak Supervision}, 
      author={Zirui Wang and Jiahui Yu and Adams Wei Yu and Zihang Dai and Yulia Tsvetkov and Yuan Cao},
      year={2022},
      eprint={2108.10904},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{yao2021FILIP,
      title={FILIP: Fine-grained Interactive Language-Image Pre-Training}, 
      author={Lewei Yao and Runhui Huang and Lu Hou and Guansong Lu and Minzhe Niu and Hang Xu and Xiaodan Liang and Zhenguo Li and Xin Jiang and Chunjing Xu},
      year={2021},
      eprint={2111.07783},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@article{zhang_rs5m_2023,
   title={RS5M and GeoRSCLIP: A Large-Scale Vision- Language Dataset and a Large Vision-Language Model for Remote Sensing},
   volume={62},
   ISSN={1558-0644},
   journal={IEEE Transactions on Geoscience and Remote Sensing},
   publisher={Institute of Electrical and Electronics Engineers (IEEE)},
   author={Zhang, Zilun and Zhao, Tiancheng and Guo, Yulong and Yin, Jianwei},
   year={2024},
   pages={1–23} }


@misc{norelli_asif_2022,
      title={ASIF: Coupled Data Turns Unimodal Models to Multimodal Without Training}, 
      author={Antonio Norelli and Marco Fumero and Valentino Maiorca and Luca Moschella and Emanuele Rodolà and Francesco Locatello},
      year={2023},
      eprint={2210.01738},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{kim_magvlt_2023,
      title={MAGVLT: Masked Generative Vision-and-Language Transformer}, 
      author={Sungwoong Kim and Daejin Jo and Donghoon Lee and Jongmin Kim},
      year={2023},
      eprint={2303.12208},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{dai_clip_2022,
      title={Enabling Multimodal Generation on CLIP via Vision-Language Knowledge Distillation}, 
      author={Wenliang Dai and Lu Hou and Lifeng Shang and Xin Jiang and Qun Liu and Pascale Fung},
      year={2022},
      eprint={2203.06386},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}



@article{tang2023multiway,
  title={MMT: Multi-way Multi-modal Transformer for Multimodal Learning},
  author={Tang, Jiajia and Li, Kang and Hou, Ming and Jin, Xuanyu and Kong, Wanzeng and Ding, Yu and Zhao, Qianchuan},
  journal={International Joint Conference on Artificial Intelligence (IJCAI)},
  year={2023},
  pages={3458-3465},
}

@inproceedings{wang_multimodal_2022,
  title={Multimodal Token Fusion for Vision Transformers},
  author={Wang, Yikai and Chen, Xinghao and Cao, Lele and Huang, Wen-bing and Sun, Fuchun and Wang, Yunhe},
  booktitle={2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  pages={12176-12185},
  year={2022},
}

@misc{recasens_zorro_2023,
      title={Zorro: the masked multimodal transformer}, 
      author={Adrià Recasens and Jason Lin and Joāo Carreira and Drew Jaegle and Luyu Wang and Jean-baptiste Alayrac and Pauline Luc and Antoine Miech and Lucas Smaira and Ross Hemsley and Andrew Zisserman},
      year={2023},
      eprint={2301.09595},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@article{ma_crossmodal_2022,
  title={A Crossmodal Multiscale Fusion Network for Semantic Segmentation of Remote Sensing Data},
  author={Ma, Xianping and Zhang, Xiaokang and Pun, Man-On},
  journal={IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing},
  volume={15},
  pages={3463-3474},
  year={2022},
}


@article{zhao_deep_2024_Transaction,
  title={Deep Multimodal Learning with Vision, Audio, and Text: Challenges and Innovations},
  author={Zhao, Lihong and Wang, Huan},
  journal={IEEE Transactions on Neural Networks and Learning Systems},
  year={2024},
  volume={35},
  pages={1172-1184},
  doi={10.1109/TNNLS.2023.3040008},
}


@article{rahate_multimodal_2021,
   title={Multimodal Co-learning: Challenges, applications with datasets, recent advances and future directions},
   volume={81},
   ISSN={1566-2535},
   journal={Information Fusion},
   publisher={Elsevier BV},
   author={Rahate, Anil and Walambe, Rahee and Ramanna, Sheela and Kotecha, Ketan},
   year={2022},
   month=may, pages={203–239} }

@misc{qian_contrastive_2022,
      title={Contrastive Regularization for Multimodal Emotion Recognition Using Audio and Text}, 
      author={Fan Qian and Jiqing Han},
      year={2022},
      eprint={2211.10885},
      archivePrefix={arXiv},
      primaryClass={cs.SD}
}

@article{parekh_representation_2020,
  title={Weakly Supervised Representation Learning for Audio-Visual Scene Analysis},
  author={Parekh, Sanjeel and Essid, Slim and Ozerov, Alexey and Duong, Ngoc Q. K. and Pérez, Patrick and Richard, Gaël},
  journal={IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  volume={28},
  pages={416-428},
  year={2020},
  doi={10.1109/TASLP.2019.2957889},
}

@article{zhou_sentiment_2021,
  title={Visual-Textual Sentiment Analysis Enhanced by Hierarchical Cross-Modality Interaction},
  author={Zhou, Tao and Cao, Jiuxin and Zhu, Xueling and Liu, Bo and Li, Shancang},
  journal={IEEE Systems Journal},
  volume={15},
  pages={4303-4314},
  year={2021},
  doi={10.1109/jsyst.2020.3026879},
}

@misc{nakada_multimodal_2023,
      title={Understanding Multimodal Contrastive Learning and Incorporating Unpaired Data}, 
      author={Ryumei Nakada and Halil Ibrahim Gulluk and Zhun Deng and Wenlong Ji and James Zou and Linjun Zhang},
      year={2023},
      eprint={2302.06232},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{asai_set_2018,
      title={Set Cross Entropy: Likelihood-based Permutation Invariant Loss Function for Probability Distributions}, 
      author={Masataro Asai},
      year={2018},
      eprint={1812.01217},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@inproceedings{choi_amc_2020,
  title={AMC-Loss: Angular Margin Contrastive Loss for Improved Explainability in Image Classification},
  author={Choi, Hongjun and Som, Anirudh and Turaga, Pavan},
  booktitle={2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)},
  pages={3659-3666},
  year={2020},
  doi={10.1109/CVPRW50498.2020.00427},
}

@misc{khosla_supervised_2020,
      title={Supervised Contrastive Learning}, 
      author={Prannay Khosla and Piotr Teterwak and Chen Wang and Aaron Sarna and Yonglong Tian and Phillip Isola and Aaron Maschinot and Ce Liu and Dilip Krishnan},
      year={2021},
      eprint={2004.11362},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2004.11362}, 
}

@article{zang_scehr_2021,
  title={SCEHR: Supervised Contrastive Learning for Clinical Risk Prediction using Electronic Health Records},
  author={Zang, Chengxi and Wang, Fei},
  journal={2021 IEEE International Conference on Data Mining (ICDM)},
  pages={857-866},
  year={2021},
  doi={10.1109/ICDM51629.2021.00097},
}

@misc{Chen2023VAST,
      title={VAST: A Vision-Audio-Subtitle-Text Omni-Modality Foundation Model and Dataset}, 
      author={Sihan Chen and Handong Li and Qunbo Wang and Zijia Zhao and Mingzhen Sun and Xinxin Zhu and Jing Liu},
      year={2023},
      eprint={2305.18500},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@article{tang2022facial,
  title={Facial expression translation using landmark guided gans},
  author={Tang, Hao and Sebe, Nicu},
  journal={IEEE Transactions on Affective Computing},
  volume={13},
  number={4},
  pages={1986--1997},
  year={2022}
}

@article{qian2024dual,
  title={Dual-Path Transformer-Based GAN for Co-speech Gesture Synthesis},
  author={Qian, Xinyuan and Tang, Hao and Yang, Jichen and Zhu, Hongxu and Yin, Xu-Cheng},
  journal={International Journal of Social Robotics},
  pages={1--12},
  year={2024},
  publisher={Springer}
}

@article{tang2025pure,
  title={A pure MLP-Mixer-based GAN framework for guided image translation},
  author={Tang, Hao and Ren, Bin and Sebe, Nicu},
  journal={Elsevier PR},
  volume={157},
  pages={110894},
  year={2025}
}

@inproceedings{tao2023galip,
  title={Galip: Generative adversarial clips for text-to-image synthesis},
  author={Tao, Ming and Bao, Bing-Kun and Tang, Hao and Xu, Changsheng},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={14214--14223},
  year={2023}
}

@article{sun2024barbie,
  title={Barbie: Text to Barbie-Style 3D Avatars},
  author={Sun, Xiaokun and Zhang, Zhenyu and Tai, Ying and Wang, Qian and Tang, Hao and Yi, Zili and Yang, Jian},
  journal={arXiv preprint arXiv:2408.09126},
  year={2024}
}

@article{zhang2023enlighten,
  title={Enlighten-Your-Voice: When Multimodal Meets Zero-shot Low-light Image Enhancement},
  author={Zhang, Xiaofeng and Xu, Zishan and Tang, Hao and Gu, Chaochen and Chen, Wei and Zhu, Shanying and Guan, Xinping},
  journal={arXiv:2312.10109},
  year={2023}
}

@article{ni2024survey,
  title={A Survey on Multimodal Wearable Sensor-based Human Action Recognition},
  author={Ni, Jianyuan and Tang, Hao and Haque, Syed Tousiful and Yan, Yan and Ngu, Anne HH},
  journal={arXiv preprint arXiv:2404.15349},
  year={2024}
}

@article{zhang2024redundancy,
  title={From Redundancy to Relevance: Enhancing Explainability in Multimodal Large Language Models},
  author={Zhang, Xiaofeng and Shen, Chen and Yuan, Xiaosong and Yan, Shaotian and Xie, Liang and Wang, Wenxiao and Gu, Chaochen and Tang, Hao and Ye, Jieping},
  journal={arXiv preprint arXiv:2406.06579},
  year={2024}
}

@article{zhang2024shadclips,
  title={Shadclips: When Parameter-Efficient Fine-Tuning with Multimodal Meets Shadow Removal},
  author={Zhang, Xiaofeng and Xu, Zishan and Tang, Hao and Gu, Chaochen and Zhu, Shanying and Guan, Xinping},
  year={2024}
}

@misc{Wang2024ViLA,
      title={ViLA: Efficient Video-Language Alignment for Video Question Answering}, 
      author={Xijun Wang and Junbang Liang and Chun-Kai Wang and Kenan Deng and Yu Lou and Ming Lin and Shan Yang},
      year={2024},
      eprint={2312.08367},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@article{zhu2023minigpt4enhancingvisionlanguageunderstanding,
  title={Minigpt-4: Enhancing vision-language understanding with advanced large language models},
  author={Zhu, Deyao and Chen, Jun and Shen, Xiaoqian and Li, Xiang and Elhoseiny, Mohamed},
  journal={arXiv preprint arXiv:2304.10592},
  year={2023}
}

@misc{long2015fullyconvolutionalnetworkssemantic,
      title={Fully Convolutional Networks for Semantic Segmentation}, 
      author={Jonathan Long and Evan Shelhamer and Trevor Darrell},
      year={2015},
      eprint={1411.4038},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@inproceedings{danapal2020sensofusion,
  author={Danapal, Gokulesh and Santos, Giovanni A. and da Costa, João Paulo C. L. and Praciano, Bruno J. G. and Pinheiro, Gabriel P. M.},
  booktitle={2020 Workshop on Communication Networks and Power Systems (WCNPS)}, 
  title={Sensor fusion of camera and LiDAR raw data for vehicle detection}, 
  year={2020},
  volume={},
  number={},
  pages={1-6},
  keywords={Laser radar;Cameras;Data integration;Three-dimensional displays;Feature extraction;Roads;Object detection;YOLO;vehicle detection;raw data fusion;sensor fusion;camera;LiDAR},
  doi={10.1109/WCNPS50723.2020.9263724}}


@misc{Wang2023ONEPEACE,
      title={ONE-PEACE: Exploring One General Representation Model Toward Unlimited Modalities}, 
      author={Peng Wang and Shijie Wang and Junyang Lin and Shuai Bai and Xiaohuan Zhou and Jingren Zhou and Xinggang Wang and Chang Zhou},
      year={2023},
      eprint={2305.11172},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{wang2024qwen2vlenhancingvisionlanguagemodels,
      title={Qwen2-VL: Enhancing Vision-Language Model's Perception of the World at Any Resolution}, 
      author={Peng Wang and Shuai Bai and Sinan Tan and Shijie Wang and Zhihao Fan and Jinze Bai and Keqin Chen and Xuejing Liu and Jialin Wang and Wenbin Ge and Yang Fan and Kai Dang and Mengfei Du and Xuancheng Ren and Rui Men and Dayiheng Liu and Chang Zhou and Jingren Zhou and Junyang Lin},
      year={2024},
      eprint={2409.12191},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}


@misc{Lu2022UnifiedIO,
      title={Unified-IO: A Unified Model for Vision, Language, and Multi-Modal Tasks}, 
      author={Jiasen Lu and Christopher Clark and Rowan Zellers and Roozbeh Mottaghi and Aniruddha Kembhavi},
      year={2022},
      eprint={2206.08916},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{Li2023BLIP2,
      title={BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models}, 
      author={Junnan Li and Dongxu Li and Silvio Savarese and Steven Hoi},
      year={2023},
      eprint={2301.12597},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{bai2023qwenvl,
      title={Qwen-VL: A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond}, 
      author={Jinze Bai and Shuai Bai and Shusheng Yang and Shijie Wang and Sinan Tan and Peng Wang and Junyang Lin and Chang Zhou and Jingren Zhou},
      year={2023},
      eprint={2308.12966},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{shi2021neuroimaging,
      title={Heterogeneous Graph-Based Multimodal Brain Network Learning}, 
      author={Gen Shi and Yifan Zhu and Wenjin Liu and Quanming Yao and Xuesong Li},
      year={2022},
      eprint={2110.08465},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@inproceedings{xiong2014socialmedia,
  title={Multimodal Data Fusion in Text-Image Heterogeneous Graph for Social Media Recommendation},
  author={Xiong, Yu and Wang, Daling and Zhang, Yifei and Feng, Shi and Wang, Guoren},
  booktitle={International Conference on Neural Information Processing},
  pages={96-99},
  year={2014}
}

@inproceedings{golo2023earlyfusion,
  title={On the Use of Early Fusion Operators on Heterogeneous Graph Neural Networks for One-Class Learning},
  author={Gôlo, M. and De Moraes, Marcelo Isaias and Goularte, R. and Marcacini, R.},
  booktitle={Proceedings of the 29th Brazilian Symposium on Multimedia and the Web},
  year={2023},
  doi={10.1145/3617023.3617041}
}


@article{boehm2021harnessing,
  title={Harnessing Multimodal Data Integration to Advance Precision Oncology},
  author={Boehm, K. and Khosravi, P. and Vanguri, R. and Gao, Jianjiong and Shah, S.},
  journal={Nature Reviews Cancer},
  volume={22},
  pages={114-126},
  year={2021}
}

@misc{shankar2022fusion,
      title={Progressive Fusion for Multimodal Integration}, 
      author={Shiv Shankar and Laure Thompson and Madalina Fiterau},
      year={2022},
      eprint={2209.00302},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{tian2019multimodal,
  title={Multimodal Deep Representation Learning for Video Classification},
  author={Tian, Haiman and Tao, Yudong and Pouyanfar, Samira and Chen, Shu-Ching and Shyu, Mei-Ling},
  journal={World Wide Web},
  volume={22},
  pages={1325-1341},
  year={2019}
}

@article{li2020multiview,
  title={A Review on Machine Learning Principles for Multi-View Biological Data Integration},
  author={Li, Yifeng and Wu, Fang-Xiang and Ngom, Alioune},
  journal={Briefings in Bioinformatics},
  volume={19},
  pages={325–340},
  year={2020}
}

@article{mahdum2017applications,
  title={Applications of Deep Learning and Reinforcement Learning to Biological Data},
  author={Mahmud, M. and Kaiser, M.S. and Hussain, A. and Vassanelli, Stefano},
  journal={IEEE Transactions on Neural Networks and Learning Systems},
  volume={29},
  pages={2063-2079},
  year={2017}
}

@article{liang2024multimodal,
  title={Multimodal representation learning for tourism recommendation with two-tower architecture},
  author={Cui, Y. and Liang, S. and Zhang, YY},
  journal={PLoS One},
  year={2024}
}

@misc{vasilakis2024instrument,
      title={I can listen but cannot read: An evaluation of two-tower multimodal systems for instrument recognition}, 
      author={Yannis Vasilakis and Rachel Bittner and Johan Pauwels},
      year={2024},
      eprint={2407.18058},
      archivePrefix={arXiv},
      primaryClass={cs.SD}
}

@inproceedings{xu2023bridgetower,
  title={Bridgetower: Building bridges between encoders in vision-language representation learning},
  author={Xu, X. and Wu, C. and Rosenman, S. and Lal, V. and Che, W.},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  year={2023}
}

@inproceedings{su2023beyond,
  title={Beyond Two-Tower Matching: Learning Sparse Retrievable Cross-Interactions for Recommendation},
  author={Su, L. and Yan, F. and Zhu, J. and Xiao, X. and Duan, H.},
  booktitle={Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval},
  year={2023}
}

@article{du2023touchformer,
  title={Touchformer: A Transformer-based two-tower architecture for tactile temporal signal classification},
  author={Liu, C. and Liu, H. and Chen, H. and Du, W.},
  journal={IEEE Transactions on Multimedia},
  year={2023}
}

@article{chen2024mixtower,
  title={Mix-tower: Light visual question answering framework based on exclusive self-attention mechanism},
  author={Chen, D. and Chen, J. and Yang, L. and Shang, F.},
  journal={Neurocomputing},
  year={2024}
}

@article{fei2022towards,
  title={Towards artificial general intelligence via a multimodal foundation model},
  author={Fei, N. and Lu, Z. and Gao, Y. and Yang, G. and Huo, Y. and Wen, J. and Lu, H.},
  journal={Nature Communications},
  year={2022}
}

@misc{wen2024multimodal,
      title={Multimodal Reranking for Knowledge-Intensive Visual Question Answering}, 
      author={Haoyang Wen and Honglei Zhuang and Hamed Zamani and Alexander Hauptmann and Michael Bendersky},
      year={2024},
      eprint={2407.12277},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article {Bradshawjnumed.124.268072,
	author = {Bradshaw, Tyler J. and Tie, Xin and Warner, Joshua and Hu, Junjie and Li, Quanzheng and Li, Xiang},
	title = {Large Language Models and Large Multimodal Models in Medical Imaging: A Primer for Physicians},
	elocation-id = {jnumed.124.268072},
	year = {2025},
	doi = {10.2967/jnumed.124.268072},
	publisher = {Society of Nuclear Medicine},
	issn = {0161-5505},
	journal = {Journal of Nuclear Medicine}
}



@misc{pattnayak2024surveylargemultimodalmodel,
      title={Survey of Large Multimodal Model Datasets, Application Categories and Taxonomy}, 
      author={Priyaranjan Pattnayak and Hitesh Laxmichand Patel and Bhargava Kumar and Amit Agarwal and Ishan Banerjee and Srikant Panda and Tejaswini Kumar},
      year={2024},
      eprint={2412.17759},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
}

@inproceedings{tu2022crossmodal,
  title={Differentiable cross-modal hashing via multimodal transformers},
  author={Tu, J. and Liu, X. and Lin, Z.and Hong, R. and Wang, M.},
  booktitle={Proceedings of the 30th ACM International Conference on Multimedia},
  year={2022}
}

@inproceedings{yuan2021medication,
  title={Towards User Friendly Medication Mapping Using Entity-Boosted Two-Tower Neural Network},
  author={Yuan, S. and Bhatia, P. and Celikkaya, B. and Liu, H. and Choi, K.},
  booktitle={International Workshop on Deep Learning for Human Activity Recognition},
  year={2021}
}

%
@article{Liang2022MindTG,
  title={Mind the gap: Understanding the modality gap in multi-modal contrastive representation learning},
  author={Liang, Victor Weixin and Zhang, Yuhui and Kwon, Yongchan and Yeung, Serena and Zou, James Y},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={17612--17625},
  year={2022}
}

@InProceedings{tong2024eyeswideshutexploring,
    author    = {Tong, Shengbang and Liu, Zhuang and Zhai, Yuexiang and Ma, Yi and LeCun, Yann and Xie, Saining},
    title     = {Eyes Wide Shut? Exploring the Visual Shortcomings of Multimodal LLMs},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2024},
    pages     = {9568-9578}
}

@misc{yuksekgonul2023visionlanguagemodelsbehavelike,
      title={When and why vision-language models behave like bags-of-words, and what to do about it?}, 
      author={Mert Yuksekgonul and Federico Bianchi and Pratyusha Kalluri and Dan Jurafsky and James Zou},
      year={2023},
      eprint={2210.01936},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2210.01936}, 
}

@inproceedings{kim2024hyperbolicentailmentfiltering,
  title={Hype: Hyperbolic entailment filtering for underspecified images and texts},
  author={Kim, Wonjae and Chun, Sanghyuk and Kim, Taekyung and Han, Dongyoon and Yun, Sangdoo},
  booktitle={European Conference on Computer Vision},
  pages={247--265},
  year={2024},
  organization={Springer}
}

@misc{ding2021sparsefusion,
      title={Sparse Fusion for Multimodal Transformers}, 
      author={Yi Ding and Alex Rich and Mason Wang and Noah Stier and Matthew Turk and Pradeep Sen and Tobias Höllerer},
      year={2021},
      eprint={2111.11992},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2111.11992}, 
}

@InProceedings{xue2022dynamicfusion, author = {Xue, Zihui and Marculescu, Radu}, title = {Dynamic Multimodal Fusion}, booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops}, month = {June}, year = {2023}, pages = {2575-2584} }

@inproceedings{liu2018lowrank,
    title = "Efficient Low-rank Multimodal Fusion With Modality-Specific Factors",
    author = "Liu, Zhun  and
      Shen, Ying  and
      Lakshminarasimhan, Varun Bharadhwaj  and
      Liang, Paul Pu  and
      Bagher Zadeh, AmirAli  and
      Morency, Louis-Philippe",
    editor = "Gurevych, Iryna  and
      Miyao, Yusuke",
    booktitle = "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2018",
    address = "Melbourne, Australia",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P18-1209/",
    doi = "10.18653/v1/P18-1209",
    pages = "2247--2256",
}

@misc{jia2024geminifusion,
      title={GeminiFusion: Efficient Pixel-wise Multimodal Fusion for Vision Transformer}, 
      author={Ding Jia and Jianyuan Guo and Kai Han and Han Wu and Chao Zhang and Chang Xu and Xinghao Chen},
      year={2024},
      eprint={2406.01210},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2406.01210}, 
}

@inproceedings{yan2020mitigating,
  title={Mitigating biases in multimodal personality assessment},
  author={Yan, Shen and Huang, Di and Soleymani, Mohammad},
  booktitle={Proceedings of the 2020 International Conference on Multimodal Interaction},
  pages={361--369},
  year={2020}
}

@misc{zhang2023theory,
      title={Understanding Unimodal Bias in Multimodal Deep Linear Networks}, 
      author={Yedi Zhang and Peter E. Latham and Andrew Saxe},
      year={2024},
      eprint={2312.00935},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2312.00935}, 
}

@misc{roger2023towards,
      title={Towards ethical multimodal systems}, 
      author={Alexis Roger and Esma Aïmeur and Irina Rish},
      year={2024},
      eprint={2304.13765},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2304.13765}, 
}

@misc{zhang2024genderalign,
      title={GenderAlign: An Alignment Dataset for Mitigating Gender Bias in Large Language Models}, 
      author={Tao Zhang and Ziqian Zeng and Yuxiang Xiao and Huiping Zhuang and Cen Chen and James Foulds and Shimei Pan},
      year={2024},
      eprint={2406.13925},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2406.13925}, 
}

@article{zhang2023bimodal,
AUTHOR = {Zhang, Rui and Xue, Chengrong and Qi, Qingfu and Lin, Liyuan and Zhang, Jing and Zhang, Lun},
TITLE = {Bimodal Fusion Network with Multi-Head Attention for Multimodal Sentiment Analysis},
JOURNAL = {Applied Sciences},
VOLUME = {13},
YEAR = {2023},
NUMBER = {3},
ARTICLE-NUMBER = {1915},
URL = {https://www.mdpi.com/2076-3417/13/3/1915},
ISSN = {2076-3417},
DOI = {10.3390/app13031915}
}


@article{qian2023leveraging,
title = {Leveraging multimodal features for knowledge graph entity alignment based on dynamic self-attention networks},
journal = {Expert Systems with Applications},
volume = {228},
pages = {120363},
year = {2023},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2023.120363},
url = {https://www.sciencedirect.com/science/article/pii/S0957417423008655},
author = {Ye Qian and Li Pan},
}

@inproceedings{hu2023provla,
    author    = {Hu, Zhizhang and Zhu, Xinliang and Tran, Son and Vidal, Ren\'e and Dhua, Arnab},
    title     = {ProVLA: Compositional Image Search with Progressive Vision-Language Alignment and Multimodal Fusion},
    booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV) Workshops},
    month     = {October},
    year      = {2023},
    pages     = {2772-2777}
}

@article{ning2024abftnet,
AUTHOR = {Ning, Meng and Zhou, Fan and Wang, Wei and Wang, Shaoqiang and Zhang, Peiying and Wang, Jian},
TITLE = {AbFTNet: An Efficient Transformer Network with Alignment before Fusion for Multimodal Automatic Modulation Recognition},
JOURNAL = {Electronics},
VOLUME = {13},
YEAR = {2024},
NUMBER = {18},
ARTICLE-NUMBER = {3725},
URL = {https://www.mdpi.com/2079-9292/13/18/3725},
ISSN = {2079-9292},
}


@misc{geng2023hiclip,
      title={HiCLIP: Contrastive Language-Image Pretraining with Hierarchy-aware Attention}, 
      author={Shijie Geng and Jianbo Yuan and Yu Tian and Yuxiao Chen and Yongfeng Zhang},
      year={2023},
      eprint={2303.02995},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2303.02995}, 
}

@misc{chen2024comkdclip,
      title={ComKD-CLIP: Comprehensive Knowledge Distillation for Contrastive Language-Image Pre-traning Model}, 
      author={Yifan Chen and Xiaozhen Qiao and Zhe Sun and Xuelong Li},
      year={2024},
      eprint={2408.04145},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2408.04145}, 
}

@misc{ma2024sycoca,
      title={SyCoCa: Symmetrizing Contrastive Captioners with Attentive Masking for Multimodal Alignment}, 
      author={Ziping Ma and Furong Xu and Jian Liu and Ming Yang and Qingpei Guo},
      year={2024},
      eprint={2401.02137},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2401.02137}, 
}


@InProceedings{gondal2023dac,
    author    = {Gondal, Muhammad Waleed and Gast, Jochen and Ruiz, Inigo Alonso and Droste, Richard and Macri, Tommaso and Kumar, Suren and Staudigl, Luitpold},
    title     = {Domain Aligned CLIP for Few-Shot Classification},
    booktitle = {Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)},
    month     = {January},
    year      = {2024},
    pages     = {5721-5730}
}

@article{wu2024mfeclip,
  author={Wu, Fei and Ma, Yongheng and Jin, Hao and Jing, Xiao-Yuan and Jiang, Guo-Ping},
  journal={IEEE Signal Processing Letters}, 
  title={MFECLIP: CLIP With Mapping-Fusion Embedding for Text-Guided Image Editing}, 
  year={2024},
  volume={31},
  number={},
  pages={116-120},
  keywords={Semantics;Generative adversarial networks;Training;Task analysis;Flowering plants;Birds;Telecommunications;Text-guided image editing;GAN;CLIP},
  doi={10.1109/LSP.2023.3342649}}


@misc{li2024gsclip,
      title={GS-CLIP: Gaussian Splatting for Contrastive Language-Image-3D Pretraining from Real-World Data}, 
      author={Haoyuan Li and Yanpeng Zhou and Yihan Zeng and Hang Xu and Xiaodan Liang},
      year={2024},
      eprint={2402.06198},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2402.06198}, 
}


@article{yang2024dmfgan,
  author={Yang, Bing and Xiang, Xueqin and Kong, Wangzeng and Zhang, Jianhai and Peng, Yong},
  journal={IEEE Transactions on Multimedia}, 
  title={DMF-GAN: Deep Multimodal Fusion Generative Adversarial Networks for Text-to-Image Synthesis}, 
  year={2024},
  volume={26},
  number={},
  pages={6956-6967},
  keywords={Semantics;Generative adversarial networks;Generators;Training;Visualization;Image synthesis;Fuses;Deep multimodal fusion;generative adversarial network;text-to-image (T2I) synthesis},
  doi={10.1109/TMM.2024.3358086}}

@inproceedings{tao2022df,
  title={Df-gan: A simple and effective baseline for text-to-image synthesis},
  author={Tao, Ming and Tang, Hao and Wu, Fei and Jing, Xiao-Yuan and Bao, Bing-Kun and Xu, Changsheng},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition (CVPR)},
  pages={16515--16525},
  year={2022}
}

@inproceedings{kim2024diffusiongan,
    author    = {Kim, Jihyun and Oh, Changjae and Do, Hoseok and Kim, Soohyun and Sohn, Kwanghoon},
    title     = {Diffusion-driven GAN Inversion for Multi-Modal Face Image Generation},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2024},
    pages     = {10403-10412}
}

@article{cao2025generative,
  author  = {Yongnian Cao and Xuechun Yang and Rui Sun},
  title   = {Generative AI Models: Theoretical Foundations and Algorithmic Practices},
  journal = {Journal of Industrial Engineering and Applied Science},
  year    = {2025},
  volume  = {3},
  number  = {1},
  pages   = {1--9},
  doi = {10.70393/6a69656173.323633}
}

@misc{bousetouane2025vision,
      title={Generative AI for Vision: A Comprehensive Study of Frameworks and Applications}, 
      author={Fouad Bousetouane},
      year={2025},
      eprint={2501.18033},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2501.18033}, 
}


@article{sk2022mlfc,
  title={Multimodal Event Detection in Big Data using Multi-Level Fusion Classifier},
  author={S. K. and V. D.},
  journal={Indian Journal of Computer Science and Engineering},
  year={2022},
  doi={10.21817/indjcse/2022/v13i3/221303111}
}

@inproceedings{lin2021vx2text,
  title={VX2TEXT: End-to-End Learning of Video-Based Text Generation From Multimodal Inputs},
  author={Xudong Lin and Gedas Bertasius and Jue Wang and Shih-Fu Chang and Devi Parikh and Lorenzo Torresani},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2021},
  pages={7001--7011},
  doi={10.1109/CVPR46437.2021.00693}
}

@inproceedings{fu2022nhfnet,
  title={NHFNet: A Non-Homogeneous Fusion Network for Multimodal Sentiment Analysis},
  author={Ziwang Fu and Feng Liu and Qing Xu and Jiayin Qi and Xiangling Fu and Aimin Zhou and Zhibin Li},
  booktitle={IEEE International Conference on Multimedia and Expo (ICME)},
  year={2022},
  pages={1--6},
  doi={10.1109/ICME52920.2022.9859836}
}

@inproceedings{sun2022depression,
  title={Multimodal depression detection using a deep feature fusion network},
  author={Guangyao Sun and Shenghui Zhao and Bochao Zou and Yubo An},
  booktitle={SPIE Medical Imaging},
  volume={12506},
  pages={125066A},
  year={2022},
  doi={10.1117/12.2662620}
}

@article{wang2022tetfn,
  title={TETFN: A text enhanced transformer fusion network for multimodal sentiment analysis},
  author={Di Wang and Xutong Guo and Yu-min Tian and Jinhui Liu and Lihuo He and Xuemei Luo},
  journal={Pattern Recognition},
  volume={136},
  pages={109259},
  year={2022},
  doi={10.1016/j.patcog.2022.109259}
}

@article{yang2024triclt,
  title={Tri-CLT: Learning Tri-Modal Representations with Contrastive Learning and Transformer for Multimodal Sentiment Recognition},
  author={Zhiyong Yang and Zijian Li and Dongdong Zhu and Yu Zhou},
  journal={Information Technology and Control},
  volume={53},
  pages={206--219},
  year={2024},
  doi={10.5755/j01.itc.53.1.35060}
}

@article{fu2025tmfn,
  title={TMFN: A Text-Based Multimodal Fusion Network with Multi-Scale Feature Extraction and Unsupervised Contrastive Learning for Multimodal Sentiment Analysis},
  author={Junsong Fu and Youjia Fu and Huixia Xue and Zihao Xu},
  journal={Complex \& Intelligent Systems},
  year={2025},
  doi={10.1007/s40747-024-01724-5}
}

@article{dou2022fiber,
  title={Coarse-to-Fine Vision-Language Pretraining with Fusion in the Backbone},
  author={Zi-Yi Dou and Aishwarya Kamath and Zhe Gan and Pengchuan Zhang and Jianfeng Wang and Linjie Li and Zicheng Liu and Ce Liu and Yann LeCun and Nanyun Peng and Jianfeng Gao and Lijuan Wang},
  journal={arXiv preprint arXiv:2206.07643},
  year={2022},
  url={https://arxiv.org/abs/2206.07643}
}


@article{yoon2023robust,
  title={Multimedia Analysis of Robustly Optimized Multimodal Transformer Based on Vision and Language Co-learning},
  author={Junho Yoon and Gyuhong Choi and Changyun Choi},
  journal={Information Fusion},
  volume={100},
  pages={101922},
  year={2023},
  doi={10.1016/j.inffus.2023.101922}
}

@inproceedings{nemati2018cca,
  title={Canonical Correlation Analysis for Data Fusion in Multimodal Emotion Recognition},
  author={Shahla Nemati},
  booktitle={2018 9th International Symposium on Telecommunications (IST)},
  pages={676--681},
  year={2018},
  doi={10.1109/ISTEL.2018.8661140}
}

@inproceedings{NEURIPS2023_5e84e441,
 author = {Luo, Gen and Zhou, Yiyi and Ren, Tianhe and Chen, Shengxin and Sun, Xiaoshuai and Ji, Rongrong},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Oh and T. Naumann and A. Globerson and K. Saenko and M. Hardt and S. Levine},
 pages = {29615--29627},
 publisher = {Curran Associates, Inc.},
 title = {Cheap and Quick: Efficient Vision-Language Instruction Tuning for Large Language Models},
 url = {https://proceedings.neurips.cc/paper_files/paper/2023/file/5e84e4413268b713f0d4a1b23a9dae57-Paper-Conference.pdf},
 volume = {36},
 year = {2023}
}

@ARTICLE{10445007,
  author={Zhang, Jingyi and Huang, Jiaxing and Jin, Sheng and Lu, Shijian},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
  title={Vision-Language Models for Vision Tasks: A Survey}, 
  year={2024},
  volume={46},
  number={8},
  pages={5625-5644},
  keywords={Task analysis;Visualization;Training;Deep learning;Surveys;Data models;Predictive models;Big Data;big model;deep learning;deep neural network;knowledge distillation;object detection;pre-training;semantic segmentation;transfer learning;vision-language model;visual recognition;image classification},
  doi={10.1109/TPAMI.2024.3369699}}

@InProceedings{Zhou_2023_CVPR,
    author    = {Zhou, Yutong and Shimada, Nobutaka},
    title     = {Vision + Language Applications: A Survey},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops},
    month     = {June},
    year      = {2023},
    pages     = {826-842}
}

@misc{li2025surveystateartlarge,
      title={A Survey of State of the Art Large Vision Language Models: Alignment, Benchmark, Evaluations and Challenges}, 
      author={Zongxia Li and Xiyang Wu and Hongyang Du and Fuxiao Liu and Huy Nghiem and Guangyao Shi},
      year={2025},
      eprint={2501.02189},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2501.02189}, 
}

@inproceedings{sarkar2024mitigating,
	title = {Mitigating Object Hallucination in {MLLMs} via Data-augmented Phrase-level Alignment},
	url = {https://openreview.net/forum?id=yG1fW8igzP},
	booktitle = {The Thirteenth International Conference on Learning Representations},
	author = {Sarkar, Pritam and Ebrahimi, Sayna and Etemad, Ali and Beirami, Ahmad and Arik, Sercan O. and Pfister, Tomas},
	urldate = {2025-06-14},
	date = {2024-10-04},
    year = {2025},
	langid = {english},
}

@misc{wang2024pfram,
      title={Understanding Multimodal Hallucination with Parameter-Free Representation Alignment}, 
      author={Yueqian Wang and Jianxin Liang and Yuxuan Wang and Huishuai Zhang and Dongyan Zhao},
      year={2024},
      eprint={2409.01151},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2409.01151}, 
}

@InProceedings{yu2023rlhfv,
    author    = {Yu, Tianyu and Yao, Yuan and Zhang, Haoye and He, Taiwen and Han, Yifeng and Cui, Ganqu and Hu, Jinyi and Liu, Zhiyuan and Zheng, Hai-Tao and Sun, Maosong and Chua, Tat-Seng},
    title     = {RLHF-V: Towards Trustworthy MLLMs via Behavior Alignment from Fine-grained Correctional Human Feedback},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2024},
    pages     = {13807-13816}
}

@misc{li2025llavast,
      title={LLaVA-ST: A Multimodal Large Language Model for Fine-Grained Spatial-Temporal Understanding}, 
      author={Hongyu Li and Jinyu Chen and Ziyu Wei and Shaofei Huang and Tianrui Hui and Jialin Gao and Xiaoming Wei and Si Liu},
      year={2025},
      eprint={2501.08282},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2501.08282}, 
}

@misc{gong2025vrs,
      title={The Devil is in Temporal Token: High Quality Video Reasoning Segmentation}, 
      author={Sitong Gong and Yunzhi Zhuge and Lu Zhang and Zongxin Yang and Pingping Zhang and Huchuan Lu},
      year={2025},
      eprint={2501.08549},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2501.08549}, 
}

@misc{jiang2024modalityfair,
      title={Modality-Fair Preference Optimization for Trustworthy MLLM Alignment}, 
      author={Songtao Jiang and Yan Zhang and Ruizhe Chen and Tianxiang Hu and Yeying Jin and Qinglin He and Yang Feng and Jian Wu and Zuozhu Liu},
      year={2025},
      eprint={2410.15334},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2410.15334}, 
}

@misc{lou2025saev,
      title={SAE-V: Interpreting Multimodal Models for Enhanced Alignment}, 
      author={Hantao Lou and Changye Li and Jiaming Ji and Yaodong Yang},
      year={2025},
      eprint={2502.17514},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2502.17514}, 
}

@inproceedings{wang2021role,
author = {Wang, Xingyue and Shu, Kuang and Kuang, Haowei and Luo, Shiwei and Jin, Richu and Liu, Jiang},
title = {The Role of Spatial Alignment in Multimodal Medical Image Fusion Using Deep Learning for Diagnostic Problems},
year = {2022},
isbn = {9781450385909},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3484377.3484384},
doi = {10.1145/3484377.3484384},
booktitle = {Proceedings of the 2021 International Conference on Intelligent Medicine and Health},
pages = {40–46},
numpages = {7},
keywords = {Deep Learning, Medical Image Analysis, Multimodal Medical Image Fusion, Spatial Alignment},
location = {Macau, China},
series = {ICIMH '21}
}



@article{li2025mulfscap,
  author={Li, Huafeng and Yang, Zengyi and Zhang, Yafei and Jia, Wei and Yu, Zhengtao and Liu, Yu},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
  title={MulFS-CAP: Multimodal Fusion-Supervised Cross-Modality Alignment Perception for Unregistered Infrared-Visible Image Fusion}, 
  year={2025},
  volume={47},
  number={5},
  pages={3673-3690},
  keywords={Feature extraction;Image fusion;Image registration;Dictionaries;Training;Correlation;Visualization;Transformers;Image reconstruction;Sensor fusion;Cross-modality alignment perception;unregistered infrared-visible image fusion;learnable modality dictionary;registration and fusion},
  doi={10.1109/TPAMI.2025.3535617}}

@misc{lin2024stalign,
      title={ST-Align: A Multimodal Foundation Model for Image-Gene Alignment in Spatial Transcriptomics}, 
      author={Yuxiang Lin and Ling Luo and Ying Chen and Xushi Zhang and Zihui Wang and Wenxian Yang and Mengsha Tong and Rongshan Yu},
      year={2024},
      eprint={2411.16793},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2411.16793}, 
}

@inproceedings{zadeh-etal-2017-tensor,
    title = "Tensor Fusion Network for Multimodal Sentiment Analysis",
    author = "Zadeh, Amir  and
      Chen, Minghai  and
      Poria, Soujanya  and
      Cambria, Erik  and
      Morency, Louis-Philippe",
    editor = "Palmer, Martha  and
      Hwa, Rebecca  and
      Riedel, Sebastian",
    booktitle = "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing",
    month = sep,
    year = "2017",
    address = "Copenhagen, Denmark",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D17-1115/",
    doi = "10.18653/v1/D17-1115",
    pages = "1103--1114",
}

@misc{tsai2019learningfactorizedmultimodalrepresentations,
      title={Learning Factorized Multimodal Representations}, 
      author={Yao-Hung Hubert Tsai and Paul Pu Liang and Amir Zadeh and Louis-Philippe Morency and Ruslan Salakhutdinov},
      year={2019},
      eprint={1806.06176},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1806.06176}, 
}

@misc{jaegle2021perceivergeneralperceptioniterative,
      title={Perceiver: General Perception with Iterative Attention}, 
      author={Andrew Jaegle and Felix Gimeno and Andrew Brock and Andrew Zisserman and Oriol Vinyals and Joao Carreira},
      year={2021},
      eprint={2103.03206},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2103.03206}, 
}

@misc{singh2022flavafoundationallanguagevision,
      title={FLAVA: A Foundational Language And Vision Alignment Model}, 
      author={Amanpreet Singh and Ronghang Hu and Vedanuj Goswami and Guillaume Couairon and Wojciech Galuba and Marcus Rohrbach and Douwe Kiela},
      year={2022},
      eprint={2112.04482},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2112.04482}, 
}

@misc{lu2019vilbertpretrainingtaskagnosticvisiolinguistic,
      title={ViLBERT: Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language Tasks}, 
      author={Jiasen Lu and Dhruv Batra and Devi Parikh and Stefan Lee},
      year={2019},
      eprint={1908.02265},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/1908.02265}, 
}

@misc{zhai2023sigmoidlosslanguageimage,
      title={Sigmoid Loss for Language Image Pre-Training}, 
      author={Xiaohua Zhai and Basil Mustafa and Alexander Kolesnikov and Lucas Beyer},
      year={2023},
      eprint={2303.15343},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2303.15343}, 
}

@misc{girdhar2023imagebindembeddingspacebind,
      title={ImageBind: One Embedding Space To Bind Them All}, 
      author={Rohit Girdhar and Alaaeldin El-Nouby and Zhuang Liu and Mannat Singh and Kalyan Vasudev Alwala and Armand Joulin and Ishan Misra},
      year={2023},
      eprint={2305.05665},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2305.05665}, 
}

@inproceedings{li-etal-2024-textbind,
    title = "{T}ext{B}ind: Multi-turn Interleaved Multimodal Instruction-following in the Wild",
    author = "Li, Huayang  and
      Li, Siheng  and
      Cai, Deng  and
      Wang, Longyue  and
      Liu, Lemao  and
      Watanabe, Taro  and
      Yang, Yujiu  and
      Shi, Shuming",
    editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
    booktitle = "Findings of the Association for Computational Linguistics: ACL 2024",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.findings-acl.537/",
    doi = "10.18653/v1/2024.findings-acl.537",
    pages = "9053--9076",
}

@inproceedings{zuo2023svp,
  title={SVP-T: A Shape-Level Variable-Position Transformer for Multivariate Time Series Classification},
  author={Zuo, Rui and Li, Guoqing and Choi, Bongshin and Bhowmick, Sourav and Mah, Daphne N. -yin and Wong, Gary L.},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={37},
  issue={9},
  pages={11497--11505},
  year={2023},
  doi={10.1609/aaai.v37i9.26359}
}

@inproceedings{grail-etal-2021-globalizing,
    title = "Globalizing {BERT}-based Transformer Architectures for Long Document Summarization",
    author = "Grail, Quentin  and
      Perez, Julien  and
      Gaussier, Eric",
    editor = "Merlo, Paola  and
      Tiedemann, Jorg  and
      Tsarfaty, Reut",
    booktitle = "Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume",
    month = apr,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.eacl-main.154/",
    doi = "10.18653/v1/2021.eacl-main.154",
    pages = "1792--1810",
}

@misc{zhang2023neuralattentionenhancingqkv,
      title={Neural Attention: Enhancing QKV Calculation in Self-Attention Mechanism with Neural Networks}, 
      author={Muhan Zhang},
      year={2023},
      eprint={2310.11398},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2310.11398}, 
}

@inproceedings{mseg3d_cvpr2023,
author    = {Jiale Li and
            Hang Dai and
            Hao Han and
            Yong Ding},
title     = {MSeg3D: Multi-modal 3D Semantic Segmentation for Autonomous Driving},
booktitle = {CVPR},
pages     = {21694--21704},
year      = {2023},
}

@inproceedings{sdseg3d_eccv2022,
author    = {Jiale Li and
            Hang Dai and
            Yong Ding},
title     = {Self-Distillation for Robust {LiDAR} Semantic Segmentation in Autonomous Driving},
booktitle = {ECCV},
pages     = {659--676},
year      = {2022},
}

@misc{zhang2024risurconvrotationinvariantsurface,
      title={RISurConv: Rotation Invariant Surface Attention-Augmented Convolutions for 3D Point Cloud Classification and Segmentation}, 
      author={Zhiyuan Zhang and Licheng Yang and Zhiyu Xiang},
      year={2024},
      eprint={2408.06110},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2408.06110}, 
}

@misc{bachmann20244m21anytoanyvisionmodel,
      title={4M-21: An Any-to-Any Vision Model for Tens of Tasks and Modalities}, 
      author={Roman Bachmann and Oğuzhan Fatih Kar and David Mizrahi and Ali Garjani and Mingfei Gao and David Griffiths and Jiaming Hu and Afshin Dehghan and Amir Zamir},
      year={2024},
      eprint={2406.09406},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2406.09406}, 
}

@inproceedings{45857,title	= {Audio Set: An ontology and human-labeled dataset for audio events},author	= {Jort F. Gemmeke and Daniel P. W. Ellis and Dylan Freedman and Aren Jansen and Wade Lawrence and R. Channing Moore and Manoj Plakal and Marvin Ritter},year	= {2017},booktitle	= {Proc. IEEE ICASSP 2017},address	= {New Orleans, LA}}

@ARTICLE{5871728,
  author={Koelstra, Sander and Muhl, Christian and Soleymani, Mohammad and Lee, Jong-Seok and Yazdani, Ashkan and Ebrahimi, Touradj and Pun, Thierry and Nijholt, Anton and Patras, Ioannis},
  journal={IEEE Transactions on Affective Computing}, 
  title={DEAP: A Database for Emotion Analysis ;Using Physiological Signals}, 
  year={2012},
  volume={3},
  number={1},
  pages={18-31},
  keywords={Videos;Databases;Electroencephalography;Motion pictures;Multimedia communication;Visualization;Face;Emotion classification;EEG;physiological signals;signal processing;pattern classification;affective computing.},
  doi={10.1109/T-AFFC.2011.15}}

@InProceedings{Chung18b,
              author       = "Chung, J.~S. and Nagrani, A. and Zisserman, A.",
              title        = "VoxCeleb2: Deep Speaker Recognition",
              booktitle    = "INTERSPEECH",
              year         = "2018",
            }

@INPROCEEDINGS{6246152,
  author={Reiss, Attila and Stricker, Didier},
  booktitle={2012 16th International Symposium on Wearable Computers}, 
  title={Introducing a New Benchmarked Dataset for Activity Monitoring}, 
  year={2012},
  volume={},
  number={},
  pages={108-109},
  keywords={Benchmark testing;Monitoring;Standards;Accuracy;Decision trees;Heart rate;Biomedical monitoring},
  doi={10.1109/ISWC.2012.13}}

@article{10.1145/1964897.1964918,
author = {Kwapisz, Jennifer R. and Weiss, Gary M. and Moore, Samuel A.},
title = {Activity recognition using cell phone accelerometers},
year = {2011},
issue_date = {December 2010},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {2},
issn = {1931-0145},
url = {https://doi.org/10.1145/1964897.1964918},
doi = {10.1145/1964897.1964918},
journal = {SIGKDD Explor. Newsl.},
month = mar,
pages = {74–82},
numpages = {9},
keywords = {sensors, sensor mining, induction, cell phone, activity recognition, accelerometer}
}

@INPROCEEDINGS{ninapro,
     author = {Atzori, Manfredo and Gijsberts, Arjan and Heynen, Simone and Hager, Anne-Gabrielle Mittaz and Deriaz, Olivier and der Smagt, Patrick Vand and Castellini, Claudio and Caputo, Barbara and M{\"{u}}ller, Henning},
      month = jun,
      title = {Building the NINAPRO Database: A Resource for the Biorobotics Community},
  booktitle = {Proceedings of the IEEE International Conference on Biomedical Robotics and Biomechatronics},
       year = {2012},
      pages = {51},
   location = {Roma, Italy}
}

@article{Atzori2014,
  author    = {Atzori, Manfredo and Gijsberts, Arjan and Castellini, Claudio and Caputo, Barbara and Hager, Anne-Gabrielle Mittaz and Elsig, Simone and Giatsidis, Giorgio and Bassetto, Franco and M{\"u}ller, Henning},
  title     = {Electromyography data for non-invasive naturally-controlled robotic hand prostheses},
  journal   = {Scientific Data},
  year      = {2014},
  volume    = {1},
  number    = {1},
  pages     = {140053},
  doi       = {10.1038/sdata.2014.53},
  url       = {https://doi.org/10.1038/sdata.2014.53},
  issn      = {2052-4463},
  date      = {2014-12-23},
}

@ARTICLE{6825822,
  author={Atzori, Manfredo and Gijsberts, Arjan and Kuzborskij, Ilja and Elsig, Simone and Mittaz Hager, Anne-Gabrielle and Deriaz, Olivier and Castellini, Claudio and Müller, Henning and Caputo, Barbara},
  journal={IEEE Transactions on Neural Systems and Rehabilitation Engineering}, 
  title={Characterization of a Benchmark Database for Myoelectric Movement Classification}, 
  year={2015},
  volume={23},
  number={1},
  pages={73-83},
  keywords={Electrodes;Databases;Benchmark testing;Wrist;Protocols;Prosthetics;Standards;Electromyography;machine learning;prosthetics;publicly available databases},
  doi={10.1109/TNSRE.2014.2328495}}

@InProceedings{10.1007/978-3-319-13105-4_14,
author="Banos, Oresti
and Garcia, Rafael
and Holgado-Terriza, Juan A.
and Damas, Miguel
and Pomares, Hector
and Rojas, Ignacio
and Saez, Alejandro
and Villalonga, Claudia",
editor="Pecchia, Leandro
and Chen, Liming Luke
and Nugent, Chris
and Bravo, Jos{\'e}",
title="mHealthDroid: A Novel Framework for Agile Development of Mobile Health Applications",
booktitle="Ambient Assisted Living and Daily Activities",
year="2014",
publisher="Springer International Publishing",
address="Cham",
pages="91--98",
isbn="978-3-319-13105-4"
}


@article{schalk2004bci2000,
  author    = {Schalk, Gerwin and McFarland, Dennis J. and Hinterberger, Thilo and Birbaumer, Niels and Wolpaw, Jonathan R.},
  title     = {BCI2000: A General-Purpose Brain-Computer Interface (BCI) System},
  journal   = {IEEE Transactions on Biomedical Engineering},
  year      = {2004},
  volume    = {51},
  number    = {6},
  pages     = {1034--1043},
  month     = {Jun},
  doi       = {10.1109/TBME.2004.827072},
  pmid      = {15188875},
  issn      = {0018-9294},
  address   = {United States},
  language  = {eng},
  publisher = {IEEE},
}

@INPROCEEDINGS{9747631,
  author={Guzhov, Andrey and Raue, Federico and Hees, Jörn and Dengel, Andreas},
  booktitle={ICASSP 2022 - 2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, 
  title={Audioclip: Extending Clip to Image, Text and Audio}, 
  year={2022},
  volume={},
  number={},
  pages={976-980},
  keywords={Training;Visualization;Codes;Fuses;Conferences;Signal processing;Market research;Audio;multimodal;zero-shot;classification},
  doi={10.1109/ICASSP43922.2022.9747631}}

@inproceedings{han-etal-2021-improving,
    title = "Improving Multimodal Fusion with Hierarchical Mutual Information Maximization for Multimodal Sentiment Analysis",
    author = "Han, Wei  and
      Chen, Hui  and
      Poria, Soujanya",
    editor = "Moens, Marie-Francine  and
      Huang, Xuanjing  and
      Specia, Lucia  and
      Yih, Scott Wen-tau",
    booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2021",
    address = "Online and Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.emnlp-main.723/",
    doi = "10.18653/v1/2021.emnlp-main.723",
    pages = "9180--9192",
}

@INPROCEEDINGS{8954271,
  author={Akbari, Hassan and Karaman, Svebor and Bhargava, Surabhi and Chen, Brian and Vondrick, Carl and Chang, Shih-Fu},
  booktitle={2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={Multi-Level Multimodal Common Semantic Space for Image-Phrase Grounding}, 
  year={2019},
  volume={},
  number={},
  pages={12468-12478},
  keywords={Location awareness;Visualization;Grounding;Semantics;Performance gain;Feature extraction;Question answering (information retrieval);Vision + Language;Deep Learning;Representation Learning;Visual Reasoning},
  doi={10.1109/CVPR.2019.01276}}


@misc{li2024coupledmambaenhancedmultimodal,
      title={Coupled Mamba: Enhanced Multi-modal Fusion with Coupled State Space Model}, 
      author={Wenbing Li and Hang Zhou and Junqing Yu and Zikai Song and Wei Yang},
      year={2024},
      eprint={2405.18014},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2405.18014}, 
}

@inproceedings{NEURIPS2024_b7eecb72,
 author = {Li, Honglin and Zhang, Yunlong and Chen, Pingyi and Shui, Zhongyi and Zhu, Chenglu and Yang, Lin},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Globerson and L. Mackey and D. Belgrave and A. Fan and U. Paquet and J. Tomczak and C. Zhang},
 pages = {101498--101528},
 publisher = {Curran Associates, Inc.},
 title = {Rethinking Transformer for Long Contextual Histopathology Whole Slide Image Analysis},
 url = {https://proceedings.neurips.cc/paper_files/paper/2024/file/b7eecb72574b043ad0c69ea296212450-Paper-Conference.pdf},
 volume = {37},
 year = {2024}
}

@misc{kim2024missingmodalitypredictionunpaired,
      title={Missing Modality Prediction for Unpaired Multimodal Learning via Joint Embedding of Unimodal Models}, 
      author={Donggeun Kim and Taesup Kim},
      year={2024},
      eprint={2407.12616},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2407.12616}, 
}

@inproceedings{wang2024driving,
  title={Driving into the future: Multiview visual forecasting and planning with world model for autonomous driving},
  author={Wang, Yuqi and He, Jiawei and Fan, Lue and Li, Hongxin and Chen, Yuntao and Zhang, Zhaoxiang},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={14749--14759},
  year={2024}
}

@misc{munia2025differentialattentionmultimodalcrisis,
      title={Differential Attention for Multimodal Crisis Event Analysis}, 
      author={Nusrat Munia and Junfeng Zhu and Olfa Nasraoui and Abdullah-Al-Zubaer Imran},
      year={2025},
      eprint={2507.05165},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2507.05165}, 
}

@misc{bai2025qwen25vltechnicalreport,
      title={Qwen2.5-VL Technical Report}, 
      author={Shuai Bai and Keqin Chen and Xuejing Liu and Jialin Wang and Wenbin Ge and Sibo Song and Kai Dang and Peng Wang and Shijie Wang and Jun Tang and Humen Zhong and Yuanzhi Zhu and Mingkun Yang and Zhaohai Li and Jianqiang Wan and Pengfei Wang and Wei Ding and Zheren Fu and Yiheng Xu and Jiabo Ye and Xi Zhang and Tianbao Xie and Zesen Cheng and Hang Zhang and Zhibo Yang and Haiyang Xu and Junyang Lin},
      year={2025},
      eprint={2502.13923},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2502.13923}, 
}

@misc{microsoft2025phi4minitechnicalreportcompact,
      title={Phi-4-Mini Technical Report: Compact yet Powerful Multimodal Language Models via Mixture-of-LoRAs}, 
      author={Microsoft and : and Abdelrahman Abouelenin and Atabak Ashfaq and Adam Atkinson and Hany Awadalla and Nguyen Bach and Jianmin Bao and Alon Benhaim and Martin Cai and Vishrav Chaudhary and Congcong Chen and Dong Chen and Dongdong Chen and Junkun Chen and Weizhu Chen and Yen-Chun Chen and Yi-ling Chen and Qi Dai and Xiyang Dai and Ruchao Fan and Mei Gao and Min Gao and Amit Garg and Abhishek Goswami and Junheng Hao and Amr Hendy and Yuxuan Hu and Xin Jin and Mahmoud Khademi and Dongwoo Kim and Young Jin Kim and Gina Lee and Jinyu Li and Yunsheng Li and Chen Liang and Xihui Lin and Zeqi Lin and Mengchen Liu and Yang Liu and Gilsinia Lopez and Chong Luo and Piyush Madan and Vadim Mazalov and Arindam Mitra and Ali Mousavi and Anh Nguyen and Jing Pan and Daniel Perez-Becker and Jacob Platin and Thomas Portet and Kai Qiu and Bo Ren and Liliang Ren and Sambuddha Roy and Ning Shang and Yelong Shen and Saksham Singhal and Subhojit Som and Xia Song and Tetyana Sych and Praneetha Vaddamanu and Shuohang Wang and Yiming Wang and Zhenghao Wang and Haibin Wu and Haoran Xu and Weijian Xu and Yifan Yang and Ziyi Yang and Donghan Yu and Ishmam Zabir and Jianwen Zhang and Li Lyna Zhang and Yunan Zhang and Xiren Zhou},
      year={2025},
      eprint={2503.01743},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2503.01743}, 
}

@misc{lyu2023macawllmmultimodallanguagemodeling,
      title={Macaw-LLM: Multi-Modal Language Modeling with Image, Audio, Video, and Text Integration}, 
      author={Chenyang Lyu and Minghao Wu and Longyue Wang and Xinting Huang and Bingshuai Liu and Zefeng Du and Shuming Shi and Zhaopeng Tu},
      year={2023},
      eprint={2306.09093},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2306.09093}, 
}

@misc{liu2023audioldmtexttoaudiogenerationlatent,
      title={AudioLDM: Text-to-Audio Generation with Latent Diffusion Models}, 
      author={Haohe Liu and Zehua Chen and Yi Yuan and Xinhao Mei and Xubo Liu and Danilo Mandic and Wenwu Wang and Mark D. Plumbley},
      year={2023},
      eprint={2301.12503},
      archivePrefix={arXiv},
      primaryClass={cs.SD},
      url={https://arxiv.org/abs/2301.12503}, 
}

@misc{zhang2025llavavideovideoinstructiontuning,
      title={LLaVA-Video: Video Instruction Tuning With Synthetic Data}, 
      author={Yuanhan Zhang and Jinming Wu and Wei Li and Bo Li and Zejun Ma and Ziwei Liu and Chunyuan Li},
      year={2025},
      eprint={2410.02713},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2410.02713}, 
}

@misc{hong20233dllminjecting3dworld,
      title={3D-LLM: Injecting the 3D World into Large Language Models}, 
      author={Yining Hong and Haoyu Zhen and Peihao Chen and Shuhong Zheng and Yilun Du and Zhenfang Chen and Chuang Gan},
      year={2023},
      eprint={2307.12981},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2307.12981}, 
}

@misc{défossez2024moshispeechtextfoundationmodel,
      title={Moshi: a speech-text foundation model for real-time dialogue}, 
      author={Alexandre Défossez and Laurent Mazaré and Manu Orsini and Amélie Royer and Patrick Pérez and Hervé Jégou and Edouard Grave and Neil Zeghidour},
      year={2024},
      eprint={2410.00037},
      archivePrefix={arXiv},
      primaryClass={eess.AS},
      url={https://arxiv.org/abs/2410.00037}, 
}

@misc{xu2025qwen25omnitechnicalreport,
      title={Qwen2.5-Omni Technical Report}, 
      author={Jin Xu and Zhifang Guo and Jinzheng He and Hangrui Hu and Ting He and Shuai Bai and Keqin Chen and Jialin Wang and Yang Fan and Kai Dang and Bin Zhang and Xiong Wang and Yunfei Chu and Junyang Lin},
      year={2025},
      eprint={2503.20215},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2503.20215}, 
}

@inproceedings{zhang-etal-2024-mm,
    title = "{MM}-{LLM}s: Recent Advances in {M}ulti{M}odal Large Language Models",
    author = "Zhang, Duzhen  and
      Yu, Yahan  and
      Dong, Jiahua  and
      Li, Chenxing  and
      Su, Dan  and
      Chu, Chenhui  and
      Yu, Dong",
    editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
    booktitle = "Findings of the Association for Computational Linguistics: ACL 2024",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.findings-acl.738/",
    doi = "10.18653/v1/2024.findings-acl.738",
    pages = "12401--12430",
}

@misc{chu2023qwenaudioadvancinguniversalaudio,
      title={Qwen-Audio: Advancing Universal Audio Understanding via Unified Large-Scale Audio-Language Models}, 
      author={Yunfei Chu and Jin Xu and Xiaohuan Zhou and Qian Yang and Shiliang Zhang and Zhijie Yan and Chang Zhou and Jingren Zhou},
      year={2023},
      eprint={2311.07919},
      archivePrefix={arXiv},
      primaryClass={eess.AS},
      url={https://arxiv.org/abs/2311.07919}, 
}

@misc{rubenstein2023audiopalmlargelanguagemodel,
      title={AudioPaLM: A Large Language Model That Can Speak and Listen}, 
      author={Paul K. Rubenstein and Chulayuth Asawaroengchai and Duc Dung Nguyen and Ankur Bapna and Zalán Borsos and Félix de Chaumont Quitry and Peter Chen and Dalia El Badawy and Wei Han and Eugene Kharitonov and Hannah Muckenhirn and Dirk Padfield and James Qin and Danny Rozenberg and Tara Sainath and Johan Schalkwyk and Matt Sharifi and Michelle Tadmor Ramanovich and Marco Tagliasacchi and Alexandru Tudor and Mihajlo Velimirović and Damien Vincent and Jiahui Yu and Yongqiang Wang and Vicky Zayats and Neil Zeghidour and Yu Zhang and Zhishuai Zhang and Lukas Zilka and Christian Frank},
      year={2023},
      eprint={2306.12925},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2306.12925}, 
}

@inproceedings{yu-etal-2020-ch,
    title = "{CH}-{SIMS}: A {C}hinese Multimodal Sentiment Analysis Dataset with Fine-grained Annotation of Modality",
    author = "Yu, Wenmeng  and
      Xu, Hua  and
      Meng, Fanyang  and
      Zhu, Yilin  and
      Ma, Yixiao  and
      Wu, Jiele  and
      Zou, Jiyun  and
      Yang, Kaicheng",
    editor = "Jurafsky, Dan  and
      Chai, Joyce  and
      Schluter, Natalie  and
      Tetreault, Joel",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.343/",
    doi = "10.18653/v1/2020.acl-main.343",
    pages = "3718--3727",
}

@misc{stappen2021multimodalsentimentanalysiscar,
      title={The Multimodal Sentiment Analysis in Car Reviews (MuSe-CaR) Dataset: Collection, Insights and Improvements}, 
      author={Lukas Stappen and Alice Baird and Lea Schumann and Björn Schuller},
      year={2021},
      eprint={2101.06053},
      archivePrefix={arXiv},
      primaryClass={cs.MM},
      url={https://arxiv.org/abs/2101.06053}, 
}

@inproceedings{xie2025tttfusion,
  author    = {Qinhua Xie and Hao Tang},
  title     = {{TTTFusion: A Test-Time Training-Based Strategy for Multimodal Medical Image Fusion in Surgical Robots}},
  booktitle = {Proceedings of the IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
  year      = {2025},
  publisher = {IEEE},
}

@INPROCEEDINGS{10944080,
  author={Yu, Pinrui and Kong, Zhenglun and Zhao, Pu and Dong, Peiyan and Tang, Hao and Sun, Fei and Lin, Xue and Wang, Yanzhi},
  booktitle={2025 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)}, 
  title={Q-TempFusion: Quantization-Aware Temporal Multi-Sensor Fusion on Bird's-Eye View Representation}, 
  year={2025},
  volume={},
  number={},
  pages={5489-5499},
  keywords={Performance evaluation;Solid modeling;Three-dimensional displays;Quantization (signal);Computational modeling;Predictive models;Real-time systems;Spatiotemporal phenomena;Reliability;Standards},
  doi={10.1109/WACV61041.2025.00536}}



